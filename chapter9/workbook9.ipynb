{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9 : Backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start with a simplified forward pass with just one neuron. Rather than backpropagating from the loss function for a full neural network, let’s backpropagate the ReLU function for a single neuron and act as if we intend to minimize the output for this single neuron. We’re first doing this only as a demonstration to simplify the explanation, since minimizing the output from a ReLU activated \n",
    "neuron doesn’t serve any purpose other than as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example neuron with three inputs\n",
    "x = [1.0,-2.0,3.0] # input values\n",
    "w = [-3.0,-1.0,2.0] # weights\n",
    "b = 1.0 # bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplying input sby weights\n",
    "xw0 = x[0]*w[0]\n",
    "xw1 = x[1]*w[1]\n",
    "xw2 = x[2]*w[2]\n",
    "\n",
    "# adding weighted inputs and bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Pass\n",
    "dvalue = 1.0\n",
    "\n",
    "#Derivative of ReLU and chain rule \n",
    "drelu_dz = dvalue*(1.0 if z>0 else 0.0) # z is denoting the sum only\n",
    "print(drelu_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial deivatives of sums and the chain rule\n",
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 1\n",
    "dsum_dxw2 = 1\n",
    "dsum_db = 1\n",
    "drelu_dxw0 = drelu_dz*dsum_dxw0\n",
    "drelu_dxw1 = drelu_dz*dsum_dxw1\n",
    "drelu_dxw2 = drelu_dz*dsum_dxw2\n",
    "drelu_db = drelu_dz*dsum_db\n",
    "print(drelu_dxw0,drelu_dxw1,drelu_dxw2,drelu_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partial derivatives of multiplications and chain rule\n",
    "dmul_dx0 = w[0] \n",
    "dmul_dx1 = w[1] \n",
    "dmul_dx2 = w[2] \n",
    "dmul_dw0 = x[0] \n",
    "dmul_dw1 = x[1] \n",
    "dmul_dw2 = x[2] \n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0 \n",
    "drelu_dw0 = drelu_dxw0 * dmul_dw0 \n",
    "drelu_dx1 = drelu_dxw1 * dmul_dx1 \n",
    "drelu_dw1 = drelu_dxw1 * dmul_dw1 \n",
    "drelu_dx2 = drelu_dxw2 * dmul_dx2 \n",
    "drelu_dw2 = drelu_dxw2 * dmul_dw2 \n",
    "print(drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Passed in gradient from the next layer \n",
    "# for the purpose of this example we're going to use \n",
    "# a vector of 1s \n",
    "dvalues = np.array([[1.,1.,1.]])\n",
    "\n",
    "# We have 3 sets of weights - one set for each neuron \n",
    "# we have 4 inputs, thus 4 weights \n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1], \n",
    "                    [0.5, -0.91, 0.26, -0.5], \n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "# sum weights of given input \n",
    "# and multiply by the passed in gradient for this neuron \n",
    "# dx0 = sum(weights[0]*dvalues[0]) \n",
    "# dx1 = sum(weights[1]*dvalues[0]) \n",
    "# dx2 = sum(weights[2]*dvalues[0])\n",
    "# dx3 = sum(weights[3]*dvalues[0])\n",
    "# dinputs = np.array([dx0, dx1, dx2, dx3]) \n",
    "\n",
    "# or a better method\n",
    "dinputs = np.dot(dvalues[0],weights.T)\n",
    "\n",
    "\n",
    "\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvalues = np.array([[1.,1.,1.],\n",
    "                    [2.,2.,2.],\n",
    "                    [3.,3.,3.]])\n",
    "\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1], \n",
    "                    [0.5, -0.91, 0.26, -0.5], \n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "dinputs = np.dot(dvalues,weights.T)\n",
    "\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"For weights\"\"\"\n",
    "\n",
    "dvalues = np.array([[1.,1.,1.],\n",
    "                    [2.,2.,2.],\n",
    "                    [3.,3.,3.]])\n",
    "\n",
    "# We have 3 sets of inputs - samples \n",
    "inputs = np.array([[1, 2, 3, 2.5], \n",
    "                   [2., 5., -1., 2], \n",
    "                   [-1.5, 2.7, 3.3, -0.8]]) \n",
    "\n",
    "# sum weights of given input \n",
    "# and multiply by the passed in gradient for this neuron \n",
    "dweights = np.dot(inputs.T, dvalues)\n",
    "\n",
    "print(dweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passed in gradient from the next layer \n",
    "# for the purpose of this example we're going to use \n",
    "# an array of an incremental gradient values \n",
    "dvalues = np.array([[1., 1., 1.], \n",
    "                    [2., 2., 2.], \n",
    "                    [3., 3., 3.]]) \n",
    "\n",
    "# One bias for each neuron \n",
    "# biases are the row vector with a shape (1, neurons) \n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "\n",
    "# dbiases - sum values, do this over samples (first axis), keepdims \n",
    "# since this by default will produce a plain list - \n",
    "# we explained this in the chapter 4 \n",
    "dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "                 \n",
    "print(dbiases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example layer output \n",
    "z = np.array([[1, 2, -3, -4], \n",
    "              [2, -7, -1, 3], \n",
    "              [-1, 2, 5, -1]]) \n",
    " \n",
    "dvalues = np.array([[1, 2, 3, 4], \n",
    "                    [5, 6, 7, 8], \n",
    "                    [9, 10, 11, 12]]) \n",
    " \n",
    "# ReLU activation's derivative \n",
    "drelu = np.zeros_like(z) \n",
    "drelu[z > 0] = 1 \n",
    " \n",
    "print(drelu) \n",
    " \n",
    "# The chain rule \n",
    "drelu *= dvalues \n",
    " \n",
    "print(drelu) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([[0.2, 0.8, -0.5, 1], \n",
    "                    [0.5, -0.91, 0.26, -0.5], \n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(weights[0])*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvalue = np.array([[1.,1.,1.]])\n",
    "dvalue[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.2+0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.7-0.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    #Layer initialisation\n",
    "    def __init__(self,inputs,neurons):\n",
    "        self.weights = 0.01*np.random.randn(inputs,neurons)\n",
    "        self.biases = np.zeros((1,neurons))\n",
    "        self.inputs = inputs\n",
    "\n",
    "    #Forward pass\n",
    "    def forward(self,inputs):\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases \n",
    "\n",
    "\n",
    "    #Backward Pass\n",
    "    def backward(self,dvalues):\n",
    "        #Gradients on params\n",
    "        self.dweights = np.dot(self.inputs.T,dvalues)\n",
    "        self.dbiases = np.sum(dvalues,axis = 0,keepdims = True)\n",
    "\n",
    "        # gradient on values\n",
    "        self.dinputs = np.dot(dvalues,self.weights.T)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU Activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs)\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "        # Since we need to modify the original variable, \n",
    "        # let's make a copy of the values first \n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        #zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class \n",
    "class Loss: \n",
    " \n",
    "    # Calculates the data and regularization losses \n",
    "    # given model output and ground truth values \n",
    "    def calculate(self, output, y): \n",
    " \n",
    "        # Calculate sample losses \n",
    "        sample_losses = self.forward(output, y) \n",
    " \n",
    "        # Calculate mean loss \n",
    "        data_loss = np.mean(sample_losses) \n",
    " \n",
    "        # Return loss \n",
    "        return data_loss\n",
    "        \n",
    "# cross entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    #backward pass\n",
    "    def backward(self,dvalues,y_true):\n",
    "         # Number of samples \n",
    "        samples = len(dvalues) \n",
    "        # Number of labels in every sample \n",
    "        # We'll use the first sample to count them \n",
    "        labels = len(dvalues[0]) \n",
    " \n",
    "        # If labels are sparse, turn them into one-hot vector \n",
    "        if len(y_true.shape) == 1: \n",
    "            y_true = np.eye(labels)[y_true] \n",
    " \n",
    "        # Calculate gradient \n",
    "        self.dinputs = -y_true / dvalues \n",
    "        # Normalize gradient \n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation \n",
    "class Activation_Softmax: \n",
    "    ... \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    " \n",
    "        # Create uninitialized array \n",
    "        self.dinputs = np.empty_like(dvalues) \n",
    " \n",
    "        # Enumerate outputs and gradients \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)): \n",
    "            # Flatten output array \n",
    "            single_output = single_output.reshape(-1, 1) \n",
    "            # Calculate Jacobian matrix of the output and \n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T) \n",
    "            # Calculate sample-wise gradient \n",
    "            # and add it to the array of sample gradients \n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, \n",
    "                                         single_dvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation \n",
    "# and cross-entropy loss for faster backward step \n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy(): \n",
    " \n",
    "    # Creates activation and loss function objects \n",
    "    def __init__(self): \n",
    "        self.activation = Activation_Softmax() \n",
    "        self.loss = Loss_CategoricalCrossentropy() \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs, y_true): \n",
    "        # Output layer's activation function \n",
    "        self.activation.forward(inputs) \n",
    "        # Set the output \n",
    "        self.output = self.activation.output \n",
    "        # Calculate and return loss value \n",
    "        return self.loss.calculate(self.output, y_true) \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues, y_true): \n",
    " \n",
    "        # Number of samples \n",
    "        samples = len(dvalues)\n",
    "\n",
    "    # If labels are one-hot encoded, \n",
    "        # turn them into discrete values \n",
    "        if len(y_true.shape) == 2: \n",
    "            y_true = np.argmax(y_true, axis=1) \n",
    " \n",
    "        # Copy so we can safely modify \n",
    "        self.dinputs = dvalues.copy() \n",
    "        # Calculate gradient \n",
    "        self.dinputs[range(samples), y_true] -= 1 \n",
    "        # Normalize gradient \n",
    "        self.dinputs = self.dinputs/samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvalues = np.array([[1.,1.,1.]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvalues[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([[0.2, 0.8, -0.5, 1], \n",
    "                    [0.5, -0.91, 0.26, -0.5], \n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(weights[0]*dvalues[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dvalues = np.array([[1.,1.,1.],\n",
    "                    [2.,2.,2.],\n",
    "                    [3.,3.,3.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.array([[0.2, 0.8, -0.5, 1], \n",
    "                    [0.5, -0.91, 0.26, -0.5], \n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.2 ,  0.5 , -0.26],\n",
       "       [ 0.8 , -0.91, -0.27],\n",
       "       [-0.5 ,  0.26,  0.17],\n",
       "       [ 1.  , -0.5 ,  0.87]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.44, -0.38, -0.07,  1.37],\n",
       "       [ 0.88, -0.76, -0.14,  2.74],\n",
       "       [ 1.32, -1.14, -0.21,  4.11]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dinputs = np.dot(dvalues,weights.T)\n",
    "dinputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[1, 2, 3, 2.5], \n",
    "                   [2., 5., -1., 2], \n",
    "                   [-1.5, 2.7, 3.3, -0.8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dweights = np.dot(inputs.T,dvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5,  0.5,  0.5],\n",
       "       [20.1, 20.1, 20.1],\n",
       "       [10.9, 10.9, 10.9],\n",
       "       [ 4.1,  4.1,  4.1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dweights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1. , 1. , 1. ],\n",
       "       [2. , 2. , 2. ],\n",
       "       [3. , 3. , 3. ],\n",
       "       [2.5, 2.5, 2.5]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input1 = np.array([[1,2,3,2.5]])\n",
    "dvalue1 = np.array([[1.,1.,1.]])\n",
    "dweight1 = np.dot(input1.T,dvalue1)\n",
    "dweight1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.,  4.,  4.],\n",
       "       [10., 10., 10.],\n",
       "       [-2., -2., -2.],\n",
       "       [ 4.,  4.,  4.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input2 = np.array([[2.,5.,-1.,2.]])\n",
    "dvalue2 = np.array([[2.,2.,2.]])\n",
    "dweight2 = np.dot(input2.T,dvalue2)\n",
    "dweight2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.5, -4.5, -4.5],\n",
       "       [ 8.1,  8.1,  8.1],\n",
       "       [ 9.9,  9.9,  9.9],\n",
       "       [-2.4, -2.4, -2.4]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input3 = np.array([[-1.5, 2.7, 3.3, -0.8]])\n",
    "dvalue3 = np.array([[3.,3.,3.]])\n",
    "dweight3 = np.dot(input3.T,dvalue3)\n",
    "dweight3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True],\n",
       "       [ True,  True,  True]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(dweight1 + dweight2 + dweight3) == dweights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbiases = np.sum(dvalues,axis=0,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6., 6., 6.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dbiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example layer output \n",
    "z = np.array([[1, 2, -3, -4], \n",
    "              [2, -7, -1, 3], \n",
    "              [-1, 2, 5, -1]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dvalues = np.array([[1, 2, 3, 4], \n",
    "                    [5, 6, 7, 8], \n",
    "                    [9, 10, 11, 12]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU's activation's derivative\n",
    "drelu = np.zeros_like(z)\n",
    "drelu[z>0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 0],\n",
       "       [1, 0, 0, 1],\n",
       "       [0, 1, 1, 0]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  0,  0],\n",
       "       [ 5,  0,  0,  8],\n",
       "       [ 0, 10, 11,  0]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the chain rule \n",
    "drelu *= dvalues\n",
    "drelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example layer output \n",
    "z = np.array([[1, 2, -3, -4], \n",
    "              [2, -7, -1, 3], \n",
    "              [-1, 2, 5, -1]]) \n",
    " \n",
    "dvalues = np.array([[1, 2, 3, 4], \n",
    "                    [5, 6, 7, 8], \n",
    "                    [9, 10, 11, 12]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "drelu = dvalues.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "drelu[z<0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  0,  0],\n",
       "       [ 5,  0,  0,  8],\n",
       "       [ 0, 10, 11,  0]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.1985  0.5005 -0.2615]\n",
      " [ 0.7903 -0.9147 -0.2797]\n",
      " [-0.5053  0.2537  0.1647]\n",
      " [ 0.9963 -0.5017  0.8663]]\n",
      "[[1.997 2.998 0.497]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Passed in gradient from the next layer \n",
    "# for the purpose of this example we're going to use \n",
    "# an array of an incremental gradient values \n",
    "dvalues = np.array([[1., 1., 1.], \n",
    "                    [2., 2., 2.], \n",
    "                    [3., 3., 3.]])  \n",
    " \n",
    "# We have 3 sets of inputs - samples \n",
    "inputs = np.array([[1, 2, 3, 2.5], \n",
    "                   [2., 5., -1., 2], \n",
    "                   [-1.5, 2.7, 3.3, -0.8]]) \n",
    "\n",
    "# We have 3 sets of weights - one set for each neuron \n",
    "# we have 4 inputs, thus 4 weights \n",
    "# recall that we keep weights transposed \n",
    "weights = np.array([[0.2, 0.8, -0.5, 1], \n",
    "                    [0.5, -0.91, 0.26, -0.5], \n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T \n",
    "\n",
    "# One bias for each neuron \n",
    "# biases are the row vector with a shape (1, neurons) \n",
    "biases = np.array([[2, 3, 0.5]]) \n",
    "\n",
    "# forward pass\n",
    "layer_outputs = np.dot(inputs,weights) + biases # Dense layer\n",
    "relu_outputs = np.maximum(0,layer_outputs) #ReLU Activation\n",
    "\n",
    "# Let's optimize and test backpropagation here \n",
    "# ReLU activation - simulates derivative with respect to input values \n",
    "# from next layer passed to current layer during backpropagation \n",
    "drelu = relu_outputs.copy() \n",
    "drelu = np.where(drelu>0,1,0)\n",
    "\n",
    "#dense layer\n",
    "# dinputs -multiply by weights\n",
    "dinputs = np.dot(drelu,weights.T)\n",
    "# dweights - multiply by inputs\n",
    "dweights = np.dot(inputs.T,drelu)\n",
    "# dbiases - sum values, do this over samples (first axis), keepdims\n",
    "dbiases = np.sum(drelu,axis=0,keepdims=True)\n",
    "\n",
    "#update parameters\n",
    "weights -= 0.001*dweights\n",
    "biases -= 0.001*dbiases\n",
    "\n",
    "print(weights)\n",
    "print(biases)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 5, 6, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,-2,-9,-7,5,6,-7])\n",
    "a[a<0] = 0\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "\n",
    "    # Layer initialisation\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        self.weights = 0.01*np.random.randn(n_inputs,n_neurons)\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "    # forward base\n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = np.dot(inputs,self.weights) + self.biases\n",
    "\n",
    "    # backward pass\n",
    "    def backward(self,dvalues):\n",
    "        # gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T,dvalues)\n",
    "        self.biases = np.sum(dvalues,axis=0,keepdims=True)\n",
    "\n",
    "        # gradients on inputs\n",
    "        self.dinputs = np.dot(dvalues,self.weights.T)\n",
    "\n",
    "class Activation_ReLU:\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs)\n",
    "\n",
    "    # backward pass\n",
    "    def backward(self,dvalues):\n",
    "        # Since we need to modify the original variable, \n",
    "        # let's make a copy of the values first \n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        #zero gradient where input values are negative\n",
    "        self.dinputs[self.inputs <= 0 ] = 0\n",
    "        \n",
    "\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "\n",
    "    # backward pass\n",
    "    def backward(self,dvalues,y_true):\n",
    "\n",
    "        # number of samples\n",
    "        samples = len(dvalues)\n",
    "\n",
    "        # number of labels in each sample\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # if labels are sparse then turn them into one hot vector\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # calculate gradient\n",
    "        self.dinputs = -y_true/dvalues\n",
    "\n",
    "        # normalise gradinet\n",
    "        self.dinputs = self.dinputs/samples\n",
    "\n",
    "class Activation_Softmax:\n",
    "\n",
    "    #backward pass\n",
    "    def backward(self,dvalues):\n",
    "\n",
    "        # create uninitialised array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        #enumerate outputs and gradients\n",
    "        for index,(single_output,single_dvalues) in enumerate(zip(self.output,dvalues)):\n",
    "            #flatten output array\n",
    "            single_output = single_output.reshape(-1,1)\n",
    "\n",
    "            #calculate JAcobian matrix of the output \n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T) \n",
    "\n",
    "        # Calculate sample-wise gradient \n",
    "        # and add it to the array of sample gradients \n",
    "        self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
