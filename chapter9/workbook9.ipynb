{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9 : Backpropogation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start with a simplified forward pass with just one neuron. Rather than backpropagating from the loss function for a full neural network, let’s backpropagate the ReLU function for a single neuron and act as if we intend to minimize the output for this single neuron. We’re first doing this only as a demonstration to simplify the explanation, since minimizing the output from a ReLU activated \n",
    "neuron doesn’t serve any purpose other than as an exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example neuron with three inputs\n",
    "x = [1.0,-2.0,3.0] # input values\n",
    "w = [-3.0,-1.0,2.0] # weights\n",
    "b = 1.0 # bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiplying input sby weights\n",
    "xw0 = x[0]*w[0]\n",
    "xw1 = x[1]*w[1]\n",
    "xw2 = x[2]*w[2]\n",
    "\n",
    "# adding weighted inputs and bias\n",
    "z = xw0 + xw1 + xw2 + b\n",
    "\n",
    "# ReLU activation function\n",
    "y = max(z,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "# Backward Pass\n",
    "dvalue = 1.0\n",
    "\n",
    "#Derivative of ReLU and chain rule \n",
    "drelu_dz = dvalue*(1.0 if z>0 else 0.0) # z is denoting the sum only\n",
    "print(drelu_dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "# partial deivatives of sums and the chain rule\n",
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 1\n",
    "dsum_dxw2 = 1\n",
    "dsum_db = 1\n",
    "drelu_dxw0 = drelu_dz*dsum_dxw0\n",
    "drelu_dxw1 = drelu_dz*dsum_dxw1\n",
    "drelu_dxw2 = drelu_dz*dsum_dxw2\n",
    "drelu_db = drelu_dz*dsum_db\n",
    "print(drelu_dxw0,drelu_dxw1,drelu_dxw2,drelu_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0 1.0 -1.0 -2.0 2.0 3.0\n"
     ]
    }
   ],
   "source": [
    "# partial derivatives of multiplications and chain rule\n",
    "dmul_dx0 = w[0] \n",
    "dmul_dx1 = w[1] \n",
    "dmul_dx2 = w[2] \n",
    "dmul_dw0 = x[0] \n",
    "dmul_dw1 = x[1] \n",
    "dmul_dw2 = x[2] \n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0 \n",
    "drelu_dw0 = drelu_dxw0 * dmul_dw0 \n",
    "drelu_dx1 = drelu_dxw1 * dmul_dx1 \n",
    "drelu_dw1 = drelu_dxw1 * dmul_dw1 \n",
    "drelu_dx2 = drelu_dxw2 * dmul_dx2 \n",
    "drelu_dw2 = drelu_dxw2 * dmul_dw2 \n",
    "print(drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.44 -0.38 -0.07  1.37]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Passed in gradient from the next layer \n",
    "# for the purpose of this example we're going to use \n",
    "# a vector of 1s \n",
    "dvalues = np.array([[1.,1.,1.]])\n",
    "\n",
    "# We have 3 sets of weights - one set for each neuron \n",
    "# we have 4 inputs, thus 4 weights \n",
    "# recall that we keep weights transposed\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1], \n",
    "                    [0.5, -0.91, 0.26, -0.5], \n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "# sum weights of given input \n",
    "# and multiply by the passed in gradient for this neuron \n",
    "# dx0 = sum(weights[0]*dvalues[0]) \n",
    "# dx1 = sum(weights[1]*dvalues[0]) \n",
    "# dx2 = sum(weights[2]*dvalues[0])\n",
    "# dx3 = sum(weights[3]*dvalues[0])\n",
    "# dinputs = np.array([dx0, dx1, dx2, dx3]) \n",
    "\n",
    "# or a better method\n",
    "dinputs = np.dot(dvalues[0],weights.T)\n",
    "\n",
    "\n",
    "\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.44 -0.38 -0.07  1.37]\n",
      " [ 0.88 -0.76 -0.14  2.74]\n",
      " [ 1.32 -1.14 -0.21  4.11]]\n"
     ]
    }
   ],
   "source": [
    "dvalues = np.array([[1.,1.,1.],\n",
    "                    [2.,2.,2.],\n",
    "                    [3.,3.,3.]])\n",
    "\n",
    "weights = np.array([[0.2, 0.8, -0.5, 1], \n",
    "                    [0.5, -0.91, 0.26, -0.5], \n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "dinputs = np.dot(dvalues,weights.T)\n",
    "\n",
    "print(dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.5  0.5]\n",
      " [20.1 20.1 20.1]\n",
      " [10.9 10.9 10.9]\n",
      " [ 4.1  4.1  4.1]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"For weights\"\"\"\n",
    "\n",
    "dvalues = np.array([[1.,1.,1.],\n",
    "                    [2.,2.,2.],\n",
    "                    [3.,3.,3.]])\n",
    "\n",
    "# We have 3 sets of inputs - samples \n",
    "inputs = np.array([[1, 2, 3, 2.5], \n",
    "                   [2., 5., -1., 2], \n",
    "                   [-1.5, 2.7, 3.3, -0.8]]) \n",
    "\n",
    "# sum weights of given input \n",
    "# and multiply by the passed in gradient for this neuron \n",
    "dweights = np.dot(inputs.T, dvalues)\n",
    "\n",
    "print(dweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6. 6. 6.]]\n"
     ]
    }
   ],
   "source": [
    "# Passed in gradient from the next layer \n",
    "# for the purpose of this example we're going to use \n",
    "# an array of an incremental gradient values \n",
    "dvalues = np.array([[1., 1., 1.], \n",
    "                    [2., 2., 2.], \n",
    "                    [3., 3., 3.]]) \n",
    "\n",
    "# One bias for each neuron \n",
    "# biases are the row vector with a shape (1, neurons) \n",
    "biases = np.array([[2, 3, 0.5]])\n",
    "\n",
    "# dbiases - sum values, do this over samples (first axis), keepdims \n",
    "# since this by default will produce a plain list - \n",
    "# we explained this in the chapter 4 \n",
    "dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "                 \n",
    "print(dbiases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0]\n",
      " [1 0 0 1]\n",
      " [0 1 1 0]]\n",
      "[[ 1  2  0  0]\n",
      " [ 5  0  0  8]\n",
      " [ 0 10 11  0]]\n"
     ]
    }
   ],
   "source": [
    "# Example layer output \n",
    "z = np.array([[1, 2, -3, -4], \n",
    "              [2, -7, -1, 3], \n",
    "              [-1, 2, 5, -1]]) \n",
    " \n",
    "dvalues = np.array([[1, 2, 3, 4], \n",
    "                    [5, 6, 7, 8], \n",
    "                    [9, 10, 11, 12]]) \n",
    " \n",
    "# ReLU activation's derivative \n",
    "drelu = np.zeros_like(z) \n",
    "drelu[z > 0] = 1 \n",
    " \n",
    "print(drelu) \n",
    " \n",
    "# The chain rule \n",
    "drelu *= dvalues \n",
    " \n",
    "print(drelu) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.2   0.5  -0.26]\n",
      " [ 0.8  -0.91 -0.27]\n",
      " [-0.5   0.26  0.17]\n",
      " [ 1.   -0.5   0.87]]\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([[0.2, 0.8, -0.5, 1], \n",
    "                    [0.5, -0.91, 0.26, -0.5], \n",
    "                    [-0.26, -0.27, 0.17, 0.87]]).T\n",
    "\n",
    "print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43999999999999995"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(weights[0])*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1.])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dvalue = np.array([[1.,1.,1.]])\n",
    "dvalue[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.2 ,  0.5 , -0.26])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2+0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43999999999999995"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.7-0.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer\n",
    "class Layer_Dense:\n",
    "\n",
    "    #Layer initialisation\n",
    "    def __init__(self,inputs,neurons):\n",
    "        self.weights = 0.01*np.random.randn(inputs,neurons)\n",
    "        self.biases = np.zeros((1,neurons))\n",
    "        self.inputs = inputs\n",
    "\n",
    "    #Forward pass\n",
    "    def forward(self,inputs):\n",
    "        self.output = np.dot(inputs,self.weights) + self.biases \n",
    "\n",
    "\n",
    "    #Backward Pass\n",
    "    def backward(self,dvalues):\n",
    "        #Gradients on params\n",
    "        self.dweights = np.dot(self.inputs.T,dvalues)\n",
    "        self.dbiases = np.sum(dvalues,axis = 0,keepdims = True)\n",
    "\n",
    "        # gradient on values\n",
    "        self.dinputs = np.dot(dvalues,self.weights.T)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU Activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    def forward(self,inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0,inputs)\n",
    "\n",
    "    def backward(self,dvalues):\n",
    "        # Since we need to modify the original variable, \n",
    "        # let's make a copy of the values first \n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "        #zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class \n",
    "class Loss: \n",
    " \n",
    "    # Calculates the data and regularization losses \n",
    "    # given model output and ground truth values \n",
    "    def calculate(self, output, y): \n",
    " \n",
    "        # Calculate sample losses \n",
    "        sample_losses = self.forward(output, y) \n",
    " \n",
    "        # Calculate mean loss \n",
    "        data_loss = np.mean(sample_losses) \n",
    " \n",
    "        # Return loss \n",
    "        return data_loss\n",
    "        \n",
    "# cross entropy loss\n",
    "class Loss_CategoricalCrossentropy(Loss):\n",
    "\n",
    "    #backward pass\n",
    "    def backward(self,dvalues,y_true):\n",
    "         # Number of samples \n",
    "        samples = len(dvalues) \n",
    "        # Number of labels in every sample \n",
    "        # We'll use the first sample to count them \n",
    "        labels = len(dvalues[0]) \n",
    " \n",
    "        # If labels are sparse, turn them into one-hot vector \n",
    "        if len(y_true.shape) == 1: \n",
    "            y_true = np.eye(labels)[y_true] \n",
    " \n",
    "        # Calculate gradient \n",
    "        self.dinputs = -y_true / dvalues \n",
    "        # Normalize gradient \n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax activation \n",
    "class Activation_Softmax: \n",
    "    ... \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    " \n",
    "        # Create uninitialized array \n",
    "        self.dinputs = np.empty_like(dvalues) \n",
    " \n",
    "        # Enumerate outputs and gradients \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)): \n",
    "            # Flatten output array \n",
    "            single_output = single_output.reshape(-1, 1) \n",
    "            # Calculate Jacobian matrix of the output and \n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T) \n",
    "            # Calculate sample-wise gradient \n",
    "            # and add it to the array of sample gradients \n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, \n",
    "                                         single_dvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax classifier - combined Softmax activation \n",
    "# and cross-entropy loss for faster backward step \n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy(): \n",
    " \n",
    "    # Creates activation and loss function objects \n",
    "    def __init__(self): \n",
    "        self.activation = Activation_Softmax() \n",
    "        self.loss = Loss_CategoricalCrossentropy() \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs, y_true): \n",
    "        # Output layer's activation function \n",
    "        self.activation.forward(inputs) \n",
    "        # Set the output \n",
    "        self.output = self.activation.output \n",
    "        # Calculate and return loss value \n",
    "        return self.loss.calculate(self.output, y_true) \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues, y_true): \n",
    " \n",
    "        # Number of samples \n",
    "        samples = len(dvalues)\n",
    "\n",
    "    # If labels are one-hot encoded, \n",
    "        # turn them into discrete values \n",
    "        if len(y_true.shape) == 2: \n",
    "            y_true = np.argmax(y_true, axis=1) \n",
    " \n",
    "        # Copy so we can safely modify \n",
    "        self.dinputs = dvalues.copy() \n",
    "        # Calculate gradient \n",
    "        self.dinputs[range(samples), y_true] -= 1 \n",
    "        # Normalize gradient \n",
    "        self.dinputs = self.dinputs/samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
