{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer \n",
    "class Layer_Dense: \n",
    " \n",
    "    # Layer initialization \n",
    "    def __init__(self, n_inputs, n_neurons): \n",
    "        # Initialize weights and biases \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons) \n",
    "        self.biases = np.zeros((1, n_neurons)) \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs): \n",
    "        # Remember input values \n",
    "        self.inputs = inputs \n",
    "        # Calculate output values from inputs, weights and biases \n",
    "        self.output = np.dot(inputs, self.weights) + self.biases \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    "        # Gradients on parameters \n",
    "        self.dweights = np.dot(self.inputs.T, dvalues) \n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True) \n",
    "        # Gradient on values \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T) \n",
    " \n",
    " \n",
    "# ReLU activation \n",
    "class Activation_ReLU: \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs): \n",
    "          # Remember input values \n",
    "        self.inputs = inputs \n",
    "        # Calculate output values from inputs \n",
    "        self.output = np.maximum(0, inputs) \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    "        # Since we need to modify original variable, \n",
    "        # let's make a copy of values first \n",
    "        self.dinputs = dvalues.copy() \n",
    " \n",
    "        # Zero gradient where input values were negative \n",
    "        self.dinputs[self.inputs <= 0] = 0 \n",
    " \n",
    " \n",
    "# Softmax activation \n",
    "class Activation_Softmax: \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs): \n",
    "        # Remember input values \n",
    "        self.inputs = inputs \n",
    " \n",
    "        # Get unnormalized probabilities \n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, \n",
    "                                            keepdims=True)) \n",
    "        # Normalize them for each sample \n",
    "        probabilities = exp_values/np.sum(exp_values, axis=1, \n",
    "                                            keepdims=True) \n",
    " \n",
    "        self.output = probabilities \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    " \n",
    "        # Create uninitialized array \n",
    "        self.dinputs = np.empty_like(dvalues) \n",
    " \n",
    "        # Enumerate outputs and gradients \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)): \n",
    "            # Flatten output array \n",
    "            single_output = single_output.reshape(-1, 1) \n",
    "            # Calculate Jacobian matrix of the output and \n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T) \n",
    "            # Calculate sample-wise gradient \n",
    "            # and add it to the array of sample gradients \n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, \n",
    "                                         single_dvalues) \n",
    " \n",
    " \n",
    "# Common loss class \n",
    "class Loss: \n",
    " \n",
    "    # Calculates the data and regularization losses \n",
    "    # given model output and ground truth values \n",
    "    def calculate(self, output, y): \n",
    " \n",
    "        # Calculate sample losses \n",
    "        sample_losses = self.forward(output, y) \n",
    " \n",
    "        # Calculate mean loss \n",
    "        data_loss = np.mean(sample_losses) \n",
    " \n",
    "        # Return loss \n",
    "        return data_loss \n",
    " \n",
    " \n",
    "# Cross-entropy loss \n",
    "class Loss_CategoricalCrossentropy(Loss): \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, y_pred, y_true): \n",
    " \n",
    "        # Number of samples in a batch \n",
    "        samples = len(y_pred) \n",
    " \n",
    "        # Clip data to prevent division by 0 \n",
    "        # Clip both sides to not drag mean towards any value \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) \n",
    " \n",
    "        # Probabilities for target values - \n",
    "        # only if categorical labels \n",
    "        if len(y_true.shape) == 1: \n",
    "            correct_confidences = y_pred_clipped[ \n",
    "                range(samples), \n",
    "                y_true \n",
    "            ] \n",
    "        # Mask values - only for one-hot encoded labels \n",
    "        elif len(y_true.shape) == 2: \n",
    "            correct_confidences = np.sum( \n",
    "                y_pred_clipped * y_true, \n",
    "                axis=1 \n",
    "            ) \n",
    " \n",
    "        # Losses \n",
    "        negative_log_likelihoods = -np.log(correct_confidences) \n",
    "        return negative_log_likelihoods \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues, y_true): \n",
    " \n",
    "        # Number of samples \n",
    "        samples = len(dvalues) \n",
    "        # Number of labels in every sample \n",
    "        # We'll use the first sample to count them \n",
    "        labels = len(dvalues[0]) \n",
    " \n",
    "        # If labels are sparse, turn them into one-hot vector \n",
    "        if len(y_true.shape) == 1: \n",
    "            y_true = np.eye(labels)[y_true] \n",
    " \n",
    "        # Calculate gradient \n",
    "        self.dinputs = -y_true / dvalues \n",
    "        # Normalize gradient \n",
    "        self.dinputs = self.dinputs/samples \n",
    " \n",
    " \n",
    "# Softmax classifier - combined Softmax activation \n",
    "# and cross-entropy loss for faster backward step \n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy(): \n",
    " \n",
    "    # Creates activation and loss function objects \n",
    "    def __init__(self): \n",
    "        self.activation = Activation_Softmax() \n",
    "        self.loss = Loss_CategoricalCrossentropy() \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs, y_true): \n",
    "        # Output layer's activation function \n",
    "        self.activation.forward(inputs) \n",
    "        # Set the output \n",
    "        self.output = self.activation.output \n",
    "        # Calculate and return loss value \n",
    "        return self.loss.calculate(self.output, y_true) \n",
    "    \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues, y_true): \n",
    " \n",
    "        # Number of samples \n",
    "        samples = len(dvalues) \n",
    " \n",
    "        # If labels are one-hot encoded, \n",
    "        # turn them into discrete values \n",
    "        if len(y_true.shape) == 2: \n",
    "            y_true = np.argmax(y_true, axis=1) \n",
    " \n",
    "        # Copy so we can safely modify \n",
    "        self.dinputs = dvalues.copy() \n",
    "        # Calculate gradient \n",
    "        self.dinputs[range(samples), y_true] -= 1 \n",
    "        # Normalize gradient \n",
    "        self.dinputs = self.dinputs/samples \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimiser_SGD:\n",
    "\n",
    "    # initialise optimiser -set settings\n",
    "    # learning rate ofg 1.0 is deafult for this optimiser\n",
    "    def __init__(self,learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    # update parameters\n",
    "    def update_params(self,layer):\n",
    "        layer.weights += -self.learning_rate*layer.dweights\n",
    "        layer.biases += -self.learning_rate*layer.dbiases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the layer object contains its parameters (weights and biases) and also, at this stage, the \n",
    "gradient that is calculated during backpropagation. We store these in the layer’s properties so that \n",
    "the optimizer can make use of them. In our main neural network code, we’d bring the \n",
    "optimization in after backpropagation. Let’s make a 1x64 densely-connected neural network (1 \n",
    "hidden layer with 64 neurons) and use the same dataset as before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = spiral_data(samples=100,classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create second Dense layer with 64 input features (as we take output  \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next step is to create the optimizer’s object: \n",
    "# Create optimizer \n",
    "optimizer = Optimiser_SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now peform a forward pass of the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass through this layer\n",
    "dense1.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a forward pass through activation function \n",
    "# takes the output of first dense layer here \n",
    "activation1.forward(dense1.output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a forward pass through second Dense layer \n",
    "# takes outputs of activation function of first layer as inputs \n",
    "dense2.forward(activation1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a forward pass through the activation/loss function \n",
    "# takes the output of second dense layer here and returns loss \n",
    "loss = loss_activation.forward(dense2.output, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.0985943\n"
     ]
    }
   ],
   "source": [
    "# let's print the loss\n",
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.36\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy from output of activation2 and targets \n",
    "# calculate values along first axis \n",
    "predictions = np.argmax(loss_activation.output, axis=1) \n",
    "if len(y.shape) == 2: \n",
    "    y = np.argmax(y, axis=1) \n",
    "accuracy = np.mean(predictions==y) \n",
    "print('acc:', accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do backward pass,which is called backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "loss_activation.backward(loss_activation.output,y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the optimiser to update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.update_params(dense1)\n",
    "optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each full pass through all of the training data is called an epoch.** <br>\n",
    "In most deep learning tasks, a neural network will be trained for multiple epochs, though the ideal scenario would be to \n",
    "have a perfect model with ideal weights and biases after only one epoch.  To add multiple epochs \n",
    "of training into our code, we will initialize our model and run a loop around all the code \n",
    "performing the forward pass, backward pass, and optimization calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    " \n",
    "# Create second Dense layer with 64 input features (as we take output  \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimiser = Optimiser_SGD() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.343, loss: 1.099\n",
      "epoch: 100, acc: 0.407, loss: 1.083\n",
      "epoch: 200, acc: 0.397, loss: 1.071\n",
      "epoch: 300, acc: 0.410, loss: 1.070\n",
      "epoch: 400, acc: 0.410, loss: 1.069\n",
      "epoch: 500, acc: 0.413, loss: 1.067\n",
      "epoch: 600, acc: 0.410, loss: 1.064\n",
      "epoch: 700, acc: 0.423, loss: 1.058\n",
      "epoch: 800, acc: 0.450, loss: 1.047\n",
      "epoch: 900, acc: 0.430, loss: 1.050\n",
      "epoch: 1000, acc: 0.427, loss: 1.045\n",
      "epoch: 1100, acc: 0.440, loss: 1.038\n",
      "epoch: 1200, acc: 0.453, loss: 1.029\n",
      "epoch: 1300, acc: 0.400, loss: 1.019\n",
      "epoch: 1400, acc: 0.480, loss: 1.026\n",
      "epoch: 1500, acc: 0.413, loss: 1.003\n",
      "epoch: 1600, acc: 0.397, loss: 0.994\n",
      "epoch: 1700, acc: 0.443, loss: 0.976\n",
      "epoch: 1800, acc: 0.403, loss: 0.995\n",
      "epoch: 1900, acc: 0.463, loss: 0.973\n",
      "epoch: 2000, acc: 0.487, loss: 0.969\n",
      "epoch: 2100, acc: 0.470, loss: 0.956\n",
      "epoch: 2200, acc: 0.497, loss: 0.951\n",
      "epoch: 2300, acc: 0.480, loss: 0.936\n",
      "epoch: 2400, acc: 0.470, loss: 0.915\n",
      "epoch: 2500, acc: 0.493, loss: 0.904\n",
      "epoch: 2600, acc: 0.587, loss: 0.862\n",
      "epoch: 2700, acc: 0.557, loss: 0.821\n",
      "epoch: 2800, acc: 0.543, loss: 0.827\n",
      "epoch: 2900, acc: 0.580, loss: 0.814\n",
      "epoch: 3000, acc: 0.637, loss: 0.801\n",
      "epoch: 3100, acc: 0.650, loss: 0.761\n",
      "epoch: 3200, acc: 0.613, loss: 0.813\n",
      "epoch: 3300, acc: 0.570, loss: 0.728\n",
      "epoch: 3400, acc: 0.667, loss: 0.716\n",
      "epoch: 3500, acc: 0.670, loss: 0.714\n",
      "epoch: 3600, acc: 0.657, loss: 0.755\n",
      "epoch: 3700, acc: 0.673, loss: 0.687\n",
      "epoch: 3800, acc: 0.637, loss: 0.651\n",
      "epoch: 3900, acc: 0.637, loss: 0.647\n",
      "epoch: 4000, acc: 0.660, loss: 0.630\n",
      "epoch: 4100, acc: 0.637, loss: 0.634\n",
      "epoch: 4200, acc: 0.630, loss: 0.632\n",
      "epoch: 4300, acc: 0.653, loss: 0.622\n",
      "epoch: 4400, acc: 0.670, loss: 0.605\n",
      "epoch: 4500, acc: 0.653, loss: 0.612\n",
      "epoch: 4600, acc: 0.677, loss: 0.614\n",
      "epoch: 4700, acc: 0.717, loss: 0.615\n",
      "epoch: 4800, acc: 0.650, loss: 0.611\n",
      "epoch: 4900, acc: 0.670, loss: 0.599\n",
      "epoch: 5000, acc: 0.673, loss: 0.602\n",
      "epoch: 5100, acc: 0.517, loss: 1.290\n",
      "epoch: 5200, acc: 0.687, loss: 0.588\n",
      "epoch: 5300, acc: 0.673, loss: 0.591\n",
      "epoch: 5400, acc: 0.677, loss: 0.586\n",
      "epoch: 5500, acc: 0.683, loss: 0.587\n",
      "epoch: 5600, acc: 0.683, loss: 0.596\n",
      "epoch: 5700, acc: 0.547, loss: 1.120\n",
      "epoch: 5800, acc: 0.673, loss: 0.647\n",
      "epoch: 5900, acc: 0.690, loss: 0.570\n",
      "epoch: 6000, acc: 0.690, loss: 0.569\n",
      "epoch: 6100, acc: 0.693, loss: 0.568\n",
      "epoch: 6200, acc: 0.693, loss: 0.565\n",
      "epoch: 6300, acc: 0.703, loss: 0.571\n",
      "epoch: 6400, acc: 0.537, loss: 1.350\n",
      "epoch: 6500, acc: 0.693, loss: 0.587\n",
      "epoch: 6600, acc: 0.700, loss: 0.583\n",
      "epoch: 6700, acc: 0.703, loss: 0.576\n",
      "epoch: 6800, acc: 0.710, loss: 0.568\n",
      "epoch: 6900, acc: 0.720, loss: 0.568\n",
      "epoch: 7000, acc: 0.700, loss: 0.561\n",
      "epoch: 7100, acc: 0.597, loss: 0.951\n",
      "epoch: 7200, acc: 0.697, loss: 0.602\n",
      "epoch: 7300, acc: 0.720, loss: 0.570\n",
      "epoch: 7400, acc: 0.717, loss: 0.557\n",
      "epoch: 7500, acc: 0.700, loss: 0.540\n",
      "epoch: 7600, acc: 0.703, loss: 0.638\n",
      "epoch: 7700, acc: 0.720, loss: 0.563\n",
      "epoch: 7800, acc: 0.717, loss: 0.556\n",
      "epoch: 7900, acc: 0.713, loss: 0.543\n",
      "epoch: 8000, acc: 0.713, loss: 0.535\n",
      "epoch: 8100, acc: 0.670, loss: 0.669\n",
      "epoch: 8200, acc: 0.723, loss: 0.545\n",
      "epoch: 8300, acc: 0.720, loss: 0.541\n",
      "epoch: 8400, acc: 0.740, loss: 0.507\n",
      "epoch: 8500, acc: 0.723, loss: 0.542\n",
      "epoch: 8600, acc: 0.723, loss: 0.545\n",
      "epoch: 8700, acc: 0.723, loss: 0.531\n",
      "epoch: 8800, acc: 0.727, loss: 0.528\n",
      "epoch: 8900, acc: 0.720, loss: 0.505\n",
      "epoch: 9000, acc: 0.730, loss: 0.520\n",
      "epoch: 9100, acc: 0.743, loss: 0.521\n",
      "epoch: 9200, acc: 0.743, loss: 0.495\n",
      "epoch: 9300, acc: 0.767, loss: 0.503\n",
      "epoch: 9400, acc: 0.720, loss: 0.606\n",
      "epoch: 9500, acc: 0.777, loss: 0.497\n",
      "epoch: 9600, acc: 0.770, loss: 0.503\n",
      "epoch: 9700, acc: 0.777, loss: 0.489\n",
      "epoch: 9800, acc: 0.780, loss: 0.496\n",
      "epoch: 9900, acc: 0.777, loss: 0.474\n",
      "epoch: 10000, acc: 0.787, loss: 0.485\n"
     ]
    }
   ],
   "source": [
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    " \n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}') \n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " \n",
    "    # Update weights and biases \n",
    "    optimiser.update_params(dense1) \n",
    "    optimiser.update_params(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above gives us an update of where we are (epochs), the model’s accuracy, and loss every 100 epochs. We can see consistent improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
