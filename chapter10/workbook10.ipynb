{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer \n",
    "class Layer_Dense: \n",
    " \n",
    "    # Layer initialization \n",
    "    def __init__(self, n_inputs, n_neurons): \n",
    "        # Initialize weights and biases \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons) \n",
    "        self.biases = np.zeros((1, n_neurons)) \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs): \n",
    "        # Remember input values \n",
    "        self.inputs = inputs \n",
    "        # Calculate output values from inputs, weights and biases \n",
    "        self.output = np.dot(inputs, self.weights) + self.biases \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    "        # Gradients on parameters \n",
    "        self.dweights = np.dot(self.inputs.T, dvalues) \n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True) \n",
    "        # Gradient on values \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T) \n",
    " \n",
    " \n",
    "# ReLU activation \n",
    "class Activation_ReLU: \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs): \n",
    "          # Remember input values \n",
    "        self.inputs = inputs \n",
    "        # Calculate output values from inputs \n",
    "        self.output = np.maximum(0, inputs) \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    "        # Since we need to modify original variable, \n",
    "        # let's make a copy of values first \n",
    "        self.dinputs = dvalues.copy() \n",
    " \n",
    "        # Zero gradient where input values were negative \n",
    "        self.dinputs[self.inputs <= 0] = 0 \n",
    " \n",
    " \n",
    "# Softmax activation \n",
    "class Activation_Softmax: \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs): \n",
    "        # Remember input values \n",
    "        self.inputs = inputs \n",
    " \n",
    "        # Get unnormalized probabilities \n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, \n",
    "                                            keepdims=True)) \n",
    "        # Normalize them for each sample \n",
    "        probabilities = exp_values/np.sum(exp_values, axis=1, \n",
    "                                            keepdims=True) \n",
    " \n",
    "        self.output = probabilities \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    " \n",
    "        # Create uninitialized array \n",
    "        self.dinputs = np.empty_like(dvalues) \n",
    " \n",
    "        # Enumerate outputs and gradients \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)): \n",
    "            # Flatten output array \n",
    "            single_output = single_output.reshape(-1, 1) \n",
    "            # Calculate Jacobian matrix of the output and \n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T) \n",
    "            # Calculate sample-wise gradient \n",
    "            # and add it to the array of sample gradients \n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, \n",
    "                                         single_dvalues) \n",
    " \n",
    " \n",
    "# Common loss class \n",
    "class Loss: \n",
    " \n",
    "    # Calculates the data and regularization losses \n",
    "    # given model output and ground truth values \n",
    "    def calculate(self, output, y): \n",
    " \n",
    "        # Calculate sample losses \n",
    "        sample_losses = self.forward(output, y) \n",
    " \n",
    "        # Calculate mean loss \n",
    "        data_loss = np.mean(sample_losses) \n",
    " \n",
    "        # Return loss \n",
    "        return data_loss \n",
    " \n",
    " \n",
    "# Cross-entropy loss \n",
    "class Loss_CategoricalCrossentropy(Loss): \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, y_pred, y_true): \n",
    " \n",
    "        # Number of samples in a batch \n",
    "        samples = len(y_pred) \n",
    " \n",
    "        # Clip data to prevent division by 0 \n",
    "        # Clip both sides to not drag mean towards any value \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) \n",
    " \n",
    "        # Probabilities for target values - \n",
    "        # only if categorical labels \n",
    "        if len(y_true.shape) == 1: \n",
    "            correct_confidences = y_pred_clipped[ \n",
    "                range(samples), \n",
    "                y_true \n",
    "            ] \n",
    "        # Mask values - only for one-hot encoded labels \n",
    "        elif len(y_true.shape) == 2: \n",
    "            correct_confidences = np.sum( \n",
    "                y_pred_clipped * y_true, \n",
    "                axis=1 \n",
    "            ) \n",
    " \n",
    "        # Losses \n",
    "        negative_log_likelihoods = -np.log(correct_confidences) \n",
    "        return negative_log_likelihoods \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues, y_true): \n",
    " \n",
    "        # Number of samples \n",
    "        samples = len(dvalues) \n",
    "        # Number of labels in every sample \n",
    "        # We'll use the first sample to count them \n",
    "        labels = len(dvalues[0]) \n",
    " \n",
    "        # If labels are sparse, turn them into one-hot vector \n",
    "        if len(y_true.shape) == 1: \n",
    "            y_true = np.eye(labels)[y_true] \n",
    " \n",
    "        # Calculate gradient \n",
    "        self.dinputs = -y_true / dvalues \n",
    "        # Normalize gradient \n",
    "        self.dinputs = self.dinputs/samples \n",
    " \n",
    " \n",
    "# Softmax classifier - combined Softmax activation \n",
    "# and cross-entropy loss for faster backward step \n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy(): \n",
    " \n",
    "    # Creates activation and loss function objects \n",
    "    def __init__(self): \n",
    "        self.activation = Activation_Softmax() \n",
    "        self.loss = Loss_CategoricalCrossentropy() \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs, y_true): \n",
    "        # Output layer's activation function \n",
    "        self.activation.forward(inputs) \n",
    "        # Set the output \n",
    "        self.output = self.activation.output \n",
    "        # Calculate and return loss value \n",
    "        return self.loss.calculate(self.output, y_true) \n",
    "    \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues, y_true): \n",
    " \n",
    "        # Number of samples \n",
    "        samples = len(dvalues) \n",
    " \n",
    "        # If labels are one-hot encoded, \n",
    "        # turn them into discrete values \n",
    "        if len(y_true.shape) == 2: \n",
    "            y_true = np.argmax(y_true, axis=1) \n",
    " \n",
    "        # Copy so we can safely modify \n",
    "        self.dinputs = dvalues.copy() \n",
    "        # Calculate gradient \n",
    "        self.dinputs[range(samples), y_true] -= 1 \n",
    "        # Normalize gradient \n",
    "        self.dinputs = self.dinputs/samples \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimiser_SGD:\n",
    "\n",
    "    # initialise optimiser -set settings\n",
    "    # learning rate ofg 1.0 is deafult for this optimiser\n",
    "    def __init__(self,learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    # update parameters\n",
    "    def update_params(self,layer):\n",
    "        layer.weights += -self.learning_rate*layer.dweights\n",
    "        layer.biases += -self.learning_rate*layer.dbiases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the layer object contains its parameters (weights and biases) and also, at this stage, the \n",
    "gradient that is calculated during backpropagation. We store these in the layer’s properties so that \n",
    "the optimizer can make use of them. In our main neural network code, we’d bring the \n",
    "optimization in after backpropagation. Let’s make a 1x64 densely-connected neural network (1 \n",
    "hidden layer with 64 neurons) and use the same dataset as before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = spiral_data(samples=100,classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create second Dense layer with 64 input features (as we take output  \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next step is to create the optimizer’s object: \n",
    "# Create optimizer \n",
    "optimizer = Optimiser_SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now peform a forward pass of the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass through this layer\n",
    "dense1.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a forward pass through activation function \n",
    "# takes the output of first dense layer here \n",
    "activation1.forward(dense1.output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a forward pass through second Dense layer \n",
    "# takes outputs of activation function of first layer as inputs \n",
    "dense2.forward(activation1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a forward pass through the activation/loss function \n",
    "# takes the output of second dense layer here and returns loss \n",
    "loss = loss_activation.forward(dense2.output, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.0985943\n"
     ]
    }
   ],
   "source": [
    "# let's print the loss\n",
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.36\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy from output of activation2 and targets \n",
    "# calculate values along first axis \n",
    "predictions = np.argmax(loss_activation.output, axis=1) \n",
    "if len(y.shape) == 2: \n",
    "    y = np.argmax(y, axis=1) \n",
    "accuracy = np.mean(predictions==y) \n",
    "print('acc:', accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do backward pass,which is called backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "loss_activation.backward(loss_activation.output,y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the optimiser to update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.update_params(dense1)\n",
    "optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each full pass through all of the training data is called an epoch.** <br>\n",
    "In most deep learning tasks, a neural network will be trained for multiple epochs, though the ideal scenario would be to \n",
    "have a perfect model with ideal weights and biases after only one epoch.  To add multiple epochs \n",
    "of training into our code, we will initialize our model and run a loop around all the code \n",
    "performing the forward pass, backward pass, and optimization calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    " \n",
    "# Create second Dense layer with 64 input features (as we take output  \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimiser = Optimiser_SGD() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.343, loss: 1.099\n",
      "epoch: 100, acc: 0.407, loss: 1.083\n",
      "epoch: 200, acc: 0.397, loss: 1.071\n",
      "epoch: 300, acc: 0.410, loss: 1.070\n",
      "epoch: 400, acc: 0.410, loss: 1.069\n",
      "epoch: 500, acc: 0.413, loss: 1.067\n",
      "epoch: 600, acc: 0.410, loss: 1.064\n",
      "epoch: 700, acc: 0.423, loss: 1.058\n",
      "epoch: 800, acc: 0.450, loss: 1.047\n",
      "epoch: 900, acc: 0.430, loss: 1.050\n",
      "epoch: 1000, acc: 0.427, loss: 1.045\n",
      "epoch: 1100, acc: 0.440, loss: 1.038\n",
      "epoch: 1200, acc: 0.453, loss: 1.029\n",
      "epoch: 1300, acc: 0.400, loss: 1.019\n",
      "epoch: 1400, acc: 0.480, loss: 1.026\n",
      "epoch: 1500, acc: 0.413, loss: 1.003\n",
      "epoch: 1600, acc: 0.397, loss: 0.994\n",
      "epoch: 1700, acc: 0.443, loss: 0.976\n",
      "epoch: 1800, acc: 0.403, loss: 0.995\n",
      "epoch: 1900, acc: 0.463, loss: 0.973\n",
      "epoch: 2000, acc: 0.487, loss: 0.969\n",
      "epoch: 2100, acc: 0.470, loss: 0.956\n",
      "epoch: 2200, acc: 0.497, loss: 0.951\n",
      "epoch: 2300, acc: 0.480, loss: 0.936\n",
      "epoch: 2400, acc: 0.470, loss: 0.915\n",
      "epoch: 2500, acc: 0.493, loss: 0.904\n",
      "epoch: 2600, acc: 0.587, loss: 0.862\n",
      "epoch: 2700, acc: 0.557, loss: 0.821\n",
      "epoch: 2800, acc: 0.543, loss: 0.827\n",
      "epoch: 2900, acc: 0.580, loss: 0.814\n",
      "epoch: 3000, acc: 0.637, loss: 0.801\n",
      "epoch: 3100, acc: 0.650, loss: 0.761\n",
      "epoch: 3200, acc: 0.613, loss: 0.813\n",
      "epoch: 3300, acc: 0.570, loss: 0.728\n",
      "epoch: 3400, acc: 0.667, loss: 0.716\n",
      "epoch: 3500, acc: 0.670, loss: 0.714\n",
      "epoch: 3600, acc: 0.657, loss: 0.755\n",
      "epoch: 3700, acc: 0.673, loss: 0.687\n",
      "epoch: 3800, acc: 0.637, loss: 0.651\n",
      "epoch: 3900, acc: 0.637, loss: 0.647\n",
      "epoch: 4000, acc: 0.660, loss: 0.630\n",
      "epoch: 4100, acc: 0.637, loss: 0.634\n",
      "epoch: 4200, acc: 0.630, loss: 0.632\n",
      "epoch: 4300, acc: 0.653, loss: 0.622\n",
      "epoch: 4400, acc: 0.670, loss: 0.605\n",
      "epoch: 4500, acc: 0.653, loss: 0.612\n",
      "epoch: 4600, acc: 0.677, loss: 0.614\n",
      "epoch: 4700, acc: 0.717, loss: 0.615\n",
      "epoch: 4800, acc: 0.650, loss: 0.611\n",
      "epoch: 4900, acc: 0.670, loss: 0.599\n",
      "epoch: 5000, acc: 0.673, loss: 0.602\n",
      "epoch: 5100, acc: 0.517, loss: 1.290\n",
      "epoch: 5200, acc: 0.687, loss: 0.588\n",
      "epoch: 5300, acc: 0.673, loss: 0.591\n",
      "epoch: 5400, acc: 0.677, loss: 0.586\n",
      "epoch: 5500, acc: 0.683, loss: 0.587\n",
      "epoch: 5600, acc: 0.683, loss: 0.596\n",
      "epoch: 5700, acc: 0.547, loss: 1.120\n",
      "epoch: 5800, acc: 0.673, loss: 0.647\n",
      "epoch: 5900, acc: 0.690, loss: 0.570\n",
      "epoch: 6000, acc: 0.690, loss: 0.569\n",
      "epoch: 6100, acc: 0.693, loss: 0.568\n",
      "epoch: 6200, acc: 0.693, loss: 0.565\n",
      "epoch: 6300, acc: 0.703, loss: 0.571\n",
      "epoch: 6400, acc: 0.537, loss: 1.350\n",
      "epoch: 6500, acc: 0.693, loss: 0.587\n",
      "epoch: 6600, acc: 0.700, loss: 0.583\n",
      "epoch: 6700, acc: 0.703, loss: 0.576\n",
      "epoch: 6800, acc: 0.710, loss: 0.568\n",
      "epoch: 6900, acc: 0.720, loss: 0.568\n",
      "epoch: 7000, acc: 0.700, loss: 0.561\n",
      "epoch: 7100, acc: 0.597, loss: 0.951\n",
      "epoch: 7200, acc: 0.697, loss: 0.602\n",
      "epoch: 7300, acc: 0.720, loss: 0.570\n",
      "epoch: 7400, acc: 0.717, loss: 0.557\n",
      "epoch: 7500, acc: 0.700, loss: 0.540\n",
      "epoch: 7600, acc: 0.703, loss: 0.638\n",
      "epoch: 7700, acc: 0.720, loss: 0.563\n",
      "epoch: 7800, acc: 0.717, loss: 0.556\n",
      "epoch: 7900, acc: 0.713, loss: 0.543\n",
      "epoch: 8000, acc: 0.713, loss: 0.535\n",
      "epoch: 8100, acc: 0.670, loss: 0.669\n",
      "epoch: 8200, acc: 0.723, loss: 0.545\n",
      "epoch: 8300, acc: 0.720, loss: 0.541\n",
      "epoch: 8400, acc: 0.740, loss: 0.507\n",
      "epoch: 8500, acc: 0.723, loss: 0.542\n",
      "epoch: 8600, acc: 0.723, loss: 0.545\n",
      "epoch: 8700, acc: 0.723, loss: 0.531\n",
      "epoch: 8800, acc: 0.727, loss: 0.528\n",
      "epoch: 8900, acc: 0.720, loss: 0.505\n",
      "epoch: 9000, acc: 0.730, loss: 0.520\n",
      "epoch: 9100, acc: 0.743, loss: 0.521\n",
      "epoch: 9200, acc: 0.743, loss: 0.495\n",
      "epoch: 9300, acc: 0.767, loss: 0.503\n",
      "epoch: 9400, acc: 0.720, loss: 0.606\n",
      "epoch: 9500, acc: 0.777, loss: 0.497\n",
      "epoch: 9600, acc: 0.770, loss: 0.503\n",
      "epoch: 9700, acc: 0.777, loss: 0.489\n",
      "epoch: 9800, acc: 0.780, loss: 0.496\n",
      "epoch: 9900, acc: 0.777, loss: 0.474\n",
      "epoch: 10000, acc: 0.787, loss: 0.485\n"
     ]
    }
   ],
   "source": [
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    " \n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}') \n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " \n",
    "    # Update weights and biases \n",
    "    optimiser.update_params(dense1) \n",
    "    optimiser.update_params(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above gives us an update of where we are (epochs), the model’s accuracy, and loss every 100 epochs. We can see consistent improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the learning rate is too small,then small updates to the parameters caused stagnation in the model’s learning — the model got stuck in a local minimum. <br>\n",
    "![](img1.png)\n",
    "-  With our example here, as well as with optimizing full neural networks, we do not know where the global minimum is. How do we know if we’ve reached the global minimum or at least gotten close? <br>\n",
    "The loss function measures how far the model is with its predictions to the real target values, so, as long as the loss value is not 0​ or very close to 0​, and the model stopped learning, we’re at some local minimum.\n",
    "- In reality, we almost never approach a loss of 0​ for various reasons:\n",
    " 1. One reason for this may be imperfect neural network hyperparameters. \n",
    " 2. Another reason for this may be insufficient data. If you did reach a loss of 0 with a neural network, you should find it suspicious: **overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try modifying the learning rate: <br> <br>\n",
    "![](img2.png) <br> <br>\n",
    "This time, the model escaped this local minimum but got stuck at another one. Let’s see one more example after another learning rate change: <br> <br>\n",
    "![](img3.png) <br> <br>\n",
    "This time the model got stuck at a local minimum near the global minimum. The model was able to escape the “deeper” local minimums, so it might be counter-intuitive why it is stuck here. <br>\n",
    "Remember, the model follows the direction of steepest descent of the loss function, no matter how large or slight the descent is. For this reason, we’ll introduce momentum and the other techniques to prevent such situations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum in gradient descent\n",
    "\n",
    "Momentum, in an optimizer, adds to the gradient what, in the physical world, we could call inertia. <br> <br>\n",
    "![](img4.png) <br><br>\n",
    "In the above figure you can see we used a very small learning rate here with a large momentum. The color change from green, \n",
    "through orange to red presents the advancement of the gradient descent process, the steps. We can see that the model achieved the goal and found the global minimum, but this took many steps. <br>\n",
    "**Can this be done better?** <br> <br>\n",
    "![](img5.png) <br> <br>\n",
    "And even better: <br> <br>\n",
    "![](img6.png) <br><br>\n",
    "With these examples, we were able to find the global minimum in about **200, 100, and 50 steps**, respectively, by modifying the learning rate and the momentum. It’s possible to significantly shorten the training time by adjusting the parameters of the optimizer.<br>\n",
    "However, we have to be careful with these hyper-parameter adjustments, as this won’t necessarily always help the model: <br><br>\n",
    "![](img7.png) <br><br>\n",
    "In the above case,the learning rate is set too high, the model might not be able to find the global minimum. Even, at some point, if it does, further adjustments could cause it to jump out of this minimum. The model was “jumping” around some minimum and what this might mean is that we should try to:\n",
    "- lower the learning rate\n",
    "- raise the momentum, or \n",
    "- possibly apply a learning rate decay (lowering the learning rate during training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we set the learning rate far too high: <br><br>\n",
    "![](img8.png) <br><br>\n",
    "In the above situation, the model starts “jumping” around, and moves in what we might observe as random directions. This is an example of “**overshooting**,” with every step — the direction of a change is correct, but the amount of the gradient applied is too large. In an extreme situation, we could cause a **gradient explosion**: <br><br>\n",
    "![](img9.png) <br><br>\n",
    "**Note:** <br><br>\n",
    "A **gradient explosion** is a situation where the parameter updates cause the function’s output to rise instead of fall, and, with each step, the loss value and gradient become larger. At some point, the floating-point variable limitation causes an overflow as it cannot hold values of this size anymore, and the model is no longer able to train.<br>\n",
    "It’s crucial to recognize this situation forming during training, especially for large models, where the training can take days, weeks, or more. It is possible to tune the model’s hyper-parameters in time to save the model and to continue training. <br><br>\n",
    "When we choose the learning rate and the other hyper-parameters correctly, the learning process can be relatively quick:\n",
    "<img src=\"img10.png\" style=\"width: 45%; display: inline-block; margin-right: 5%;\" />\n",
    "<img src=\"img11.png\" style=\"width: 45%; display: inline-block;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge is to choose the hyper-parameters correctly, and it is not always an easy task.<br>\n",
    "**Few tips:**\n",
    "-  It is usually best to start with the optimizer defaults, perform a few steps, and observe the training process when \n",
    "tuning different settings.\n",
    "-  It is not always possible to see meaningful results in a short-enough period of time, and, in this case, it’s good to have the ability to update the optimizer’s settings during training.\n",
    "- How you choose the learning rate, and other hyper-parameters, depends on the model, data, including the amount of data, the parameter initialization method, etc. There is no single, best way to set hyper-parameters, but experience usually helps.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a summary of learning rates — if we plot the loss along an axis of steps: <br> <br>\n",
    "![](img12.png) <br><br>\n",
    "We can see various examples of relative learning rates and what loss will ideally look like as a \n",
    "graph over time (steps) of training. <br> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing what the learning rate should be to get the most out of your training process isn’t possible, but a good rule is that your initial training will benefit from a larger learning rate to take initial steps faster. If you start with steps that are too small, you might get stuck in a local minimum and be unable to leave it due to not making large enough updates to the parameters. \n",
    "**For example, what if we make the learning rate 0.85 rather than 1.0 with the SGD optimizer?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.320, loss: 1.099\n",
      "epoch: 100, acc: 0.390, loss: 1.095\n",
      "epoch: 200, acc: 0.390, loss: 1.081\n",
      "epoch: 300, acc: 0.387, loss: 1.078\n",
      "epoch: 400, acc: 0.403, loss: 1.078\n",
      "epoch: 500, acc: 0.417, loss: 1.077\n",
      "epoch: 600, acc: 0.423, loss: 1.076\n",
      "epoch: 700, acc: 0.423, loss: 1.074\n",
      "epoch: 800, acc: 0.447, loss: 1.072\n",
      "epoch: 900, acc: 0.447, loss: 1.070\n",
      "epoch: 1000, acc: 0.440, loss: 1.068\n",
      "epoch: 1100, acc: 0.443, loss: 1.065\n",
      "epoch: 1200, acc: 0.467, loss: 1.062\n",
      "epoch: 1300, acc: 0.453, loss: 1.058\n",
      "epoch: 1400, acc: 0.463, loss: 1.053\n",
      "epoch: 1500, acc: 0.477, loss: 1.046\n",
      "epoch: 1600, acc: 0.487, loss: 1.038\n",
      "epoch: 1700, acc: 0.467, loss: 1.029\n",
      "epoch: 1800, acc: 0.437, loss: 1.034\n",
      "epoch: 1900, acc: 0.470, loss: 1.022\n",
      "epoch: 2000, acc: 0.440, loss: 1.031\n",
      "epoch: 2100, acc: 0.453, loss: 1.021\n",
      "epoch: 2200, acc: 0.430, loss: 1.020\n",
      "epoch: 2300, acc: 0.453, loss: 1.023\n",
      "epoch: 2400, acc: 0.517, loss: 1.013\n",
      "epoch: 2500, acc: 0.477, loss: 1.000\n",
      "epoch: 2600, acc: 0.497, loss: 1.004\n",
      "epoch: 2700, acc: 0.493, loss: 1.007\n",
      "epoch: 2800, acc: 0.460, loss: 1.023\n",
      "epoch: 2900, acc: 0.447, loss: 1.006\n",
      "epoch: 3000, acc: 0.487, loss: 1.049\n",
      "epoch: 3100, acc: 0.460, loss: 0.999\n",
      "epoch: 3200, acc: 0.447, loss: 1.013\n",
      "epoch: 3300, acc: 0.447, loss: 0.992\n",
      "epoch: 3400, acc: 0.473, loss: 0.993\n",
      "epoch: 3500, acc: 0.463, loss: 0.993\n",
      "epoch: 3600, acc: 0.443, loss: 1.019\n",
      "epoch: 3700, acc: 0.477, loss: 0.991\n",
      "epoch: 3800, acc: 0.490, loss: 0.998\n",
      "epoch: 3900, acc: 0.507, loss: 0.980\n",
      "epoch: 4000, acc: 0.450, loss: 0.985\n",
      "epoch: 4100, acc: 0.493, loss: 0.973\n",
      "epoch: 4200, acc: 0.500, loss: 0.967\n",
      "epoch: 4300, acc: 0.510, loss: 0.959\n",
      "epoch: 4400, acc: 0.557, loss: 0.945\n",
      "epoch: 4500, acc: 0.497, loss: 0.960\n",
      "epoch: 4600, acc: 0.527, loss: 0.962\n",
      "epoch: 4700, acc: 0.493, loss: 0.949\n",
      "epoch: 4800, acc: 0.477, loss: 0.987\n",
      "epoch: 4900, acc: 0.520, loss: 0.929\n",
      "epoch: 5000, acc: 0.540, loss: 0.974\n",
      "epoch: 5100, acc: 0.533, loss: 0.915\n",
      "epoch: 5200, acc: 0.543, loss: 0.906\n",
      "epoch: 5300, acc: 0.567, loss: 0.882\n",
      "epoch: 5400, acc: 0.553, loss: 0.922\n",
      "epoch: 5500, acc: 0.550, loss: 0.908\n",
      "epoch: 5600, acc: 0.570, loss: 0.902\n",
      "epoch: 5700, acc: 0.567, loss: 0.901\n",
      "epoch: 5800, acc: 0.567, loss: 0.911\n",
      "epoch: 5900, acc: 0.557, loss: 0.906\n",
      "epoch: 6000, acc: 0.553, loss: 0.909\n",
      "epoch: 6100, acc: 0.513, loss: 0.921\n",
      "epoch: 6200, acc: 0.557, loss: 0.871\n",
      "epoch: 6300, acc: 0.547, loss: 0.889\n",
      "epoch: 6400, acc: 0.573, loss: 0.846\n",
      "epoch: 6500, acc: 0.553, loss: 0.955\n",
      "epoch: 6600, acc: 0.557, loss: 0.843\n",
      "epoch: 6700, acc: 0.590, loss: 0.824\n",
      "epoch: 6800, acc: 0.527, loss: 0.905\n",
      "epoch: 6900, acc: 0.587, loss: 0.792\n",
      "epoch: 7000, acc: 0.613, loss: 0.813\n",
      "epoch: 7100, acc: 0.543, loss: 0.839\n",
      "epoch: 7200, acc: 0.610, loss: 0.807\n",
      "epoch: 7300, acc: 0.563, loss: 0.894\n",
      "epoch: 7400, acc: 0.593, loss: 0.779\n",
      "epoch: 7500, acc: 0.557, loss: 0.858\n",
      "epoch: 7600, acc: 0.603, loss: 0.788\n",
      "epoch: 7700, acc: 0.620, loss: 0.863\n",
      "epoch: 7800, acc: 0.630, loss: 0.751\n",
      "epoch: 7900, acc: 0.580, loss: 0.779\n",
      "epoch: 8000, acc: 0.630, loss: 0.748\n",
      "epoch: 8100, acc: 0.650, loss: 0.732\n",
      "epoch: 8200, acc: 0.737, loss: 0.680\n",
      "epoch: 8300, acc: 0.697, loss: 0.693\n",
      "epoch: 8400, acc: 0.547, loss: 1.222\n",
      "epoch: 8500, acc: 0.660, loss: 0.707\n",
      "epoch: 8600, acc: 0.660, loss: 0.760\n",
      "epoch: 8700, acc: 0.683, loss: 0.692\n",
      "epoch: 8800, acc: 0.680, loss: 0.701\n",
      "epoch: 8900, acc: 0.760, loss: 0.613\n",
      "epoch: 9000, acc: 0.737, loss: 0.643\n",
      "epoch: 9100, acc: 0.703, loss: 0.651\n",
      "epoch: 9200, acc: 0.757, loss: 0.612\n",
      "epoch: 9300, acc: 0.727, loss: 0.638\n",
      "epoch: 9400, acc: 0.730, loss: 0.625\n",
      "epoch: 9500, acc: 0.743, loss: 0.624\n",
      "epoch: 9600, acc: 0.760, loss: 0.578\n",
      "epoch: 9700, acc: 0.723, loss: 0.637\n",
      "epoch: 9800, acc: 0.733, loss: 0.622\n",
      "epoch: 9900, acc: 0.753, loss: 0.597\n",
      "epoch: 10000, acc: 0.650, loss: 0.793\n"
     ]
    }
   ],
   "source": [
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimiser = Optimiser_SGD(learning_rate=.85) \n",
    " \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    " \n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(dense2.output, y) \n",
    " \n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}') \n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " \n",
    "    # Update weights and biases \n",
    "    optimiser.update_params(dense1) \n",
    "    optimiser.update_params(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'll compare with the above case where learning_rate = 1 , we have **less accuracy** and **high loss** .<br><br>\n",
    "So, it is very much possible that we git stuck in a local minima and due to smaller updates,couldn't move out of it. \n",
    "<br><br>\n",
    "**NOTE:** <br>\n",
    "Lower accuracy isn't always associated with higher loss and vice-versa. <br>\n",
    "**Why?** <br>\n",
    "-  Remember, even if we desire the best accuracy out of our model, the optimizer’s task is to decrease loss, not raise accuracy \n",
    "directly. \n",
    "- . Loss is the mean value of all of the sample losses, and some of them could drop significantly, while others might rise just slightly, changing the prediction for them from a correct to an incorrect class at the same time. \n",
    "- This would cause a lower mean loss in general, but also more incorrectly predicted samples, which will, at the same time, lower the accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning** <br>\n",
    "In a direct comparison of these two models in training, different learning rates did not show that the lower this learning rate value is, the better. In most cases, we want to start with a larger learning rate and decrease the learning rate over time/steps. <br><br>\n",
    "A commonly-used solution to keep initial updates large and explore various learning rates during \n",
    "training is to implement a **learning rate decay.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of a learning rate decay is to start with a large learning rate, say 1.0 in our case, and \n",
    "then decrease it during training. Let's program a Decay Rate which steadily decays the learning rate per batch or epoch. <br>\n",
    "We are going to update the learning rate decay each step by the reciprocal  of the step count fraction. This fraction is a new **hyperparameter** that we'll add to the optimiser called **learning rate decay**.   <br> <br>\n",
    "How this decaying works is it takes the step and the decaying ratio and \n",
    "multiplies them. The further in training, the bigger the step is, and the bigger result of this \n",
    "multiplication is. We then take its reciprocal (the further in training, the lower the value) and \n",
    "multiply the initial learning rate by it. The added 1​ makes sure that the resulting algorithm never \n",
    "raises the learning rate. <br><br>\n",
    "For example, for the first step, we might divide 1 by the learning rate, 0.001​ for example, which will result in a current learning rate of 1000​. That’s definitely not what we wanted. 1 divided by the 1+fraction ensures that the result, a fraction of the starting learning rate, will always be less than or equal to 1, decreasing over time. That’s the desired result — start with the current learning rate and make it smaller with time. The code for determining the current decay rate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "start_Learning_rate = 1\n",
    "learning_rate_decay = 0.1\n",
    "step = 1\n",
    "learning_rate = start_Learning_rate*(1./(1 + learning_rate_decay*step))\n",
    "print(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# on step = 20\n",
    "step = 20\n",
    "learning_rate = start_Learning_rate*(1./(1 + learning_rate_decay*step))\n",
    "print(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice 0.1 would be considered very aggressive decay rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9090909090909091\n",
      "0.8333333333333334\n",
      "0.7692307692307692\n",
      "0.7142857142857143\n",
      "0.6666666666666666\n",
      "0.625\n",
      "0.588235294117647\n",
      "0.5555555555555556\n",
      "0.5263157894736842\n",
      "0.5\n",
      "0.47619047619047616\n",
      "0.45454545454545453\n",
      "0.4347826086956522\n",
      "0.41666666666666663\n",
      "0.4\n",
      "0.3846153846153846\n",
      "0.37037037037037035\n",
      "0.35714285714285715\n",
      "0.3448275862068965\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "start_Learning_rate = 1.0\n",
    "learning_rate_decay = 0.1\n",
    "for step in range(21):\n",
    "    learning_rate = start_Learning_rate*(1./(1 + learning_rate_decay*step))\n",
    "    print(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This learning rate decay scheme lowers the learning rate each step using the mentioned formula. \n",
    "Initially, the learning rate drops fast, but the change in the learning rate lowers each step, letting \n",
    "the model sit as close as possible to the minimum. The model needs small updates near the end of \n",
    "training to be able to get as close to this point as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD Optimiser\n",
    "class Optimiser_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings, \n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self,learning_rate=1.0,decay = 0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "\n",
    "    \"\"\"This method will update the learning rate if decay is anything other than zero\"\"\"\n",
    "    # call once before any updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate*(1./(1. + self.decay*self.iterations))\n",
    "    \n",
    "    #update parameters\n",
    "    def update_params(self,layer):\n",
    "        layer.weights += -self.current_learning_rate*layer.dweights\n",
    "        layer.biases += -self.current_learning_rate*layer.dbiases\n",
    "\n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our model with a decay rate of (1e-2) i.e. 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.413, loss: 1.099 lr: 1.0\n",
      "epoch: 100, acc: 0.437, loss: 1.082 lr: 0.5025125628140703\n",
      "epoch: 200, acc: 0.450, loss: 1.061 lr: 0.33444816053511706\n",
      "epoch: 300, acc: 0.470, loss: 1.055 lr: 0.2506265664160401\n",
      "epoch: 400, acc: 0.473, loss: 1.053 lr: 0.2004008016032064\n",
      "epoch: 500, acc: 0.477, loss: 1.052 lr: 0.1669449081803005\n",
      "epoch: 600, acc: 0.470, loss: 1.052 lr: 0.14306151645207438\n",
      "epoch: 700, acc: 0.463, loss: 1.051 lr: 0.1251564455569462\n",
      "epoch: 800, acc: 0.473, loss: 1.051 lr: 0.11123470522803114\n",
      "epoch: 900, acc: 0.480, loss: 1.050 lr: 0.10010010010010009\n",
      "epoch: 1000, acc: 0.477, loss: 1.050 lr: 0.09099181073703366\n",
      "epoch: 1100, acc: 0.480, loss: 1.049 lr: 0.08340283569641367\n",
      "epoch: 1200, acc: 0.477, loss: 1.049 lr: 0.07698229407236336\n",
      "epoch: 1300, acc: 0.477, loss: 1.049 lr: 0.07147962830593281\n",
      "epoch: 1400, acc: 0.477, loss: 1.049 lr: 0.066711140760507\n",
      "epoch: 1500, acc: 0.477, loss: 1.048 lr: 0.06253908692933083\n",
      "epoch: 1600, acc: 0.480, loss: 1.048 lr: 0.05885815185403177\n",
      "epoch: 1700, acc: 0.480, loss: 1.048 lr: 0.055586436909394105\n",
      "epoch: 1800, acc: 0.477, loss: 1.048 lr: 0.052659294365455495\n",
      "epoch: 1900, acc: 0.477, loss: 1.047 lr: 0.05002501250625312\n",
      "epoch: 2000, acc: 0.477, loss: 1.047 lr: 0.047641734159123386\n",
      "epoch: 2100, acc: 0.477, loss: 1.047 lr: 0.04547521600727603\n",
      "epoch: 2200, acc: 0.477, loss: 1.047 lr: 0.04349717268377555\n",
      "epoch: 2300, acc: 0.477, loss: 1.047 lr: 0.04168403501458941\n",
      "epoch: 2400, acc: 0.480, loss: 1.046 lr: 0.04001600640256102\n",
      "epoch: 2500, acc: 0.480, loss: 1.046 lr: 0.03847633705271258\n",
      "epoch: 2600, acc: 0.480, loss: 1.046 lr: 0.03705075954057058\n",
      "epoch: 2700, acc: 0.480, loss: 1.046 lr: 0.03572704537334762\n",
      "epoch: 2800, acc: 0.483, loss: 1.046 lr: 0.03449465332873405\n",
      "epoch: 2900, acc: 0.483, loss: 1.045 lr: 0.03334444814938312\n",
      "epoch: 3000, acc: 0.483, loss: 1.045 lr: 0.03226847370119393\n",
      "epoch: 3100, acc: 0.480, loss: 1.045 lr: 0.03125976867771178\n",
      "epoch: 3200, acc: 0.480, loss: 1.045 lr: 0.03031221582297666\n",
      "epoch: 3300, acc: 0.480, loss: 1.045 lr: 0.02942041776993233\n",
      "epoch: 3400, acc: 0.480, loss: 1.044 lr: 0.028579594169762787\n",
      "epoch: 3500, acc: 0.480, loss: 1.044 lr: 0.027785495971103084\n",
      "epoch: 3600, acc: 0.480, loss: 1.044 lr: 0.02703433360367667\n",
      "epoch: 3700, acc: 0.480, loss: 1.044 lr: 0.026322716504343247\n",
      "epoch: 3800, acc: 0.480, loss: 1.044 lr: 0.025647601949217745\n",
      "epoch: 3900, acc: 0.480, loss: 1.044 lr: 0.02500625156289072\n",
      "epoch: 4000, acc: 0.480, loss: 1.043 lr: 0.02439619419370578\n",
      "epoch: 4100, acc: 0.480, loss: 1.043 lr: 0.023815194093831864\n",
      "epoch: 4200, acc: 0.480, loss: 1.043 lr: 0.02326122354035822\n",
      "epoch: 4300, acc: 0.480, loss: 1.043 lr: 0.022732439190725165\n",
      "epoch: 4400, acc: 0.480, loss: 1.043 lr: 0.02222716159146477\n",
      "epoch: 4500, acc: 0.480, loss: 1.043 lr: 0.021743857360295715\n",
      "epoch: 4600, acc: 0.480, loss: 1.043 lr: 0.021281123643328365\n",
      "epoch: 4700, acc: 0.480, loss: 1.042 lr: 0.02083767451552407\n",
      "epoch: 4800, acc: 0.480, loss: 1.042 lr: 0.020412329046744233\n",
      "epoch: 4900, acc: 0.480, loss: 1.042 lr: 0.020004000800160033\n",
      "epoch: 5000, acc: 0.480, loss: 1.042 lr: 0.019611688566385566\n",
      "epoch: 5100, acc: 0.480, loss: 1.042 lr: 0.019234468166955183\n",
      "epoch: 5200, acc: 0.480, loss: 1.042 lr: 0.018871485185884128\n",
      "epoch: 5300, acc: 0.480, loss: 1.042 lr: 0.018521948508983144\n",
      "epoch: 5400, acc: 0.480, loss: 1.041 lr: 0.01818512456810329\n",
      "epoch: 5500, acc: 0.480, loss: 1.041 lr: 0.01786033220217896\n",
      "epoch: 5600, acc: 0.480, loss: 1.041 lr: 0.01754693805930865\n",
      "epoch: 5700, acc: 0.480, loss: 1.041 lr: 0.01724435247456458\n",
      "epoch: 5800, acc: 0.480, loss: 1.041 lr: 0.016952025767079167\n",
      "epoch: 5900, acc: 0.480, loss: 1.041 lr: 0.01666944490748458\n",
      "epoch: 6000, acc: 0.480, loss: 1.041 lr: 0.016396130513198885\n",
      "epoch: 6100, acc: 0.477, loss: 1.041 lr: 0.016131634134537828\n",
      "epoch: 6200, acc: 0.477, loss: 1.040 lr: 0.015875535799333228\n",
      "epoch: 6300, acc: 0.477, loss: 1.040 lr: 0.01562744178777934\n",
      "epoch: 6400, acc: 0.473, loss: 1.040 lr: 0.015386982612709646\n",
      "epoch: 6500, acc: 0.473, loss: 1.040 lr: 0.015153811183512654\n",
      "epoch: 6600, acc: 0.473, loss: 1.040 lr: 0.014927601134497688\n",
      "epoch: 6700, acc: 0.473, loss: 1.040 lr: 0.014708045300779527\n",
      "epoch: 6800, acc: 0.473, loss: 1.040 lr: 0.014494854326714018\n",
      "epoch: 6900, acc: 0.473, loss: 1.040 lr: 0.014287755393627663\n",
      "epoch: 7000, acc: 0.473, loss: 1.039 lr: 0.014086491055078181\n",
      "epoch: 7100, acc: 0.473, loss: 1.039 lr: 0.013890818169190166\n",
      "epoch: 7200, acc: 0.473, loss: 1.039 lr: 0.013700506918755994\n",
      "epoch: 7300, acc: 0.473, loss: 1.039 lr: 0.013515339910798757\n",
      "epoch: 7400, acc: 0.473, loss: 1.039 lr: 0.013335111348179758\n",
      "epoch: 7500, acc: 0.473, loss: 1.039 lr: 0.013159626266614028\n",
      "epoch: 7600, acc: 0.473, loss: 1.039 lr: 0.012988699831146902\n",
      "epoch: 7700, acc: 0.473, loss: 1.039 lr: 0.012822156686754713\n",
      "epoch: 7800, acc: 0.473, loss: 1.038 lr: 0.0126598303582732\n",
      "epoch: 7900, acc: 0.473, loss: 1.038 lr: 0.012501562695336917\n",
      "epoch: 8000, acc: 0.473, loss: 1.038 lr: 0.012347203358439314\n",
      "epoch: 8100, acc: 0.470, loss: 1.038 lr: 0.012196609342602758\n",
      "epoch: 8200, acc: 0.470, loss: 1.038 lr: 0.012049644535486204\n",
      "epoch: 8300, acc: 0.470, loss: 1.038 lr: 0.011906179307060364\n",
      "epoch: 8400, acc: 0.470, loss: 1.038 lr: 0.011766090128250382\n",
      "epoch: 8500, acc: 0.470, loss: 1.038 lr: 0.01162925921618793\n",
      "epoch: 8600, acc: 0.470, loss: 1.038 lr: 0.011495574203931488\n",
      "epoch: 8700, acc: 0.470, loss: 1.038 lr: 0.011364927832708264\n",
      "epoch: 8800, acc: 0.470, loss: 1.037 lr: 0.011237217664906169\n",
      "epoch: 8900, acc: 0.470, loss: 1.037 lr: 0.011112345816201801\n",
      "epoch: 9000, acc: 0.470, loss: 1.037 lr: 0.010990218705352238\n",
      "epoch: 9100, acc: 0.467, loss: 1.037 lr: 0.010870746820306556\n",
      "epoch: 9200, acc: 0.467, loss: 1.037 lr: 0.010753844499408539\n",
      "epoch: 9300, acc: 0.467, loss: 1.037 lr: 0.010639429726566656\n",
      "epoch: 9400, acc: 0.467, loss: 1.037 lr: 0.010527423939362039\n",
      "epoch: 9500, acc: 0.467, loss: 1.037 lr: 0.010417751849150954\n",
      "epoch: 9600, acc: 0.467, loss: 1.037 lr: 0.010310341272296112\n",
      "epoch: 9700, acc: 0.467, loss: 1.037 lr: 0.010205122971731808\n",
      "epoch: 9800, acc: 0.467, loss: 1.037 lr: 0.010102030508132133\n",
      "epoch: 9900, acc: 0.467, loss: 1.036 lr: 0.01000100010001\n",
      "epoch: 10000, acc: 0.467, loss: 1.036 lr: 0.009901970492127933\n"
     ]
    }
   ],
   "source": [
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimiser = Optimiser_SGD(decay=1e-2) \n",
    " \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    " \n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(dense2.output, y) \n",
    " \n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f} ' +\n",
    "              f'lr: {optimiser.current_learning_rate}')\n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " \n",
    "    # Update weights and biases \n",
    "    optimiser.pre_update_params()\n",
    "    optimiser.update_params(dense1) \n",
    "    optimiser.update_params(dense2)\n",
    "    optimiser.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**: \n",
    "Notice that from the last run, you got an even poorer accuracy and higher loss. <br><br>\n",
    "\n",
    "So,**this definitely got stuck somwhere because your learning rate decayed far too quickly and became too small,trapping the model in some local minima. Notice how loss and accuracy stopped changing very much.** <br> <br>\n",
    "\n",
    "We can try decaying slower, try decay = 1e-3 or 0.001 :\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.297, loss: 1.099 lr: 1.0\n",
      "epoch: 100, acc: 0.427, loss: 1.081 lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.407, loss: 1.070 lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.397, loss: 1.067 lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.400, loss: 1.066 lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.413, loss: 1.065 lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.413, loss: 1.065 lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.417, loss: 1.064 lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.417, loss: 1.062 lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.427, loss: 1.059 lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.440, loss: 1.056 lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.453, loss: 1.054 lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.460, loss: 1.052 lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.450, loss: 1.050 lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.453, loss: 1.048 lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.450, loss: 1.046 lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.460, loss: 1.044 lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.457, loss: 1.043 lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.460, loss: 1.041 lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.463, loss: 1.039 lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.463, loss: 1.037 lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.460, loss: 1.035 lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.457, loss: 1.032 lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.453, loss: 1.029 lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.453, loss: 1.026 lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.457, loss: 1.022 lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.453, loss: 1.018 lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.460, loss: 1.014 lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.463, loss: 1.009 lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.470, loss: 1.004 lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.483, loss: 1.000 lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.493, loss: 0.995 lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.503, loss: 0.990 lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.503, loss: 0.986 lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.510, loss: 0.981 lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.513, loss: 0.977 lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.510, loss: 0.973 lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.503, loss: 0.967 lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.510, loss: 0.961 lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.507, loss: 0.955 lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.513, loss: 0.950 lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.523, loss: 0.945 lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.523, loss: 0.941 lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.530, loss: 0.936 lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.530, loss: 0.932 lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.530, loss: 0.927 lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.507, loss: 0.923 lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.517, loss: 0.919 lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.560, loss: 0.925 lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.557, loss: 0.923 lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.550, loss: 0.922 lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.550, loss: 0.920 lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.550, loss: 0.917 lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.550, loss: 0.914 lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.550, loss: 0.912 lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.550, loss: 0.913 lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.547, loss: 0.910 lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.547, loss: 0.907 lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.563, loss: 0.897 lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.570, loss: 0.902 lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.573, loss: 0.899 lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.573, loss: 0.895 lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.570, loss: 0.892 lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.577, loss: 0.887 lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.573, loss: 0.885 lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.577, loss: 0.882 lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.583, loss: 0.879 lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.590, loss: 0.876 lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.590, loss: 0.872 lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.590, loss: 0.870 lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.597, loss: 0.866 lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.593, loss: 0.864 lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.593, loss: 0.860 lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.593, loss: 0.857 lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.597, loss: 0.854 lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.600, loss: 0.850 lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.603, loss: 0.848 lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.603, loss: 0.845 lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.600, loss: 0.842 lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.610, loss: 0.841 lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.620, loss: 0.836 lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.627, loss: 0.830 lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.627, loss: 0.828 lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.633, loss: 0.824 lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.630, loss: 0.823 lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.640, loss: 0.818 lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.640, loss: 0.816 lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.643, loss: 0.813 lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.637, loss: 0.811 lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.640, loss: 0.808 lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.640, loss: 0.805 lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.643, loss: 0.802 lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.650, loss: 0.800 lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.660, loss: 0.796 lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.667, loss: 0.793 lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.670, loss: 0.790 lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.670, loss: 0.787 lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.673, loss: 0.784 lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.673, loss: 0.782 lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.673, loss: 0.779 lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.673, loss: 0.776 lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimiser = Optimiser_SGD(decay=1e-3) \n",
    " \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    " \n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(dense2.output, y) \n",
    " \n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f} ' +\n",
    "              f'lr: {optimiser.current_learning_rate}')\n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " \n",
    "    # Update weights and biases \n",
    "    optimiser.pre_update_params()\n",
    "    optimiser.update_params(dense1) \n",
    "    optimiser.update_params(dense2)\n",
    "    optimiser.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we get even better results? <br>\n",
    "YES,remember you might think that your starting learning rate is too high. <br><br>\n",
    "\n",
    "**Stochastic Gradient Descent with learning rate decay can do fairly well but is still a fairly basic optimization method that only follows a gradient without any additional logic that could potentially help the model find the global minimum to the loss function. One option for improving the SGD optimizer is to introduce** momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent with Momentum "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum creates a rolling average of gradients over some number of updates and uses this average with the unique gradient at each step. <br>\n",
    "Another way of understanding this is to imagine a ball going down a hill — even if it finds a small hole or hill, momentum will let it go straight through it towards a lower minimum — the bottom of this hill. This can help in cases where you’re stuck in some local minimum (a hole), bouncing back and forth. With momentum, a model is more likely to pass through local minimums, further decreasing loss. Simply put, momentum may still point towards the global gradient descent direction. <br><br>\n",
    "![](img13.png)<br><br>\n",
    "Recall this above situation from the chapter. With regular updates, the SGD optimizer might determine that the next best step is one that keeps the model in a local minimum. Remember that the gradient descent points towards the current steepest  loss ascent for that step,it's negative towards steepest descent and which may not necessarily follow descent towards global minima. We may wind up with a gradient that points in one direction and then the opposite direction in next update;<br>\n",
    "the gradient could continue to bounce back and forth around a local minimum like this, keeping the optimization \n",
    "of the loss stuck.<br>\n",
    "**Instead, momentum uses the previous update’s direction to influence the next update’s direction, minimizing the chances of bouncing around and getting stuck.** <br>\n",
    "Recall the example below:<br><br>\n",
    "\n",
    "![](img14.png)<br><br>\n",
    "\n",
    "**How to utilise momentum then?** <br><br>\n",
    "\n",
    "Set a parameter between 0 & 1,representing the farction of previous update to retain,and subtracting (adding the negative) our actual gradient,multiplied by the learning rate(like before),from it.<br>\n",
    "The update contains a portion of the gradient from preceding steps as our momentum (direction of previous changes) and only a portion of the current gradient;<br> <br>\n",
    "**Caution:** <br>\n",
    "- The bigger the role that momentum takes in the update, the slower the update can change the direction.\n",
    "- When we set the momentum fraction too high, the model might stop learning at all since the direction of the updates won’t be able to follow the global gradient descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code update is as follows:<br>\n",
    "```\n",
    "weight_updates = self.momentum*layer.weight_momentums - self.current_learning_rates*layer.dweights\n",
    "```\n",
    "self.momentum is a hyperparamter choosen at the start,layer.weight_momentums starts as all zeros and get updated during training.<br>\n",
    "```\n",
    "layer.weight_momentums = weight_updates\n",
    "```\n",
    "**The momentum is always the previous update to the parameters.** <br>\n",
    "The updated SGD Optmiser's class looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD + momentum Optimiser \n",
    "class Optimiser_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings, \n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self,learning_rate=1.0,decay = 0.,momentum = 0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    \"\"\"This method will update the learning rate if decay is anything other than zero\"\"\"\n",
    "    # call once before any updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate*(1./(1. + self.decay*self.iterations))\n",
    "    \n",
    "    \"\"\"Major changes are in this method wrt vanilla SGD\"\"\"\n",
    "    #update parameters\n",
    "    def update_params(self,layer):\n",
    "\n",
    "        # if we use momentum\n",
    "        if self.momentum:\n",
    "\n",
    "            # If layer does not contain momentum arrays, create them filled with zeros \n",
    "            if not hasattr(layer, 'weight_momentums'): \n",
    "                layer.weight_momentums = np.zeros_like(layer.weights) \n",
    "                # If there is no momentum array for weights \n",
    "                # The array doesn't exist for biases yet either. \n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous updates multiplied by retain factor and update with \n",
    "            # current gradients\n",
    "            weight_updates = self.momentum*layer.weight_momentums - self.current_learning_rate*layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # build bias updates\n",
    "            bias_updates = self.momentum*layer.bias_momentums - self.current_learning_rate*layer.biases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update) \n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate*layer.dweights\n",
    "            bias_updates = -self.current_learning_rate*layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either \n",
    "        # vanilla or momentum updates \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s show an example illustrating how adding momentum changes the learning process.<br>\n",
    "Keeping the same starting learning rate (1) and decay (1e-3) from the previous training attempt and using a momentum of 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.310, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.460, loss: 1.053, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.457, loss: 1.050, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.453, loss: 1.048, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.450, loss: 1.047, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.447, loss: 1.046, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.447, loss: 1.045, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.450, loss: 1.045, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.437, loss: 1.044, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.437, loss: 1.044, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.437, loss: 1.044, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.437, loss: 1.043, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.440, loss: 1.043, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.440, loss: 1.043, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.437, loss: 1.043, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.437, loss: 1.043, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.433, loss: 1.043, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.437, loss: 1.043, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.437, loss: 1.043, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.437, loss: 1.043, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.433, loss: 1.043, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.433, loss: 1.042, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.433, loss: 1.042, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.433, loss: 1.042, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.433, loss: 1.042, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.433, loss: 1.042, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.433, loss: 1.042, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.433, loss: 1.042, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.433, loss: 1.042, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.433, loss: 1.042, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.433, loss: 1.042, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.433, loss: 1.042, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.433, loss: 1.042, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.433, loss: 1.042, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.433, loss: 1.042, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.433, loss: 1.041, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.433, loss: 1.041, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.433, loss: 1.041, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.433, loss: 1.041, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.433, loss: 1.041, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.437, loss: 1.040, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.437, loss: 1.040, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.440, loss: 1.040, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.437, loss: 1.039, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.433, loss: 1.039, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.440, loss: 1.039, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.440, loss: 1.038, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.437, loss: 1.038, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.437, loss: 1.038, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.437, loss: 1.037, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.437, loss: 1.037, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.437, loss: 1.037, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.437, loss: 1.036, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.437, loss: 1.036, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.440, loss: 1.036, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.443, loss: 1.035, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.447, loss: 1.035, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.450, loss: 1.035, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.450, loss: 1.034, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.457, loss: 1.034, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.457, loss: 1.034, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.460, loss: 1.033, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.463, loss: 1.033, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.463, loss: 1.033, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.460, loss: 1.032, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.463, loss: 1.032, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.463, loss: 1.032, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.463, loss: 1.031, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.463, loss: 1.031, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.463, loss: 1.031, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.467, loss: 1.031, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.467, loss: 1.030, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.467, loss: 1.030, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.463, loss: 1.030, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.457, loss: 1.030, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.453, loss: 1.029, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.453, loss: 1.029, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.453, loss: 1.029, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.453, loss: 1.029, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.453, loss: 1.028, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.457, loss: 1.028, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.457, loss: 1.028, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.460, loss: 1.028, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.463, loss: 1.028, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.460, loss: 1.028, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.457, loss: 1.027, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.457, loss: 1.027, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.457, loss: 1.027, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.457, loss: 1.027, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.457, loss: 1.027, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.457, loss: 1.027, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.460, loss: 1.026, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.463, loss: 1.026, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.460, loss: 1.026, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.460, loss: 1.026, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.460, loss: 1.026, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.460, loss: 1.025, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.460, loss: 1.025, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.460, loss: 1.025, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.457, loss: 1.025, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.457, loss: 1.025, lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3)  # remember this produces 300 samples\n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    " \n",
    "# Create second Dense layer with 64 input features (as we take output \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimiser = Optimiser_SGD(decay=1e-3, momentum=0.8)\n",
    "\n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    "     # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    " \n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(dense2.output, y) \n",
    " \n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimiser.current_learning_rate}') \n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " \n",
    "    # Update weights and biases \n",
    "    optimiser.pre_update_params() \n",
    "    optimiser.update_params(dense1) \n",
    "    optimiser.update_params(dense2) \n",
    "    optimiser.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**:<br>\n",
    "So,instead of decreasing the loss even further, we came down very little. Also,notice that the loss isn't changing much as itertaions increase. Recall the cautions: <br>\n",
    "- The bigger the role that momentum takes in the update, the slower the update can change the direction.\n",
    "- When we set the momentum fraction too high, the model might stop learning at all since the direction of the updates won’t be able to follow the global gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well,I searched on the internet and found that there are multiple reasons that SGD+momentum can perform poorer than Vanilla SGD. You can try hyperparameter tuning. There might be less training data(looks like the case here). <br>\n",
    "Let's try decreasing the momentum to 0.5:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.360, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.377, loss: 1.081, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.433, loss: 1.077, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.430, loss: 1.076, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.420, loss: 1.076, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.407, loss: 1.076, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.393, loss: 1.076, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.397, loss: 1.075, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.393, loss: 1.075, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.393, loss: 1.075, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.397, loss: 1.075, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.393, loss: 1.075, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.400, loss: 1.075, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.407, loss: 1.074, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.407, loss: 1.074, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.403, loss: 1.074, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.403, loss: 1.073, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.400, loss: 1.073, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.393, loss: 1.073, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.393, loss: 1.072, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.390, loss: 1.072, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.393, loss: 1.071, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.393, loss: 1.071, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.390, loss: 1.071, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.390, loss: 1.070, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.393, loss: 1.070, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.387, loss: 1.070, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.387, loss: 1.069, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.393, loss: 1.069, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.413, loss: 1.069, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.420, loss: 1.069, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.430, loss: 1.068, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.433, loss: 1.068, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.430, loss: 1.068, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.433, loss: 1.068, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.437, loss: 1.067, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.437, loss: 1.067, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.440, loss: 1.067, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.437, loss: 1.067, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.440, loss: 1.067, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.440, loss: 1.067, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.440, loss: 1.067, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.440, loss: 1.067, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.440, loss: 1.066, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.440, loss: 1.066, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.437, loss: 1.066, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.440, loss: 1.066, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.443, loss: 1.066, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.440, loss: 1.066, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.437, loss: 1.066, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.440, loss: 1.066, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.437, loss: 1.066, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.440, loss: 1.066, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.440, loss: 1.066, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.440, loss: 1.066, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.440, loss: 1.066, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.440, loss: 1.066, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.443, loss: 1.066, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.440, loss: 1.066, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.443, loss: 1.066, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.443, loss: 1.066, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.443, loss: 1.065, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.443, loss: 1.065, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.443, loss: 1.065, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.443, loss: 1.065, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.443, loss: 1.065, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.443, loss: 1.065, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.443, loss: 1.065, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.443, loss: 1.065, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.443, loss: 1.065, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.443, loss: 1.065, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.443, loss: 1.065, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.443, loss: 1.065, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.443, loss: 1.065, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.443, loss: 1.065, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.443, loss: 1.065, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.443, loss: 1.065, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.447, loss: 1.065, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.447, loss: 1.065, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.447, loss: 1.065, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.447, loss: 1.064, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.447, loss: 1.064, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.447, loss: 1.064, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.447, loss: 1.064, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.447, loss: 1.064, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.447, loss: 1.064, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.447, loss: 1.064, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.447, loss: 1.064, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.447, loss: 1.064, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.447, loss: 1.064, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.447, loss: 1.064, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.447, loss: 1.064, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.440, loss: 1.064, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.440, loss: 1.064, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.437, loss: 1.064, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.437, loss: 1.064, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.437, loss: 1.063, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.437, loss: 1.063, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.437, loss: 1.063, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.437, loss: 1.063, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.437, loss: 1.063, lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3)  # remember this produces 300 samples\n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    " \n",
    "# Create second Dense layer with 64 input features (as we take output \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimiser = Optimiser_SGD(decay=1e-3, momentum=0.5)\n",
    "\n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    "     # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    " \n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(dense2.output, y) \n",
    " \n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimiser.current_learning_rate}') \n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " \n",
    "    # Update weights and biases \n",
    "    optimiser.pre_update_params() \n",
    "    optimiser.update_params(dense1) \n",
    "    optimiser.update_params(dense2) \n",
    "    optimiser.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much of a change in results. <br>\n",
    "**Although it didn't give desired results here but SGD Optimiser with momentum is usually one of 2 main choices for an optimizer in practice next to the Adam optimizer.** <br>\n",
    "But before that the next modification to Stochastic Gradient Descent is **AdaGrad**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AdaGrad,short for adaptive gradient,institutes a per-parameter learning rate rather than a globally-shared rate. The idea here is to normalize updates made to the features.\n",
    "- During the training process, some weights can rise significantly, while others tend to not change by much. It is usually better for weights to not rise too high compared to the other weights, and we’ll talk about this with regularization techniques.\n",
    "- AdaGrad provides a way to normalize parameter updates by keeping a history of previous updates — the bigger the sum of the updates is, in either direction (positive or negative), the smaller updates are made further in training. \n",
    "- This lets less-frequently updated parameters to keep-up with changes, effectively utilizing more neurons for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of AdaGrad can be contained in the following two lines of code:\n",
    "```\n",
    "cache += parm_gradient**2\n",
    "parm_updates = learning_rate*parm_gradient/(sqrt(cache) + eps)\n",
    "```\n",
    "- The cache holds a history of squared gradients, and the parm_updates is a function of the learning rate multiplied by the gradient (basic SGD so far) and then is divided by the square root of the cache plus some epsilon value.\n",
    "- **The division operation performed with a constantly rising \n",
    "cache might also cause the learning to stall as updates become smaller with time, due to the \n",
    "monotonic nature of updates.That’s why this optimizer is not widely used, except for some \n",
    "specific applications.**\n",
    "- The **epsilon** is a hyperparameter(pre-training control knob setting) preventing division by 0. The epsilon value is usually a small value, such as 1e-7​, which we’ll be defaulting to.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
