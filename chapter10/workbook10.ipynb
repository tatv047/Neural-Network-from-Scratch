{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer \n",
    "class Layer_Dense: \n",
    " \n",
    "    # Layer initialization \n",
    "    def __init__(self, n_inputs, n_neurons): \n",
    "        # Initialize weights and biases \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons) \n",
    "        self.biases = np.zeros((1, n_neurons)) \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs): \n",
    "        # Remember input values \n",
    "        self.inputs = inputs \n",
    "        # Calculate output values from inputs, weights and biases \n",
    "        self.output = np.dot(inputs, self.weights) + self.biases \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    "        # Gradients on parameters \n",
    "        self.dweights = np.dot(self.inputs.T, dvalues) \n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True) \n",
    "        # Gradient on values \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T) \n",
    " \n",
    " \n",
    "# ReLU activation \n",
    "class Activation_ReLU: \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs): \n",
    "          # Remember input values \n",
    "        self.inputs = inputs \n",
    "        # Calculate output values from inputs \n",
    "        self.output = np.maximum(0, inputs) \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    "        # Since we need to modify original variable, \n",
    "        # let's make a copy of values first \n",
    "        self.dinputs = dvalues.copy() \n",
    " \n",
    "        # Zero gradient where input values were negative \n",
    "        self.dinputs[self.inputs <= 0] = 0 \n",
    " \n",
    " \n",
    "# Softmax activation \n",
    "class Activation_Softmax: \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs): \n",
    "        # Remember input values \n",
    "        self.inputs = inputs \n",
    " \n",
    "        # Get unnormalized probabilities \n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, \n",
    "                                            keepdims=True)) \n",
    "        # Normalize them for each sample \n",
    "        probabilities = exp_values/np.sum(exp_values, axis=1, \n",
    "                                            keepdims=True) \n",
    " \n",
    "        self.output = probabilities \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    " \n",
    "        # Create uninitialized array \n",
    "        self.dinputs = np.empty_like(dvalues) \n",
    " \n",
    "        # Enumerate outputs and gradients \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)): \n",
    "            # Flatten output array \n",
    "            single_output = single_output.reshape(-1, 1) \n",
    "            # Calculate Jacobian matrix of the output and \n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T) \n",
    "            # Calculate sample-wise gradient \n",
    "            # and add it to the array of sample gradients \n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, \n",
    "                                         single_dvalues) \n",
    " \n",
    " \n",
    "# Common loss class \n",
    "class Loss: \n",
    " \n",
    "    # Calculates the data and regularization losses \n",
    "    # given model output and ground truth values \n",
    "    def calculate(self, output, y): \n",
    " \n",
    "        # Calculate sample losses \n",
    "        sample_losses = self.forward(output, y) \n",
    " \n",
    "        # Calculate mean loss \n",
    "        data_loss = np.mean(sample_losses) \n",
    " \n",
    "        # Return loss \n",
    "        return data_loss \n",
    " \n",
    " \n",
    "# Cross-entropy loss \n",
    "class Loss_CategoricalCrossentropy(Loss): \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, y_pred, y_true): \n",
    " \n",
    "        # Number of samples in a batch \n",
    "        samples = len(y_pred) \n",
    " \n",
    "        # Clip data to prevent division by 0 \n",
    "        # Clip both sides to not drag mean towards any value \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) \n",
    " \n",
    "        # Probabilities for target values - \n",
    "        # only if categorical labels \n",
    "        if len(y_true.shape) == 1: \n",
    "            correct_confidences = y_pred_clipped[ \n",
    "                range(samples), \n",
    "                y_true \n",
    "            ] \n",
    "        # Mask values - only for one-hot encoded labels \n",
    "        elif len(y_true.shape) == 2: \n",
    "            correct_confidences = np.sum( \n",
    "                y_pred_clipped * y_true, \n",
    "                axis=1 \n",
    "            ) \n",
    " \n",
    "        # Losses \n",
    "        negative_log_likelihoods = -np.log(correct_confidences) \n",
    "        return negative_log_likelihoods \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues, y_true): \n",
    " \n",
    "        # Number of samples \n",
    "        samples = len(dvalues) \n",
    "        # Number of labels in every sample \n",
    "        # We'll use the first sample to count them \n",
    "        labels = len(dvalues[0]) \n",
    " \n",
    "        # If labels are sparse, turn them into one-hot vector \n",
    "        if len(y_true.shape) == 1: \n",
    "            y_true = np.eye(labels)[y_true] \n",
    " \n",
    "        # Calculate gradient \n",
    "        self.dinputs = -y_true / dvalues \n",
    "        # Normalize gradient \n",
    "        self.dinputs = self.dinputs/samples \n",
    " \n",
    " \n",
    "# Softmax classifier - combined Softmax activation \n",
    "# and cross-entropy loss for faster backward step \n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy(): \n",
    " \n",
    "    # Creates activation and loss function objects \n",
    "    def __init__(self): \n",
    "        self.activation = Activation_Softmax() \n",
    "        self.loss = Loss_CategoricalCrossentropy() \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs, y_true): \n",
    "        # Output layer's activation function \n",
    "        self.activation.forward(inputs) \n",
    "        # Set the output \n",
    "        self.output = self.activation.output \n",
    "        # Calculate and return loss value \n",
    "        return self.loss.calculate(self.output, y_true) \n",
    "    \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues, y_true): \n",
    " \n",
    "        # Number of samples \n",
    "        samples = len(dvalues) \n",
    " \n",
    "        # If labels are one-hot encoded, \n",
    "        # turn them into discrete values \n",
    "        if len(y_true.shape) == 2: \n",
    "            y_true = np.argmax(y_true, axis=1) \n",
    " \n",
    "        # Copy so we can safely modify \n",
    "        self.dinputs = dvalues.copy() \n",
    "        # Calculate gradient \n",
    "        self.dinputs[range(samples), y_true] -= 1 \n",
    "        # Normalize gradient \n",
    "        self.dinputs = self.dinputs/samples \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimiser_SGD:\n",
    "\n",
    "    # initialise optimiser -set settings\n",
    "    # learning rate ofg 1.0 is deafult for this optimiser\n",
    "    def __init__(self,learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    # update parameters\n",
    "    def update_params(self,layer):\n",
    "        layer.weights += -self.learning_rate*layer.dweights\n",
    "        layer.biases += -self.learning_rate*layer.dbiases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the layer object contains its parameters (weights and biases) and also, at this stage, the \n",
    "gradient that is calculated during backpropagation. We store these in the layer’s properties so that \n",
    "the optimizer can make use of them. In our main neural network code, we’d bring the \n",
    "optimization in after backpropagation. Let’s make a 1x64 densely-connected neural network (1 \n",
    "hidden layer with 64 neurons) and use the same dataset as before: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = spiral_data(samples=100,classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create second Dense layer with 64 input features (as we take output  \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The next step is to create the optimizer’s object: \n",
    "# Create optimizer \n",
    "optimizer = Optimiser_SGD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now peform a forward pass of the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass through this layer\n",
    "dense1.forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a forward pass through activation function \n",
    "# takes the output of first dense layer here \n",
    "activation1.forward(dense1.output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a forward pass through second Dense layer \n",
    "# takes outputs of activation function of first layer as inputs \n",
    "dense2.forward(activation1.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a forward pass through the activation/loss function \n",
    "# takes the output of second dense layer here and returns loss \n",
    "loss = loss_activation.forward(dense2.output, y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.0985943\n"
     ]
    }
   ],
   "source": [
    "# let's print the loss\n",
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.36\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy from output of activation2 and targets \n",
    "# calculate values along first axis \n",
    "predictions = np.argmax(loss_activation.output, axis=1) \n",
    "if len(y.shape) == 2: \n",
    "    y = np.argmax(y, axis=1) \n",
    "accuracy = np.mean(predictions==y) \n",
    "print('acc:', accuracy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do backward pass,which is called backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass\n",
    "loss_activation.backward(loss_activation.output,y)\n",
    "dense2.backward(loss_activation.dinputs)\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the optimiser to update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.update_params(dense1)\n",
    "optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Each full pass through all of the training data is called an epoch.** <br>\n",
    "In most deep learning tasks, a neural network will be trained for multiple epochs, though the ideal scenario would be to \n",
    "have a perfect model with ideal weights and biases after only one epoch.  To add multiple epochs \n",
    "of training into our code, we will initialize our model and run a loop around all the code \n",
    "performing the forward pass, backward pass, and optimization calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    " \n",
    "# Create second Dense layer with 64 input features (as we take output  \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimiser = Optimiser_SGD() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.343, loss: 1.099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100, acc: 0.407, loss: 1.083\n",
      "epoch: 200, acc: 0.397, loss: 1.071\n",
      "epoch: 300, acc: 0.410, loss: 1.070\n",
      "epoch: 400, acc: 0.410, loss: 1.069\n",
      "epoch: 500, acc: 0.413, loss: 1.067\n",
      "epoch: 600, acc: 0.410, loss: 1.064\n",
      "epoch: 700, acc: 0.423, loss: 1.058\n",
      "epoch: 800, acc: 0.450, loss: 1.047\n",
      "epoch: 900, acc: 0.430, loss: 1.050\n",
      "epoch: 1000, acc: 0.427, loss: 1.045\n",
      "epoch: 1100, acc: 0.440, loss: 1.038\n",
      "epoch: 1200, acc: 0.453, loss: 1.029\n",
      "epoch: 1300, acc: 0.400, loss: 1.019\n",
      "epoch: 1400, acc: 0.480, loss: 1.026\n",
      "epoch: 1500, acc: 0.413, loss: 1.003\n",
      "epoch: 1600, acc: 0.397, loss: 0.994\n",
      "epoch: 1700, acc: 0.443, loss: 0.976\n",
      "epoch: 1800, acc: 0.403, loss: 0.995\n",
      "epoch: 1900, acc: 0.463, loss: 0.973\n",
      "epoch: 2000, acc: 0.487, loss: 0.969\n",
      "epoch: 2100, acc: 0.470, loss: 0.956\n",
      "epoch: 2200, acc: 0.497, loss: 0.951\n",
      "epoch: 2300, acc: 0.480, loss: 0.936\n",
      "epoch: 2400, acc: 0.470, loss: 0.915\n",
      "epoch: 2500, acc: 0.493, loss: 0.904\n",
      "epoch: 2600, acc: 0.587, loss: 0.862\n",
      "epoch: 2700, acc: 0.557, loss: 0.821\n",
      "epoch: 2800, acc: 0.543, loss: 0.827\n",
      "epoch: 2900, acc: 0.580, loss: 0.814\n",
      "epoch: 3000, acc: 0.637, loss: 0.801\n",
      "epoch: 3100, acc: 0.650, loss: 0.761\n",
      "epoch: 3200, acc: 0.613, loss: 0.813\n",
      "epoch: 3300, acc: 0.570, loss: 0.728\n",
      "epoch: 3400, acc: 0.667, loss: 0.716\n",
      "epoch: 3500, acc: 0.670, loss: 0.714\n",
      "epoch: 3600, acc: 0.657, loss: 0.755\n",
      "epoch: 3700, acc: 0.673, loss: 0.687\n",
      "epoch: 3800, acc: 0.637, loss: 0.651\n",
      "epoch: 3900, acc: 0.637, loss: 0.647\n",
      "epoch: 4000, acc: 0.660, loss: 0.630\n",
      "epoch: 4100, acc: 0.637, loss: 0.634\n",
      "epoch: 4200, acc: 0.630, loss: 0.632\n",
      "epoch: 4300, acc: 0.653, loss: 0.622\n",
      "epoch: 4400, acc: 0.670, loss: 0.605\n",
      "epoch: 4500, acc: 0.653, loss: 0.612\n",
      "epoch: 4600, acc: 0.677, loss: 0.614\n",
      "epoch: 4700, acc: 0.717, loss: 0.615\n",
      "epoch: 4800, acc: 0.650, loss: 0.611\n",
      "epoch: 4900, acc: 0.670, loss: 0.599\n",
      "epoch: 5000, acc: 0.673, loss: 0.602\n",
      "epoch: 5100, acc: 0.517, loss: 1.290\n",
      "epoch: 5200, acc: 0.687, loss: 0.588\n",
      "epoch: 5300, acc: 0.673, loss: 0.591\n",
      "epoch: 5400, acc: 0.677, loss: 0.586\n",
      "epoch: 5500, acc: 0.683, loss: 0.587\n",
      "epoch: 5600, acc: 0.683, loss: 0.596\n",
      "epoch: 5700, acc: 0.547, loss: 1.120\n",
      "epoch: 5800, acc: 0.673, loss: 0.647\n",
      "epoch: 5900, acc: 0.690, loss: 0.570\n",
      "epoch: 6000, acc: 0.690, loss: 0.569\n",
      "epoch: 6100, acc: 0.693, loss: 0.568\n",
      "epoch: 6200, acc: 0.693, loss: 0.565\n",
      "epoch: 6300, acc: 0.703, loss: 0.571\n",
      "epoch: 6400, acc: 0.537, loss: 1.350\n",
      "epoch: 6500, acc: 0.693, loss: 0.587\n",
      "epoch: 6600, acc: 0.700, loss: 0.583\n",
      "epoch: 6700, acc: 0.703, loss: 0.576\n",
      "epoch: 6800, acc: 0.710, loss: 0.568\n",
      "epoch: 6900, acc: 0.720, loss: 0.568\n",
      "epoch: 7000, acc: 0.700, loss: 0.561\n",
      "epoch: 7100, acc: 0.597, loss: 0.951\n",
      "epoch: 7200, acc: 0.697, loss: 0.602\n",
      "epoch: 7300, acc: 0.720, loss: 0.570\n",
      "epoch: 7400, acc: 0.717, loss: 0.557\n",
      "epoch: 7500, acc: 0.700, loss: 0.540\n",
      "epoch: 7600, acc: 0.703, loss: 0.638\n",
      "epoch: 7700, acc: 0.720, loss: 0.563\n",
      "epoch: 7800, acc: 0.717, loss: 0.556\n",
      "epoch: 7900, acc: 0.713, loss: 0.543\n",
      "epoch: 8000, acc: 0.713, loss: 0.535\n",
      "epoch: 8100, acc: 0.670, loss: 0.669\n",
      "epoch: 8200, acc: 0.723, loss: 0.545\n",
      "epoch: 8300, acc: 0.720, loss: 0.541\n",
      "epoch: 8400, acc: 0.740, loss: 0.507\n",
      "epoch: 8500, acc: 0.723, loss: 0.542\n",
      "epoch: 8600, acc: 0.723, loss: 0.545\n",
      "epoch: 8700, acc: 0.723, loss: 0.531\n",
      "epoch: 8800, acc: 0.727, loss: 0.528\n",
      "epoch: 8900, acc: 0.720, loss: 0.505\n",
      "epoch: 9000, acc: 0.730, loss: 0.520\n",
      "epoch: 9100, acc: 0.743, loss: 0.521\n",
      "epoch: 9200, acc: 0.743, loss: 0.495\n",
      "epoch: 9300, acc: 0.767, loss: 0.503\n",
      "epoch: 9400, acc: 0.720, loss: 0.606\n",
      "epoch: 9500, acc: 0.777, loss: 0.497\n",
      "epoch: 9600, acc: 0.770, loss: 0.503\n",
      "epoch: 9700, acc: 0.777, loss: 0.489\n",
      "epoch: 9800, acc: 0.780, loss: 0.496\n",
      "epoch: 9900, acc: 0.777, loss: 0.474\n",
      "epoch: 10000, acc: 0.787, loss: 0.485\n"
     ]
    }
   ],
   "source": [
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    " \n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}') \n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " \n",
    "    # Update weights and biases \n",
    "    optimiser.update_params(dense1) \n",
    "    optimiser.update_params(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above gives us an update of where we are (epochs), the model’s accuracy, and loss every 100 epochs. We can see consistent improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the learning rate is too small,then small updates to the parameters caused stagnation in the model’s learning — the model got stuck in a local minimum. <br>\n",
    "![](img1.png)\n",
    "-  With our example here, as well as with optimizing full neural networks, we do not know where the global minimum is. How do we know if we’ve reached the global minimum or at least gotten close? <br>\n",
    "The loss function measures how far the model is with its predictions to the real target values, so, as long as the loss value is not 0​ or very close to 0​, and the model stopped learning, we’re at some local minimum.\n",
    "- In reality, we almost never approach a loss of 0​ for various reasons:\n",
    " 1. One reason for this may be imperfect neural network hyperparameters. \n",
    " 2. Another reason for this may be insufficient data. If you did reach a loss of 0 with a neural network, you should find it suspicious: **overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try modifying the learning rate: <br> <br>\n",
    "![](img2.png) <br> <br>\n",
    "This time, the model escaped this local minimum but got stuck at another one. Let’s see one more example after another learning rate change: <br> <br>\n",
    "![](img3.png) <br> <br>\n",
    "This time the model got stuck at a local minimum near the global minimum. The model was able to escape the “deeper” local minimums, so it might be counter-intuitive why it is stuck here. <br>\n",
    "Remember, the model follows the direction of steepest descent of the loss function, no matter how large or slight the descent is. For this reason, we’ll introduce momentum and the other techniques to prevent such situations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum in gradient descent\n",
    "\n",
    "Momentum, in an optimizer, adds to the gradient what, in the physical world, we could call inertia. <br> <br>\n",
    "![](img4.png) <br><br>\n",
    "In the above figure you can see we used a very small learning rate here with a large momentum. The color change from green, \n",
    "through orange to red presents the advancement of the gradient descent process, the steps. We can see that the model achieved the goal and found the global minimum, but this took many steps. <br>\n",
    "**Can this be done better?** <br> <br>\n",
    "![](img5.png) <br> <br>\n",
    "And even better: <br> <br>\n",
    "![](img6.png) <br><br>\n",
    "With these examples, we were able to find the global minimum in about **200, 100, and 50 steps**, respectively, by modifying the learning rate and the momentum. It’s possible to significantly shorten the training time by adjusting the parameters of the optimizer.<br>\n",
    "However, we have to be careful with these hyper-parameter adjustments, as this won’t necessarily always help the model: <br><br>\n",
    "![](img7.png) <br><br>\n",
    "In the above case,the learning rate is set too high, the model might not be able to find the global minimum. Even, at some point, if it does, further adjustments could cause it to jump out of this minimum. The model was “jumping” around some minimum and what this might mean is that we should try to:\n",
    "- lower the learning rate\n",
    "- raise the momentum, or \n",
    "- possibly apply a learning rate decay (lowering the learning rate during training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we set the learning rate far too high: <br><br>\n",
    "![](img8.png) <br><br>\n",
    "In the above situation, the model starts “jumping” around, and moves in what we might observe as random directions. This is an example of “**overshooting**,” with every step — the direction of a change is correct, but the amount of the gradient applied is too large. In an extreme situation, we could cause a **gradient explosion**: <br><br>\n",
    "![](img9.png) <br><br>\n",
    "**Note:** <br><br>\n",
    "A **gradient explosion** is a situation where the parameter updates cause the function’s output to rise instead of fall, and, with each step, the loss value and gradient become larger. At some point, the floating-point variable limitation causes an overflow as it cannot hold values of this size anymore, and the model is no longer able to train.<br>\n",
    "It’s crucial to recognize this situation forming during training, especially for large models, where the training can take days, weeks, or more. It is possible to tune the model’s hyper-parameters in time to save the model and to continue training. <br><br>\n",
    "When we choose the learning rate and the other hyper-parameters correctly, the learning process can be relatively quick:\n",
    "<img src=\"img10.png\" style=\"width: 45%; display: inline-block; margin-right: 5%;\" />\n",
    "<img src=\"img11.png\" style=\"width: 45%; display: inline-block;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The challenge is to choose the hyper-parameters correctly, and it is not always an easy task.<br>\n",
    "**Few tips:**\n",
    "-  It is usually best to start with the optimizer defaults, perform a few steps, and observe the training process when \n",
    "tuning different settings.\n",
    "-  It is not always possible to see meaningful results in a short-enough period of time, and, in this case, it’s good to have the ability to update the optimizer’s settings during training.\n",
    "- How you choose the learning rate, and other hyper-parameters, depends on the model, data, including the amount of data, the parameter initialization method, etc. There is no single, best way to set hyper-parameters, but experience usually helps.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a summary of learning rates — if we plot the loss along an axis of steps: <br> <br>\n",
    "![](img12.png) <br><br>\n",
    "We can see various examples of relative learning rates and what loss will ideally look like as a \n",
    "graph over time (steps) of training. <br> <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing what the learning rate should be to get the most out of your training process isn’t possible, but a good rule is that your initial training will benefit from a larger learning rate to take initial steps faster. If you start with steps that are too small, you might get stuck in a local minimum and be unable to leave it due to not making large enough updates to the parameters. \n",
    "**For example, what if we make the learning rate 0.85 rather than 1.0 with the SGD optimizer?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.320, loss: 1.099\n",
      "epoch: 100, acc: 0.390, loss: 1.095\n",
      "epoch: 200, acc: 0.390, loss: 1.081\n",
      "epoch: 300, acc: 0.387, loss: 1.078\n",
      "epoch: 400, acc: 0.403, loss: 1.078\n",
      "epoch: 500, acc: 0.417, loss: 1.077\n",
      "epoch: 600, acc: 0.423, loss: 1.076\n",
      "epoch: 700, acc: 0.423, loss: 1.074\n",
      "epoch: 800, acc: 0.447, loss: 1.072\n",
      "epoch: 900, acc: 0.447, loss: 1.070\n",
      "epoch: 1000, acc: 0.440, loss: 1.068\n",
      "epoch: 1100, acc: 0.443, loss: 1.065\n",
      "epoch: 1200, acc: 0.467, loss: 1.062\n",
      "epoch: 1300, acc: 0.453, loss: 1.058\n",
      "epoch: 1400, acc: 0.463, loss: 1.053\n",
      "epoch: 1500, acc: 0.477, loss: 1.046\n",
      "epoch: 1600, acc: 0.487, loss: 1.038\n",
      "epoch: 1700, acc: 0.467, loss: 1.029\n",
      "epoch: 1800, acc: 0.437, loss: 1.034\n",
      "epoch: 1900, acc: 0.470, loss: 1.022\n",
      "epoch: 2000, acc: 0.440, loss: 1.031\n",
      "epoch: 2100, acc: 0.453, loss: 1.021\n",
      "epoch: 2200, acc: 0.430, loss: 1.020\n",
      "epoch: 2300, acc: 0.453, loss: 1.023\n",
      "epoch: 2400, acc: 0.517, loss: 1.013\n",
      "epoch: 2500, acc: 0.477, loss: 1.000\n",
      "epoch: 2600, acc: 0.497, loss: 1.004\n",
      "epoch: 2700, acc: 0.493, loss: 1.007\n",
      "epoch: 2800, acc: 0.460, loss: 1.023\n",
      "epoch: 2900, acc: 0.447, loss: 1.006\n",
      "epoch: 3000, acc: 0.487, loss: 1.049\n",
      "epoch: 3100, acc: 0.460, loss: 0.999\n",
      "epoch: 3200, acc: 0.447, loss: 1.013\n",
      "epoch: 3300, acc: 0.447, loss: 0.992\n",
      "epoch: 3400, acc: 0.473, loss: 0.993\n",
      "epoch: 3500, acc: 0.463, loss: 0.993\n",
      "epoch: 3600, acc: 0.443, loss: 1.019\n",
      "epoch: 3700, acc: 0.477, loss: 0.991\n",
      "epoch: 3800, acc: 0.490, loss: 0.998\n",
      "epoch: 3900, acc: 0.507, loss: 0.980\n",
      "epoch: 4000, acc: 0.450, loss: 0.985\n",
      "epoch: 4100, acc: 0.493, loss: 0.973\n",
      "epoch: 4200, acc: 0.500, loss: 0.967\n",
      "epoch: 4300, acc: 0.510, loss: 0.959\n",
      "epoch: 4400, acc: 0.557, loss: 0.945\n",
      "epoch: 4500, acc: 0.497, loss: 0.960\n",
      "epoch: 4600, acc: 0.527, loss: 0.962\n",
      "epoch: 4700, acc: 0.493, loss: 0.949\n",
      "epoch: 4800, acc: 0.477, loss: 0.987\n",
      "epoch: 4900, acc: 0.520, loss: 0.929\n",
      "epoch: 5000, acc: 0.540, loss: 0.974\n",
      "epoch: 5100, acc: 0.533, loss: 0.915\n",
      "epoch: 5200, acc: 0.543, loss: 0.906\n",
      "epoch: 5300, acc: 0.567, loss: 0.882\n",
      "epoch: 5400, acc: 0.553, loss: 0.922\n",
      "epoch: 5500, acc: 0.550, loss: 0.908\n",
      "epoch: 5600, acc: 0.570, loss: 0.902\n",
      "epoch: 5700, acc: 0.567, loss: 0.901\n",
      "epoch: 5800, acc: 0.567, loss: 0.911\n",
      "epoch: 5900, acc: 0.557, loss: 0.906\n",
      "epoch: 6000, acc: 0.553, loss: 0.909\n",
      "epoch: 6100, acc: 0.513, loss: 0.921\n",
      "epoch: 6200, acc: 0.557, loss: 0.871\n",
      "epoch: 6300, acc: 0.547, loss: 0.889\n",
      "epoch: 6400, acc: 0.573, loss: 0.846\n",
      "epoch: 6500, acc: 0.553, loss: 0.955\n",
      "epoch: 6600, acc: 0.557, loss: 0.843\n",
      "epoch: 6700, acc: 0.590, loss: 0.824\n",
      "epoch: 6800, acc: 0.527, loss: 0.905\n",
      "epoch: 6900, acc: 0.587, loss: 0.792\n",
      "epoch: 7000, acc: 0.613, loss: 0.813\n",
      "epoch: 7100, acc: 0.543, loss: 0.839\n",
      "epoch: 7200, acc: 0.610, loss: 0.807\n",
      "epoch: 7300, acc: 0.563, loss: 0.894\n",
      "epoch: 7400, acc: 0.593, loss: 0.779\n",
      "epoch: 7500, acc: 0.557, loss: 0.858\n",
      "epoch: 7600, acc: 0.603, loss: 0.788\n",
      "epoch: 7700, acc: 0.620, loss: 0.863\n",
      "epoch: 7800, acc: 0.630, loss: 0.751\n",
      "epoch: 7900, acc: 0.580, loss: 0.779\n",
      "epoch: 8000, acc: 0.630, loss: 0.748\n",
      "epoch: 8100, acc: 0.650, loss: 0.732\n",
      "epoch: 8200, acc: 0.737, loss: 0.680\n",
      "epoch: 8300, acc: 0.697, loss: 0.693\n",
      "epoch: 8400, acc: 0.547, loss: 1.222\n",
      "epoch: 8500, acc: 0.660, loss: 0.707\n",
      "epoch: 8600, acc: 0.660, loss: 0.760\n",
      "epoch: 8700, acc: 0.683, loss: 0.692\n",
      "epoch: 8800, acc: 0.680, loss: 0.701\n",
      "epoch: 8900, acc: 0.760, loss: 0.613\n",
      "epoch: 9000, acc: 0.737, loss: 0.643\n",
      "epoch: 9100, acc: 0.703, loss: 0.651\n",
      "epoch: 9200, acc: 0.757, loss: 0.612\n",
      "epoch: 9300, acc: 0.727, loss: 0.638\n",
      "epoch: 9400, acc: 0.730, loss: 0.625\n",
      "epoch: 9500, acc: 0.743, loss: 0.624\n",
      "epoch: 9600, acc: 0.760, loss: 0.578\n",
      "epoch: 9700, acc: 0.723, loss: 0.637\n",
      "epoch: 9800, acc: 0.733, loss: 0.622\n",
      "epoch: 9900, acc: 0.753, loss: 0.597\n",
      "epoch: 10000, acc: 0.650, loss: 0.793\n"
     ]
    }
   ],
   "source": [
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimiser = Optimiser_SGD(learning_rate=.85) \n",
    " \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    " \n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(dense2.output, y) \n",
    " \n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}') \n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " \n",
    "    # Update weights and biases \n",
    "    optimiser.update_params(dense1) \n",
    "    optimiser.update_params(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you'll compare with the above case where learning_rate = 1 , we have **less accuracy** and **high loss** .<br><br>\n",
    "So, it is very much possible that we git stuck in a local minima and due to smaller updates,couldn't move out of it. \n",
    "<br><br>\n",
    "**NOTE:** <br>\n",
    "Lower accuracy isn't always associated with higher loss and vice-versa. <br>\n",
    "**Why?** <br>\n",
    "-  Remember, even if we desire the best accuracy out of our model, the optimizer’s task is to decrease loss, not raise accuracy \n",
    "directly. \n",
    "- . Loss is the mean value of all of the sample losses, and some of them could drop significantly, while others might rise just slightly, changing the prediction for them from a correct to an incorrect class at the same time. \n",
    "- This would cause a lower mean loss in general, but also more incorrectly predicted samples, which will, at the same time, lower the accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Learning** <br>\n",
    "In a direct comparison of these two models in training, different learning rates did not show that the lower this learning rate value is, the better. In most cases, we want to start with a larger learning rate and decrease the learning rate over time/steps. <br><br>\n",
    "A commonly-used solution to keep initial updates large and explore various learning rates during \n",
    "training is to implement a **learning rate decay.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of a learning rate decay is to start with a large learning rate, say 1.0 in our case, and \n",
    "then decrease it during training. Let's program a Decay Rate which steadily decays the learning rate per batch or epoch. <br>\n",
    "We are going to update the learning rate decay each step by the reciprocal  of the step count fraction. This fraction is a new **hyperparameter** that we'll add to the optimiser called **learning rate decay**.   <br> <br>\n",
    "How this decaying works is it takes the step and the decaying ratio and \n",
    "multiplies them. The further in training, the bigger the step is, and the bigger result of this \n",
    "multiplication is. We then take its reciprocal (the further in training, the lower the value) and \n",
    "multiply the initial learning rate by it. The added 1​ makes sure that the resulting algorithm never \n",
    "raises the learning rate. <br><br>\n",
    "For example, for the first step, we might divide 1 by the learning rate, 0.001​ for example, which will result in a current learning rate of 1000​. That’s definitely not what we wanted. 1 divided by the 1+fraction ensures that the result, a fraction of the starting learning rate, will always be less than or equal to 1, decreasing over time. That’s the desired result — start with the current learning rate and make it smaller with time. The code for determining the current decay rate: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090909090909091\n"
     ]
    }
   ],
   "source": [
    "start_Learning_rate = 1\n",
    "learning_rate_decay = 0.1\n",
    "step = 1\n",
    "learning_rate = start_Learning_rate*(1./(1 + learning_rate_decay*step))\n",
    "print(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# on step = 20\n",
    "step = 20\n",
    "learning_rate = start_Learning_rate*(1./(1 + learning_rate_decay*step))\n",
    "print(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice 0.1 would be considered very aggressive decay rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9090909090909091\n",
      "0.8333333333333334\n",
      "0.7692307692307692\n",
      "0.7142857142857143\n",
      "0.6666666666666666\n",
      "0.625\n",
      "0.588235294117647\n",
      "0.5555555555555556\n",
      "0.5263157894736842\n",
      "0.5\n",
      "0.47619047619047616\n",
      "0.45454545454545453\n",
      "0.4347826086956522\n",
      "0.41666666666666663\n",
      "0.4\n",
      "0.3846153846153846\n",
      "0.37037037037037035\n",
      "0.35714285714285715\n",
      "0.3448275862068965\n",
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "start_Learning_rate = 1.0\n",
    "learning_rate_decay = 0.1\n",
    "for step in range(21):\n",
    "    learning_rate = start_Learning_rate*(1./(1 + learning_rate_decay*step))\n",
    "    print(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This learning rate decay scheme lowers the learning rate each step using the mentioned formula. \n",
    "Initially, the learning rate drops fast, but the change in the learning rate lowers each step, letting \n",
    "the model sit as close as possible to the minimum. The model needs small updates near the end of \n",
    "training to be able to get as close to this point as possible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD Optimiser\n",
    "class Optimiser_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings, \n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self,learning_rate=1.0,decay = 0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "\n",
    "    \"\"\"This method will update the learning rate if decay is anything other than zero\"\"\"\n",
    "    # call once before any updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate*(1./(1. + self.decay*self.iterations))\n",
    "    \n",
    "    #update parameters\n",
    "    def update_params(self,layer):\n",
    "        layer.weights += -self.current_learning_rate*layer.dweights\n",
    "        layer.biases += -self.current_learning_rate*layer.dbiases\n",
    "\n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our model with a decay rate of (1e-2) i.e. 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.350, loss: 1.099 lr: 1.0\n",
      "epoch: 100, acc: 0.443, loss: 1.072 lr: 0.5025125628140703\n",
      "epoch: 200, acc: 0.470, loss: 1.057 lr: 0.33444816053511706\n",
      "epoch: 300, acc: 0.467, loss: 1.055 lr: 0.2506265664160401\n",
      "epoch: 400, acc: 0.467, loss: 1.054 lr: 0.2004008016032064\n",
      "epoch: 500, acc: 0.470, loss: 1.053 lr: 0.1669449081803005\n",
      "epoch: 600, acc: 0.473, loss: 1.053 lr: 0.14306151645207438\n",
      "epoch: 700, acc: 0.463, loss: 1.053 lr: 0.1251564455569462\n",
      "epoch: 800, acc: 0.463, loss: 1.053 lr: 0.11123470522803114\n",
      "epoch: 900, acc: 0.463, loss: 1.052 lr: 0.10010010010010009\n",
      "epoch: 1000, acc: 0.463, loss: 1.052 lr: 0.09099181073703366\n",
      "epoch: 1100, acc: 0.463, loss: 1.052 lr: 0.08340283569641367\n",
      "epoch: 1200, acc: 0.463, loss: 1.052 lr: 0.07698229407236336\n",
      "epoch: 1300, acc: 0.467, loss: 1.052 lr: 0.07147962830593281\n",
      "epoch: 1400, acc: 0.467, loss: 1.052 lr: 0.066711140760507\n",
      "epoch: 1500, acc: 0.467, loss: 1.052 lr: 0.06253908692933083\n",
      "epoch: 1600, acc: 0.467, loss: 1.052 lr: 0.05885815185403177\n",
      "epoch: 1700, acc: 0.467, loss: 1.052 lr: 0.055586436909394105\n",
      "epoch: 1800, acc: 0.467, loss: 1.052 lr: 0.052659294365455495\n",
      "epoch: 1900, acc: 0.460, loss: 1.052 lr: 0.05002501250625312\n",
      "epoch: 2000, acc: 0.460, loss: 1.052 lr: 0.047641734159123386\n",
      "epoch: 2100, acc: 0.460, loss: 1.052 lr: 0.04547521600727603\n",
      "epoch: 2200, acc: 0.460, loss: 1.052 lr: 0.04349717268377555\n",
      "epoch: 2300, acc: 0.460, loss: 1.052 lr: 0.04168403501458941\n",
      "epoch: 2400, acc: 0.460, loss: 1.052 lr: 0.04001600640256102\n",
      "epoch: 2500, acc: 0.460, loss: 1.052 lr: 0.03847633705271258\n",
      "epoch: 2600, acc: 0.460, loss: 1.052 lr: 0.03705075954057058\n",
      "epoch: 2700, acc: 0.460, loss: 1.052 lr: 0.03572704537334762\n",
      "epoch: 2800, acc: 0.460, loss: 1.052 lr: 0.03449465332873405\n",
      "epoch: 2900, acc: 0.460, loss: 1.052 lr: 0.03334444814938312\n",
      "epoch: 3000, acc: 0.460, loss: 1.052 lr: 0.03226847370119393\n",
      "epoch: 3100, acc: 0.460, loss: 1.052 lr: 0.03125976867771178\n",
      "epoch: 3200, acc: 0.460, loss: 1.052 lr: 0.03031221582297666\n",
      "epoch: 3300, acc: 0.460, loss: 1.052 lr: 0.02942041776993233\n",
      "epoch: 3400, acc: 0.460, loss: 1.052 lr: 0.028579594169762787\n",
      "epoch: 3500, acc: 0.460, loss: 1.052 lr: 0.027785495971103084\n",
      "epoch: 3600, acc: 0.460, loss: 1.052 lr: 0.02703433360367667\n",
      "epoch: 3700, acc: 0.460, loss: 1.052 lr: 0.026322716504343247\n",
      "epoch: 3800, acc: 0.460, loss: 1.052 lr: 0.025647601949217745\n",
      "epoch: 3900, acc: 0.460, loss: 1.052 lr: 0.02500625156289072\n",
      "epoch: 4000, acc: 0.460, loss: 1.052 lr: 0.02439619419370578\n",
      "epoch: 4100, acc: 0.460, loss: 1.052 lr: 0.023815194093831864\n",
      "epoch: 4200, acc: 0.460, loss: 1.052 lr: 0.02326122354035822\n",
      "epoch: 4300, acc: 0.460, loss: 1.052 lr: 0.022732439190725165\n",
      "epoch: 4400, acc: 0.460, loss: 1.052 lr: 0.02222716159146477\n",
      "epoch: 4500, acc: 0.460, loss: 1.052 lr: 0.021743857360295715\n",
      "epoch: 4600, acc: 0.460, loss: 1.052 lr: 0.021281123643328365\n",
      "epoch: 4700, acc: 0.460, loss: 1.052 lr: 0.02083767451552407\n",
      "epoch: 4800, acc: 0.460, loss: 1.052 lr: 0.020412329046744233\n",
      "epoch: 4900, acc: 0.460, loss: 1.052 lr: 0.020004000800160033\n",
      "epoch: 5000, acc: 0.460, loss: 1.052 lr: 0.019611688566385566\n",
      "epoch: 5100, acc: 0.460, loss: 1.052 lr: 0.019234468166955183\n",
      "epoch: 5200, acc: 0.460, loss: 1.052 lr: 0.018871485185884128\n",
      "epoch: 5300, acc: 0.460, loss: 1.052 lr: 0.018521948508983144\n",
      "epoch: 5400, acc: 0.460, loss: 1.052 lr: 0.01818512456810329\n",
      "epoch: 5500, acc: 0.463, loss: 1.052 lr: 0.01786033220217896\n",
      "epoch: 5600, acc: 0.463, loss: 1.051 lr: 0.01754693805930865\n",
      "epoch: 5700, acc: 0.463, loss: 1.051 lr: 0.01724435247456458\n",
      "epoch: 5800, acc: 0.463, loss: 1.051 lr: 0.016952025767079167\n",
      "epoch: 5900, acc: 0.463, loss: 1.051 lr: 0.01666944490748458\n",
      "epoch: 6000, acc: 0.463, loss: 1.051 lr: 0.016396130513198885\n",
      "epoch: 6100, acc: 0.463, loss: 1.051 lr: 0.016131634134537828\n",
      "epoch: 6200, acc: 0.463, loss: 1.051 lr: 0.015875535799333228\n",
      "epoch: 6300, acc: 0.463, loss: 1.051 lr: 0.01562744178777934\n",
      "epoch: 6400, acc: 0.463, loss: 1.051 lr: 0.015386982612709646\n",
      "epoch: 6500, acc: 0.463, loss: 1.051 lr: 0.015153811183512654\n",
      "epoch: 6600, acc: 0.463, loss: 1.051 lr: 0.014927601134497688\n",
      "epoch: 6700, acc: 0.463, loss: 1.051 lr: 0.014708045300779527\n",
      "epoch: 6800, acc: 0.463, loss: 1.051 lr: 0.014494854326714018\n",
      "epoch: 6900, acc: 0.463, loss: 1.051 lr: 0.014287755393627663\n",
      "epoch: 7000, acc: 0.463, loss: 1.051 lr: 0.014086491055078181\n",
      "epoch: 7100, acc: 0.463, loss: 1.051 lr: 0.013890818169190166\n",
      "epoch: 7200, acc: 0.463, loss: 1.051 lr: 0.013700506918755994\n",
      "epoch: 7300, acc: 0.463, loss: 1.051 lr: 0.013515339910798757\n",
      "epoch: 7400, acc: 0.463, loss: 1.051 lr: 0.013335111348179758\n",
      "epoch: 7500, acc: 0.463, loss: 1.051 lr: 0.013159626266614028\n",
      "epoch: 7600, acc: 0.463, loss: 1.051 lr: 0.012988699831146902\n",
      "epoch: 7700, acc: 0.463, loss: 1.051 lr: 0.012822156686754713\n",
      "epoch: 7800, acc: 0.463, loss: 1.051 lr: 0.0126598303582732\n",
      "epoch: 7900, acc: 0.463, loss: 1.051 lr: 0.012501562695336917\n",
      "epoch: 8000, acc: 0.463, loss: 1.051 lr: 0.012347203358439314\n",
      "epoch: 8100, acc: 0.463, loss: 1.051 lr: 0.012196609342602758\n",
      "epoch: 8200, acc: 0.463, loss: 1.051 lr: 0.012049644535486204\n",
      "epoch: 8300, acc: 0.463, loss: 1.051 lr: 0.011906179307060364\n",
      "epoch: 8400, acc: 0.463, loss: 1.051 lr: 0.011766090128250382\n",
      "epoch: 8500, acc: 0.463, loss: 1.051 lr: 0.01162925921618793\n",
      "epoch: 8600, acc: 0.463, loss: 1.051 lr: 0.011495574203931488\n",
      "epoch: 8700, acc: 0.463, loss: 1.051 lr: 0.011364927832708264\n",
      "epoch: 8800, acc: 0.463, loss: 1.051 lr: 0.011237217664906169\n",
      "epoch: 8900, acc: 0.463, loss: 1.051 lr: 0.011112345816201801\n",
      "epoch: 9000, acc: 0.463, loss: 1.051 lr: 0.010990218705352238\n",
      "epoch: 9100, acc: 0.463, loss: 1.051 lr: 0.010870746820306556\n",
      "epoch: 9200, acc: 0.463, loss: 1.051 lr: 0.010753844499408539\n",
      "epoch: 9300, acc: 0.463, loss: 1.051 lr: 0.010639429726566656\n",
      "epoch: 9400, acc: 0.463, loss: 1.051 lr: 0.010527423939362039\n",
      "epoch: 9500, acc: 0.463, loss: 1.051 lr: 0.010417751849150954\n",
      "epoch: 9600, acc: 0.463, loss: 1.051 lr: 0.010310341272296112\n",
      "epoch: 9700, acc: 0.463, loss: 1.051 lr: 0.010205122971731808\n",
      "epoch: 9800, acc: 0.463, loss: 1.051 lr: 0.010102030508132133\n",
      "epoch: 9900, acc: 0.463, loss: 1.051 lr: 0.01000100010001\n",
      "epoch: 10000, acc: 0.463, loss: 1.051 lr: 0.009901970492127933\n"
     ]
    }
   ],
   "source": [
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimiser = Optimiser_SGD(decay=1e-2) \n",
    " \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    " \n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(dense2.output, y) \n",
    " \n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f} ' +\n",
    "              f'lr: {optimiser.current_learning_rate}')\n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " \n",
    "    # Update weights and biases \n",
    "    optimiser.pre_update_params()\n",
    "    optimiser.update_params(dense1) \n",
    "    optimiser.update_params(dense2)\n",
    "    optimiser.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**: \n",
    "Notice that from the last run, you got an even poorer accuracy and higher loss. <br><br>\n",
    "\n",
    "So,**this definitely got stuck somwhere because your learning rate decayed far too quickly and became too small,trapping the model in some local minima. Notice how loss and accuracy stopped changing very much.** <br> <br>\n",
    "\n",
    "We can try decaying slower, try decay = 1e-3 or 0.001 :\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.353, loss: 1.099 lr: 1.0\n",
      "epoch: 100, acc: 0.467, loss: 1.079 lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.450, loss: 1.067 lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.447, loss: 1.065 lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.437, loss: 1.064 lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.430, loss: 1.062 lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.427, loss: 1.061 lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.433, loss: 1.059 lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.453, loss: 1.056 lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.467, loss: 1.054 lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.467, loss: 1.051 lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.463, loss: 1.048 lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.463, loss: 1.045 lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.467, loss: 1.041 lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.470, loss: 1.037 lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.480, loss: 1.032 lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.480, loss: 1.026 lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.490, loss: 1.020 lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.493, loss: 1.014 lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.493, loss: 1.006 lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.507, loss: 0.998 lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.510, loss: 0.989 lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.527, loss: 0.979 lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.530, loss: 0.971 lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.433, loss: 0.976 lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.433, loss: 0.973 lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.427, loss: 0.968 lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.430, loss: 0.963 lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.433, loss: 0.959 lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.440, loss: 0.953 lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.443, loss: 0.948 lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.443, loss: 0.943 lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.450, loss: 0.939 lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.460, loss: 0.936 lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.470, loss: 0.930 lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.480, loss: 0.927 lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.480, loss: 0.923 lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.477, loss: 0.919 lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.483, loss: 0.915 lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.480, loss: 0.911 lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.490, loss: 0.907 lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.503, loss: 0.904 lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.513, loss: 0.900 lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.520, loss: 0.896 lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.527, loss: 0.892 lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.533, loss: 0.888 lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.537, loss: 0.885 lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.537, loss: 0.882 lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.537, loss: 0.879 lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.537, loss: 0.875 lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.547, loss: 0.871 lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.547, loss: 0.868 lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.547, loss: 0.865 lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.553, loss: 0.861 lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.557, loss: 0.857 lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.563, loss: 0.854 lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.563, loss: 0.849 lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.563, loss: 0.846 lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.567, loss: 0.843 lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.567, loss: 0.840 lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.573, loss: 0.836 lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.583, loss: 0.833 lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.587, loss: 0.830 lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.600, loss: 0.827 lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.600, loss: 0.824 lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.600, loss: 0.821 lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.593, loss: 0.817 lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.597, loss: 0.815 lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.597, loss: 0.812 lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.597, loss: 0.808 lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.603, loss: 0.805 lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.603, loss: 0.802 lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.607, loss: 0.799 lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.607, loss: 0.796 lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.613, loss: 0.794 lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.617, loss: 0.791 lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.620, loss: 0.788 lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.617, loss: 0.785 lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.620, loss: 0.782 lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.620, loss: 0.779 lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.627, loss: 0.778 lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.627, loss: 0.775 lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.627, loss: 0.772 lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.633, loss: 0.769 lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.633, loss: 0.765 lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.630, loss: 0.762 lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.630, loss: 0.759 lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.640, loss: 0.755 lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.647, loss: 0.751 lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.647, loss: 0.747 lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.647, loss: 0.743 lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.647, loss: 0.739 lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.650, loss: 0.736 lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.650, loss: 0.732 lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.650, loss: 0.729 lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.647, loss: 0.725 lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.647, loss: 0.722 lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.657, loss: 0.718 lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.657, loss: 0.715 lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.657, loss: 0.711 lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.660, loss: 0.708 lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimiser = Optimiser_SGD(decay=1e-3) \n",
    " \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    " \n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(dense2.output, y) \n",
    " \n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f} ' +\n",
    "              f'lr: {optimiser.current_learning_rate}')\n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " \n",
    "    # Update weights and biases \n",
    "    optimiser.pre_update_params()\n",
    "    optimiser.update_params(dense1) \n",
    "    optimiser.update_params(dense2)\n",
    "    optimiser.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can we get even better results? <br>\n",
    "YES,remember you might think that your starting learning rate is too high. <br><br>\n",
    "\n",
    "**Stochastic Gradient Descent with learning rate decay can do fairly well but is still a fairly basic optimization method that only follows a gradient without any additional logic that could potentially help the model find the global minimum to the loss function. One option for improving the SGD optimizer is to introduce** momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent with Momentum "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentum creates a rolling average of gradients over some number of updates and uses this average with the unique gradient at each step. <br>\n",
    "Another way of understanding this is to imagine a ball going down a hill — even if it finds a small hole or hill, momentum will let it go straight through it towards a lower minimum — the bottom of this hill. This can help in cases where you’re stuck in some local minimum (a hole), bouncing back and forth. With momentum, a model is more likely to pass through local minimums, further decreasing loss. Simply put, momentum may still point towards the global gradient descent direction. <br><br>\n",
    "![](img13.png)<br><br>\n",
    "Recall this above situation from the chapter. With regular updates, the SGD optimizer might determine that the next best step is one that keeps the model in a local minimum. Remember that the gradient descent points towards the current steepest  loss ascent for that step,it's negative towards steepest descent and which may not necessarily follow descent towards global minima. We may wind up with a gradient that points in one direction and then the opposite direction in next update;<br>\n",
    "the gradient could continue to bounce back and forth around a local minimum like this, keeping the optimization \n",
    "of the loss stuck.<br>\n",
    "**Instead, momentum uses the previous update’s direction to influence the next update’s direction, minimizing the chances of bouncing around and getting stuck.** <br>\n",
    "Recall the example below:<br><br>\n",
    "\n",
    "![](img14.png)<br><br>\n",
    "\n",
    "**How to utilise momentum then?** <br><br>\n",
    "\n",
    "Set a parameter between 0 & 1,representing the farction of previous update to retain,and subtracting (adding the negative) our actual gradient,multiplied by the learning rate(like before),from it.<br>\n",
    "The update contains a portion of the gradient from preceding steps as our momentum (direction of previous changes) and only a portion of the current gradient;<br> <br>\n",
    "**Caution:** <br>\n",
    "- The bigger the role that momentum takes in the update, the slower the update can change the direction.\n",
    "- When we set the momentum fraction too high, the model might stop learning at all since the direction of the updates won’t be able to follow the global gradient descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code update is as follows:<br>\n",
    "```\n",
    "weight_updates = self.momentum*layer.weight_momentums - self.current_learning_rates*layer.dweights\n",
    "```\n",
    "self.momentum is a hyperparamter choosen at the start,layer.weight_momentums starts as all zeros and get updated during training.<br>\n",
    "```\n",
    "layer.weight_momentums = weight_updates\n",
    "```\n",
    "**The momentum is always the previous update to the parameters.** <br>\n",
    "The updated SGD Optmiser's class looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD + momentum Optimiser \n",
    "class Optimiser_SGD:\n",
    "\n",
    "    # Initialize optimizer - set settings, \n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self,learning_rate=1.0,decay = 0.,momentum = 0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    \"\"\"This method will update the learning rate if decay is anything other than zero\"\"\"\n",
    "    # call once before any updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate*(1./(1. + self.decay*self.iterations))\n",
    "    \n",
    "    \"\"\"Major changes are in this method wrt vanilla SGD\"\"\"\n",
    "    #update parameters\n",
    "    def update_params(self,layer):\n",
    "\n",
    "        # if we use momentum\n",
    "        if self.momentum:\n",
    "\n",
    "            # If layer does not contain momentum arrays, create them filled with zeros \n",
    "            if not hasattr(layer, 'weight_momentums'): \n",
    "                layer.weight_momentums = np.zeros_like(layer.weights) \n",
    "                # If there is no momentum array for weights \n",
    "                # The array doesn't exist for biases yet either. \n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Build weight updates with momentum - take previous updates multiplied by retain factor and update with \n",
    "            # current gradients\n",
    "            weight_updates = self.momentum*layer.weight_momentums - self.current_learning_rate*layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "\n",
    "            # build bias updates\n",
    "            bias_updates = self.momentum*layer.bias_momentums - self.current_learning_rate*layer.biases\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update) \n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate*layer.dweights\n",
    "            bias_updates = -self.current_learning_rate*layer.dbiases\n",
    "\n",
    "        # Update weights and biases using either \n",
    "        # vanilla or momentum updates \n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s show an example illustrating how adding momentum changes the learning process.<br>\n",
    "Keeping the same starting learning rate (1) and decay (1e-3) from the previous training attempt and using a momentum of 0.5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.317, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.383, loss: 1.077, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.383, loss: 1.076, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.387, loss: 1.075, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.397, loss: 1.070, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.393, loss: 1.066, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.400, loss: 1.062, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.423, loss: 1.058, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.420, loss: 1.055, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.413, loss: 1.053, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.423, loss: 1.052, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.420, loss: 1.051, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.420, loss: 1.051, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.417, loss: 1.050, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.420, loss: 1.050, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.420, loss: 1.049, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.420, loss: 1.049, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.423, loss: 1.048, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.420, loss: 1.048, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.420, loss: 1.048, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.417, loss: 1.047, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.417, loss: 1.047, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.423, loss: 1.047, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.423, loss: 1.047, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.423, loss: 1.046, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.417, loss: 1.046, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.413, loss: 1.046, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.413, loss: 1.046, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.413, loss: 1.046, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.413, loss: 1.046, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.413, loss: 1.046, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.413, loss: 1.045, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.413, loss: 1.045, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.410, loss: 1.045, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.413, loss: 1.045, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.413, loss: 1.045, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.413, loss: 1.045, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.413, loss: 1.045, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.413, loss: 1.045, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.413, loss: 1.045, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.413, loss: 1.045, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.413, loss: 1.044, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.413, loss: 1.044, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.413, loss: 1.044, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.413, loss: 1.044, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.413, loss: 1.044, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.413, loss: 1.044, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.413, loss: 1.044, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.413, loss: 1.044, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.413, loss: 1.044, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.413, loss: 1.044, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.413, loss: 1.044, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.413, loss: 1.044, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.413, loss: 1.043, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.417, loss: 1.043, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.417, loss: 1.043, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.417, loss: 1.043, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.417, loss: 1.043, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.417, loss: 1.043, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.417, loss: 1.043, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.417, loss: 1.043, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.417, loss: 1.043, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.417, loss: 1.043, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.417, loss: 1.043, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.417, loss: 1.043, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.417, loss: 1.043, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.417, loss: 1.043, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.417, loss: 1.042, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.413, loss: 1.042, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.413, loss: 1.042, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.413, loss: 1.042, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.413, loss: 1.042, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.413, loss: 1.042, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.413, loss: 1.042, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.413, loss: 1.042, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.413, loss: 1.042, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.413, loss: 1.042, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.413, loss: 1.042, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.413, loss: 1.042, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.413, loss: 1.042, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.413, loss: 1.042, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.413, loss: 1.042, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.413, loss: 1.042, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.413, loss: 1.042, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.413, loss: 1.042, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.413, loss: 1.041, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.413, loss: 1.041, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.413, loss: 1.041, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.413, loss: 1.041, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.413, loss: 1.041, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.410, loss: 1.041, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.410, loss: 1.041, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.410, loss: 1.041, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.410, loss: 1.041, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.410, loss: 1.041, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.410, loss: 1.041, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.410, loss: 1.041, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.410, loss: 1.041, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.410, loss: 1.041, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.410, loss: 1.041, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.410, loss: 1.041, lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3)  # remember this produces 300 samples\n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    " \n",
    "# Create second Dense layer with 64 input features (as we take output \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimiser = Optimiser_SGD(decay=1e-3, momentum=0.8)\n",
    "\n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    "     # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    " \n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(dense2.output, y) \n",
    " \n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimiser.current_learning_rate}') \n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " \n",
    "    # Update weights and biases \n",
    "    optimiser.pre_update_params() \n",
    "    optimiser.update_params(dense1) \n",
    "    optimiser.update_params(dense2) \n",
    "    optimiser.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**:<br>\n",
    "So,instead of decreasing the loss even further, we came down very little. Also,notice that the loss isn't changing much as itertaions increase. Recall the cautions: <br>\n",
    "- The bigger the role that momentum takes in the update, the slower the update can change the direction.\n",
    "- When we set the momentum fraction too high, the model might stop learning at all since the direction of the updates won’t be able to follow the global gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well,I searched on the internet and found that there are multiple reasons that SGD+momentum can perform poorer than Vanilla SGD. You can try hyperparameter tuning. There might be less training data(looks like the case here). <br>\n",
    "Let's try decreasing the momentum to 0.5:   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.360, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.377, loss: 1.081, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.433, loss: 1.077, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.430, loss: 1.076, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.420, loss: 1.076, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.407, loss: 1.076, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.393, loss: 1.076, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.397, loss: 1.075, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.393, loss: 1.075, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.393, loss: 1.075, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.397, loss: 1.075, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.393, loss: 1.075, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.400, loss: 1.075, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.407, loss: 1.074, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.407, loss: 1.074, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.403, loss: 1.074, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.403, loss: 1.073, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.400, loss: 1.073, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.393, loss: 1.073, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.393, loss: 1.072, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.390, loss: 1.072, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.393, loss: 1.071, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.393, loss: 1.071, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.390, loss: 1.071, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.390, loss: 1.070, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.393, loss: 1.070, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.387, loss: 1.070, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.387, loss: 1.069, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.393, loss: 1.069, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.413, loss: 1.069, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.420, loss: 1.069, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.430, loss: 1.068, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.433, loss: 1.068, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.430, loss: 1.068, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.433, loss: 1.068, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.437, loss: 1.067, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.437, loss: 1.067, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.440, loss: 1.067, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.437, loss: 1.067, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.440, loss: 1.067, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.440, loss: 1.067, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.440, loss: 1.067, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.440, loss: 1.067, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.440, loss: 1.066, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.440, loss: 1.066, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.437, loss: 1.066, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.440, loss: 1.066, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.443, loss: 1.066, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.440, loss: 1.066, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.437, loss: 1.066, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.440, loss: 1.066, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.437, loss: 1.066, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.440, loss: 1.066, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.440, loss: 1.066, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.440, loss: 1.066, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.440, loss: 1.066, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.440, loss: 1.066, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.443, loss: 1.066, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.440, loss: 1.066, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.443, loss: 1.066, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.443, loss: 1.066, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.443, loss: 1.065, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.443, loss: 1.065, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.443, loss: 1.065, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.443, loss: 1.065, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.443, loss: 1.065, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.443, loss: 1.065, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.443, loss: 1.065, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.443, loss: 1.065, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.443, loss: 1.065, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.443, loss: 1.065, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.443, loss: 1.065, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.443, loss: 1.065, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.443, loss: 1.065, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.443, loss: 1.065, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.443, loss: 1.065, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.443, loss: 1.065, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.447, loss: 1.065, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.447, loss: 1.065, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.447, loss: 1.065, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.447, loss: 1.064, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.447, loss: 1.064, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.447, loss: 1.064, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.447, loss: 1.064, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.447, loss: 1.064, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.447, loss: 1.064, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.447, loss: 1.064, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.447, loss: 1.064, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.447, loss: 1.064, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.447, loss: 1.064, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.447, loss: 1.064, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.447, loss: 1.064, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.440, loss: 1.064, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.440, loss: 1.064, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.437, loss: 1.064, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.437, loss: 1.064, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.437, loss: 1.063, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.437, loss: 1.063, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.437, loss: 1.063, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.437, loss: 1.063, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.437, loss: 1.063, lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3)  # remember this produces 300 samples\n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    " \n",
    "# Create second Dense layer with 64 input features (as we take output \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimiser = Optimiser_SGD(decay=1e-3, momentum=0.5)\n",
    "\n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    "     # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    " \n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(dense2.output, y) \n",
    " \n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimiser.current_learning_rate}') \n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " \n",
    "    # Update weights and biases \n",
    "    optimiser.pre_update_params() \n",
    "    optimiser.update_params(dense1) \n",
    "    optimiser.update_params(dense2) \n",
    "    optimiser.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not much of a change in results. <br>\n",
    "**Although it didn't give desired results here but SGD Optimiser with momentum is usually one of 2 main choices for an optimizer in practice next to the Adam optimizer.** <br>\n",
    "But before that the next modification to Stochastic Gradient Descent is **AdaGrad**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- AdaGrad,short for adaptive gradient,institutes a per-parameter learning rate rather than a globally-shared rate. The idea here is to normalize updates made to the features.\n",
    "- During the training process, some weights can rise significantly, while others tend to not change by much. It is usually better for weights to not rise too high compared to the other weights, and we’ll talk about this with regularization techniques.\n",
    "- AdaGrad provides a way to normalize parameter updates by keeping a history of previous updates — the bigger the sum of the updates is, in either direction (positive or negative), the smaller updates are made further in training. \n",
    "- This lets less-frequently updated parameters to keep-up with changes, effectively utilizing more neurons for training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concept of AdaGrad can be contained in the following two lines of code:\n",
    "```\n",
    "cache += parm_gradient**2\n",
    "parm_updates = learning_rate*parm_gradient/(sqrt(cache) + eps)\n",
    "```\n",
    "- The cache holds a history of squared gradients, and the parm_updates is a function of the learning rate multiplied by the gradient (basic SGD so far) and then is divided by the square root of the cache plus some epsilon value.\n",
    "- **The division operation performed with a constantly rising \n",
    "cache might also cause the learning to stall as updates become smaller with time, due to the \n",
    "monotonic nature of updates.That’s why this optimizer is not widely used, except for some \n",
    "specific applications.**\n",
    "- The **epsilon** is a hyperparameter(pre-training control knob setting) preventing division by 0. The epsilon value is usually a small value, such as 1e-7​, which we’ll be defaulting to.\n",
    "- Overall, the impact is the learning rates for parameters with smaller gradients are decreased slowly, while the parameters with larger gradients have their learning rates decreased faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement AdaGrad, we start by copying and pasting our SGD optimizer class, changing the \n",
    "name, adding a property for epsilon with a default of 1e-7 to the __init__ method, and \n",
    "removing the momentum. Next, inside the update_params method, we’ll replace the \n",
    "momentum code,full code for AdaGrad Optmiser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimiser_Adagrad:\n",
    "\n",
    "    # Initialize optimizer - set settings, \n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self,learning_rate=1.0,decay = 0.,epsilon = 1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    \n",
    "    # call once before any updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate*(1./(1. + self.decay*self.iterations))\n",
    "    \n",
    "    #update parameters\n",
    "    def update_params(self,layer):\n",
    "\n",
    "        # If layer does not contain cache arrays, create them filled with zeros \n",
    "        if not hasattr(layer, 'weight_cache'): \n",
    "            layer.weight_cache = np.zeros_like(layer.weights)  \n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients \n",
    "        layer.weight_cache += layer.dweights**2 \n",
    "        layer.bias_cache += layer.dbiases**2 \n",
    "\n",
    "        # Vanilla SGD parameter update + normalization \n",
    "        # with square rooted cache \n",
    "        layer.weights += -self.current_learning_rate*layer.dweights/(np.sqrt(layer.weight_cache) +  self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate*layer.dbiases/(np.sqrt(layer.bias_cache) +  self.epsilon)\n",
    "\n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing this optimizer now with decaying set to 1e-3,1e-4​ as well as 1e-5​ works better than 1e-3(with only learning rate decay)​, which we have used previously. This optimizer with our dataset works better with lesser decaying:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.350, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.533, loss: 0.938, lr: 0.9099181073703367\n",
      "epoch: 200, acc: 0.587, loss: 0.863, lr: 0.8340283569641367\n",
      "epoch: 300, acc: 0.663, loss: 0.806, lr: 0.7698229407236336\n",
      "epoch: 400, acc: 0.663, loss: 0.755, lr: 0.7147962830593281\n",
      "epoch: 500, acc: 0.687, loss: 0.713, lr: 0.66711140760507\n",
      "epoch: 600, acc: 0.697, loss: 0.677, lr: 0.6253908692933083\n",
      "epoch: 700, acc: 0.713, loss: 0.647, lr: 0.5885815185403178\n",
      "epoch: 800, acc: 0.730, loss: 0.621, lr: 0.5558643690939411\n",
      "epoch: 900, acc: 0.740, loss: 0.599, lr: 0.526592943654555\n",
      "epoch: 1000, acc: 0.750, loss: 0.580, lr: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.753, loss: 0.565, lr: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.753, loss: 0.552, lr: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.763, loss: 0.541, lr: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.767, loss: 0.531, lr: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.770, loss: 0.523, lr: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.770, loss: 0.516, lr: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.773, loss: 0.509, lr: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.777, loss: 0.503, lr: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.773, loss: 0.497, lr: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.783, loss: 0.491, lr: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.783, loss: 0.486, lr: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.787, loss: 0.482, lr: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.790, loss: 0.478, lr: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.790, loss: 0.474, lr: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.790, loss: 0.471, lr: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.790, loss: 0.467, lr: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.787, loss: 0.464, lr: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.787, loss: 0.461, lr: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.793, loss: 0.458, lr: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.797, loss: 0.455, lr: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.797, loss: 0.453, lr: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.803, loss: 0.450, lr: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.797, loss: 0.448, lr: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.803, loss: 0.446, lr: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.810, loss: 0.444, lr: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.813, loss: 0.442, lr: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.810, loss: 0.436, lr: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.817, loss: 0.433, lr: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.817, loss: 0.430, lr: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.820, loss: 0.427, lr: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.823, loss: 0.425, lr: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.833, loss: 0.423, lr: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.833, loss: 0.421, lr: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.833, loss: 0.419, lr: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.837, loss: 0.417, lr: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.837, loss: 0.415, lr: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.837, loss: 0.413, lr: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.837, loss: 0.412, lr: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.837, loss: 0.410, lr: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.837, loss: 0.408, lr: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.840, loss: 0.407, lr: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.840, loss: 0.406, lr: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.840, loss: 0.404, lr: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.840, loss: 0.403, lr: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.843, loss: 0.401, lr: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.837, loss: 0.400, lr: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.833, loss: 0.398, lr: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.837, loss: 0.397, lr: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.837, loss: 0.396, lr: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.837, loss: 0.395, lr: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.837, loss: 0.394, lr: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.837, loss: 0.393, lr: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.840, loss: 0.392, lr: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.840, loss: 0.391, lr: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.840, loss: 0.390, lr: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.840, loss: 0.389, lr: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.840, loss: 0.388, lr: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.840, loss: 0.387, lr: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.840, loss: 0.386, lr: 0.126598303582732\n",
      "epoch: 7000, acc: 0.840, loss: 0.385, lr: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.840, loss: 0.385, lr: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.843, loss: 0.383, lr: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.843, loss: 0.380, lr: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.843, loss: 0.379, lr: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.843, loss: 0.378, lr: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.843, loss: 0.377, lr: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.843, loss: 0.376, lr: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.843, loss: 0.375, lr: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.843, loss: 0.375, lr: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.843, loss: 0.374, lr: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.843, loss: 0.373, lr: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.843, loss: 0.372, lr: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.843, loss: 0.371, lr: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.843, loss: 0.369, lr: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.847, loss: 0.368, lr: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.847, loss: 0.368, lr: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.847, loss: 0.367, lr: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.847, loss: 0.366, lr: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.847, loss: 0.365, lr: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.847, loss: 0.365, lr: 0.1000100010001\n",
      "epoch: 9100, acc: 0.847, loss: 0.364, lr: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.847, loss: 0.363, lr: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.850, loss: 0.363, lr: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.850, loss: 0.362, lr: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.850, loss: 0.361, lr: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.850, loss: 0.361, lr: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.850, loss: 0.360, lr: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.850, loss: 0.360, lr: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.850, loss: 0.359, lr: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.850, loss: 0.359, lr: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    " \n",
    "# Create second Dense layer with 64 input features (as we take output \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "#optimiser = Optimiser_SGD(decay=8e-8, momentum=0.9)\n",
    "\n",
    "optimiser = Optimiser_Adagrad(decay=1e-3) \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    " \n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(dense2.output, y) \n",
    " \n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimiser.current_learning_rate}') \n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " \n",
    "    # Update weights and biases \n",
    "    optimiser.pre_update_params() \n",
    "    optimiser.update_params(dense1) \n",
    "    optimiser.update_params(dense2) \n",
    "    optimiser.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best results we got so far,loss came down to 0.269 and accuracy is 0.897. <br>\n",
    "Recall when we only used learning rate decay in Vanilla SGD Optmisation,the accuracy was ~ 0.67 above. And this is much better than what SGD + Momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*RMSProp* is short for *Root Mean Square Propagation*. Similar to AdaGrad, RMSProp calculates an adaptive learning rate per parameter; it’s just calculated in a different way than AdaGrad. <br>\n",
    "Where AdaGrad calculates the cache as: \n",
    "```\n",
    "cache += param_gradients**2\n",
    "```\n",
    "RMSProp calculates the cache as:\n",
    "```\n",
    "cache = rho * cache + (1 - rho) * gradient ** 2\n",
    "```\n",
    "- This is similar to both momentum with the SGD optimizer and cache with the AdaGrad. RMSProp adds a mechanism similar to momentum but also adds a per-parameter adaptive learning rate, so the learning rate changes are smoother. This helps to retain the global direction of changes and slows changes in direction. \n",
    "- Instead of continually adding squared gradients to a cache (like in Adagrad), it uses a moving average of the cache. Each update to the cache retains a part of the cache and updates it with a fraction of the new, squared, gradients. In this way, cache \n",
    "contents “move” with data in time, and learning does not stall.\n",
    "- In the case of this optimizer, the per-parameter learning rate can either fall or rise, depending on the last updates and current \n",
    "gradient. RMSProp applies the cache in the same way as AdaGrad does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** <BR>\n",
    "The new hyperparameter here is **rho**​. **Rho**​ is the **cache memory decay rate**. Because this optimizer, \n",
    "with default values, carries over so much momentum of gradient and the adaptive learning rate \n",
    "updates, even small gradient updates are enough to keep it going; therefore, a default learning rate \n",
    "of 1​ is far too large and causes instant model instability. A learning rate that becomes stable again \n",
    "and gives fast enough updates is around 0.001​ (that’s also the default value for this optimizer used \n",
    "in well-known machine learning frameworks). That’s what we’ll use as default from now on too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimiser_RMSProp:\n",
    "\n",
    "    # Initialize optimizer - set settings, \n",
    "    # learning rate of 1. is default for this optimizer\n",
    "    def __init__(self,learning_rate=0.001,decay = 0.,epsilon = 1e-7,rho = 0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        self.rho = rho\n",
    "\n",
    "    \n",
    "    # call once before any updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate*(1./(1. + self.decay*self.iterations))\n",
    "    \n",
    "    #update parameters\n",
    "    def update_params(self,layer):\n",
    "\n",
    "        # If layer does not contain cache arrays, create them filled with zeros \n",
    "        if not hasattr(layer, 'weight_cache'): \n",
    "            layer.weight_cache = np.zeros_like(layer.weights)  \n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Update cache with squared current gradients \n",
    "        layer.weight_cache += self.rho*layer.weight_cache + (1 - self.rho)*layer.dweights**2 \n",
    "        layer.bias_cache += self.rho*layer.bias_cache + (1 - self.rho)*layer.dbiases**2 \n",
    "\n",
    "        # Vanilla SGD parameter update + normalization \n",
    "        # with square rooted cache \n",
    "        layer.weights += -self.current_learning_rate*layer.dweights/(np.sqrt(layer.weight_cache) +  self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate*layer.dbiases/(np.sqrt(layer.bias_cache) +  self.epsilon)\n",
    "\n",
    "    # call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running our test again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.297, loss: 1.099, lr: 0.001\n",
      "epoch: 100, acc: 0.417, loss: 1.098, lr: 0.0009099181073703368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_18736\\750303747.py:29: RuntimeWarning: overflow encountered in add\n",
      "  layer.bias_cache += self.rho*layer.bias_cache + (1 - self.rho)*layer.dbiases**2\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_18736\\750303747.py:28: RuntimeWarning: overflow encountered in add\n",
      "  layer.weight_cache += self.rho*layer.weight_cache + (1 - self.rho)*layer.dweights**2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 200, acc: 0.417, loss: 1.098, lr: 0.0008340283569641367\n",
      "epoch: 300, acc: 0.417, loss: 1.098, lr: 0.0007698229407236335\n",
      "epoch: 400, acc: 0.417, loss: 1.098, lr: 0.0007147962830593281\n",
      "epoch: 500, acc: 0.417, loss: 1.098, lr: 0.0006671114076050701\n",
      "epoch: 600, acc: 0.417, loss: 1.098, lr: 0.0006253908692933083\n",
      "epoch: 700, acc: 0.417, loss: 1.098, lr: 0.0005885815185403178\n",
      "epoch: 800, acc: 0.417, loss: 1.098, lr: 0.0005558643690939412\n",
      "epoch: 900, acc: 0.417, loss: 1.098, lr: 0.000526592943654555\n",
      "epoch: 1000, acc: 0.417, loss: 1.098, lr: 0.0005002501250625312\n",
      "epoch: 1100, acc: 0.417, loss: 1.098, lr: 0.0004764173415912339\n",
      "epoch: 1200, acc: 0.417, loss: 1.098, lr: 0.0004547521600727604\n",
      "epoch: 1300, acc: 0.417, loss: 1.098, lr: 0.00043497172683775554\n",
      "epoch: 1400, acc: 0.417, loss: 1.098, lr: 0.00041684035014589413\n",
      "epoch: 1500, acc: 0.417, loss: 1.098, lr: 0.0004001600640256102\n",
      "epoch: 1600, acc: 0.417, loss: 1.098, lr: 0.0003847633705271258\n",
      "epoch: 1700, acc: 0.417, loss: 1.098, lr: 0.0003705075954057058\n",
      "epoch: 1800, acc: 0.417, loss: 1.098, lr: 0.0003572704537334763\n",
      "epoch: 1900, acc: 0.417, loss: 1.098, lr: 0.0003449465332873405\n",
      "epoch: 2000, acc: 0.417, loss: 1.098, lr: 0.00033344448149383126\n",
      "epoch: 2100, acc: 0.417, loss: 1.098, lr: 0.00032268473701193933\n",
      "epoch: 2200, acc: 0.417, loss: 1.098, lr: 0.0003125976867771179\n",
      "epoch: 2300, acc: 0.417, loss: 1.098, lr: 0.00030312215822976665\n",
      "epoch: 2400, acc: 0.417, loss: 1.098, lr: 0.00029420417769932336\n",
      "epoch: 2500, acc: 0.417, loss: 1.098, lr: 0.0002857959416976279\n",
      "epoch: 2600, acc: 0.417, loss: 1.098, lr: 0.0002778549597110308\n",
      "epoch: 2700, acc: 0.417, loss: 1.098, lr: 0.0002703433360367667\n",
      "epoch: 2800, acc: 0.417, loss: 1.098, lr: 0.00026322716504343247\n",
      "epoch: 2900, acc: 0.417, loss: 1.098, lr: 0.00025647601949217746\n",
      "epoch: 3000, acc: 0.417, loss: 1.098, lr: 0.00025006251562890725\n",
      "epoch: 3100, acc: 0.417, loss: 1.098, lr: 0.00024396194193705782\n",
      "epoch: 3200, acc: 0.417, loss: 1.098, lr: 0.00023815194093831867\n",
      "epoch: 3300, acc: 0.417, loss: 1.098, lr: 0.00023261223540358225\n",
      "epoch: 3400, acc: 0.417, loss: 1.098, lr: 0.00022732439190725165\n",
      "epoch: 3500, acc: 0.417, loss: 1.098, lr: 0.00022227161591464767\n",
      "epoch: 3600, acc: 0.417, loss: 1.098, lr: 0.00021743857360295715\n",
      "epoch: 3700, acc: 0.417, loss: 1.098, lr: 0.00021281123643328368\n",
      "epoch: 3800, acc: 0.417, loss: 1.098, lr: 0.00020837674515524068\n",
      "epoch: 3900, acc: 0.417, loss: 1.098, lr: 0.00020412329046744235\n",
      "epoch: 4000, acc: 0.417, loss: 1.098, lr: 0.0002000400080016003\n",
      "epoch: 4100, acc: 0.417, loss: 1.098, lr: 0.00019611688566385565\n",
      "epoch: 4200, acc: 0.417, loss: 1.098, lr: 0.00019234468166955186\n",
      "epoch: 4300, acc: 0.417, loss: 1.098, lr: 0.00018871485185884126\n",
      "epoch: 4400, acc: 0.417, loss: 1.098, lr: 0.00018521948508983145\n",
      "epoch: 4500, acc: 0.417, loss: 1.098, lr: 0.00018185124568103294\n",
      "epoch: 4600, acc: 0.417, loss: 1.098, lr: 0.0001786033220217896\n",
      "epoch: 4700, acc: 0.417, loss: 1.098, lr: 0.00017546938059308652\n",
      "epoch: 4800, acc: 0.417, loss: 1.098, lr: 0.0001724435247456458\n",
      "epoch: 4900, acc: 0.417, loss: 1.098, lr: 0.00016952025767079165\n",
      "epoch: 5000, acc: 0.417, loss: 1.098, lr: 0.00016669444907484583\n",
      "epoch: 5100, acc: 0.417, loss: 1.098, lr: 0.00016396130513198883\n",
      "epoch: 5200, acc: 0.417, loss: 1.098, lr: 0.00016131634134537829\n",
      "epoch: 5300, acc: 0.417, loss: 1.098, lr: 0.00015875535799333228\n",
      "epoch: 5400, acc: 0.417, loss: 1.098, lr: 0.0001562744178777934\n",
      "epoch: 5500, acc: 0.417, loss: 1.098, lr: 0.00015386982612709647\n",
      "epoch: 5600, acc: 0.417, loss: 1.098, lr: 0.00015153811183512653\n",
      "epoch: 5700, acc: 0.417, loss: 1.098, lr: 0.00014927601134497688\n",
      "epoch: 5800, acc: 0.417, loss: 1.098, lr: 0.00014708045300779526\n",
      "epoch: 5900, acc: 0.417, loss: 1.098, lr: 0.00014494854326714017\n",
      "epoch: 6000, acc: 0.417, loss: 1.098, lr: 0.00014287755393627662\n",
      "epoch: 6100, acc: 0.417, loss: 1.098, lr: 0.00014086491055078179\n",
      "epoch: 6200, acc: 0.417, loss: 1.098, lr: 0.00013890818169190168\n",
      "epoch: 6300, acc: 0.417, loss: 1.098, lr: 0.00013700506918755992\n",
      "epoch: 6400, acc: 0.417, loss: 1.098, lr: 0.00013515339910798757\n",
      "epoch: 6500, acc: 0.417, loss: 1.098, lr: 0.00013335111348179756\n",
      "epoch: 6600, acc: 0.417, loss: 1.098, lr: 0.00013159626266614027\n",
      "epoch: 6700, acc: 0.417, loss: 1.098, lr: 0.000129886998311469\n",
      "epoch: 6800, acc: 0.417, loss: 1.098, lr: 0.00012822156686754713\n",
      "epoch: 6900, acc: 0.417, loss: 1.098, lr: 0.000126598303582732\n",
      "epoch: 7000, acc: 0.417, loss: 1.098, lr: 0.00012501562695336915\n",
      "epoch: 7100, acc: 0.417, loss: 1.098, lr: 0.00012347203358439312\n",
      "epoch: 7200, acc: 0.417, loss: 1.098, lr: 0.00012196609342602758\n",
      "epoch: 7300, acc: 0.417, loss: 1.098, lr: 0.00012049644535486204\n",
      "epoch: 7400, acc: 0.417, loss: 1.098, lr: 0.00011906179307060363\n",
      "epoch: 7500, acc: 0.417, loss: 1.098, lr: 0.00011766090128250381\n",
      "epoch: 7600, acc: 0.417, loss: 1.098, lr: 0.0001162925921618793\n",
      "epoch: 7700, acc: 0.417, loss: 1.098, lr: 0.00011495574203931487\n",
      "epoch: 7800, acc: 0.417, loss: 1.098, lr: 0.00011364927832708264\n",
      "epoch: 7900, acc: 0.417, loss: 1.098, lr: 0.00011237217664906169\n",
      "epoch: 8000, acc: 0.417, loss: 1.098, lr: 0.000111123458162018\n",
      "epoch: 8100, acc: 0.417, loss: 1.098, lr: 0.00010990218705352236\n",
      "epoch: 8200, acc: 0.417, loss: 1.098, lr: 0.00010870746820306555\n",
      "epoch: 8300, acc: 0.417, loss: 1.098, lr: 0.0001075384449940854\n",
      "epoch: 8400, acc: 0.417, loss: 1.098, lr: 0.00010639429726566655\n",
      "epoch: 8500, acc: 0.417, loss: 1.098, lr: 0.00010527423939362037\n",
      "epoch: 8600, acc: 0.417, loss: 1.098, lr: 0.00010417751849150952\n",
      "epoch: 8700, acc: 0.417, loss: 1.098, lr: 0.00010310341272296113\n",
      "epoch: 8800, acc: 0.417, loss: 1.098, lr: 0.0001020512297173181\n",
      "epoch: 8900, acc: 0.417, loss: 1.098, lr: 0.00010102030508132135\n",
      "epoch: 9000, acc: 0.417, loss: 1.098, lr: 0.00010001000100010001\n",
      "epoch: 9100, acc: 0.417, loss: 1.098, lr: 9.901970492127933e-05\n",
      "epoch: 9200, acc: 0.417, loss: 1.098, lr: 9.804882831650162e-05\n",
      "epoch: 9300, acc: 0.417, loss: 1.098, lr: 9.709680551509856e-05\n",
      "epoch: 9400, acc: 0.417, loss: 1.098, lr: 9.616309260505818e-05\n",
      "epoch: 9500, acc: 0.417, loss: 1.098, lr: 9.524716639679969e-05\n",
      "epoch: 9600, acc: 0.417, loss: 1.098, lr: 9.434852344560806e-05\n",
      "epoch: 9700, acc: 0.417, loss: 1.098, lr: 9.346667912889055e-05\n",
      "epoch: 9800, acc: 0.417, loss: 1.098, lr: 9.260116677470138e-05\n",
      "epoch: 9900, acc: 0.417, loss: 1.098, lr: 9.175153683824203e-05\n",
      "epoch: 10000, acc: 0.417, loss: 1.098, lr: 9.091735612328394e-05\n"
     ]
    }
   ],
   "source": [
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    " \n",
    "# Create second Dense layer with 64 input features (as we take output \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "#optimiser = Optimiser_SGD(decay=8e-8, momentum=0.9)\n",
    "\n",
    "optimiser = Optimiser_RMSProp(decay=1e-3) \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    " \n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(dense2.output, y) \n",
    " \n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimiser.current_learning_rate}') \n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " \n",
    "    # Update weights and biases \n",
    "    optimiser.pre_update_params() \n",
    "    optimiser.update_params(dense1) \n",
    "    optimiser.update_params(dense2) \n",
    "    optimiser.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results aren't great,looks like we are stuck in some local minima,with no change in loss or accuracy over epochs. Tweaking with parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.390, loss: 1.099, lr: 0.02\n",
      "epoch: 100, acc: 0.437, loss: 1.066, lr: 0.018198362147406735\n",
      "epoch: 200, acc: 0.437, loss: 1.066, lr: 0.016680567139282735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_18736\\750303747.py:29: RuntimeWarning: overflow encountered in add\n",
      "  layer.bias_cache += self.rho*layer.bias_cache + (1 - self.rho)*layer.dbiases**2\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_18736\\750303747.py:28: RuntimeWarning: overflow encountered in add\n",
      "  layer.weight_cache += self.rho*layer.weight_cache + (1 - self.rho)*layer.dweights**2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 300, acc: 0.437, loss: 1.066, lr: 0.015396458814472672\n",
      "epoch: 400, acc: 0.437, loss: 1.066, lr: 0.014295925661186561\n",
      "epoch: 500, acc: 0.437, loss: 1.066, lr: 0.0133422281521014\n",
      "epoch: 600, acc: 0.437, loss: 1.066, lr: 0.012507817385866166\n",
      "epoch: 700, acc: 0.437, loss: 1.066, lr: 0.011771630370806356\n",
      "epoch: 800, acc: 0.437, loss: 1.066, lr: 0.011117287381878822\n",
      "epoch: 900, acc: 0.437, loss: 1.066, lr: 0.010531858873091101\n",
      "epoch: 1000, acc: 0.437, loss: 1.066, lr: 0.010005002501250623\n",
      "epoch: 1100, acc: 0.437, loss: 1.066, lr: 0.009528346831824679\n",
      "epoch: 1200, acc: 0.437, loss: 1.066, lr: 0.009095043201455207\n",
      "epoch: 1300, acc: 0.437, loss: 1.066, lr: 0.008699434536755112\n",
      "epoch: 1400, acc: 0.437, loss: 1.066, lr: 0.008336807002917883\n",
      "epoch: 1500, acc: 0.437, loss: 1.066, lr: 0.008003201280512205\n",
      "epoch: 1600, acc: 0.437, loss: 1.066, lr: 0.007695267410542516\n",
      "epoch: 1700, acc: 0.437, loss: 1.066, lr: 0.007410151908114116\n",
      "epoch: 1800, acc: 0.437, loss: 1.066, lr: 0.007145409074669526\n",
      "epoch: 1900, acc: 0.437, loss: 1.066, lr: 0.006898930665746809\n",
      "epoch: 2000, acc: 0.437, loss: 1.066, lr: 0.006668889629876626\n",
      "epoch: 2100, acc: 0.437, loss: 1.066, lr: 0.006453694740238787\n",
      "epoch: 2200, acc: 0.437, loss: 1.066, lr: 0.006251953735542357\n",
      "epoch: 2300, acc: 0.437, loss: 1.066, lr: 0.006062443164595333\n",
      "epoch: 2400, acc: 0.437, loss: 1.066, lr: 0.005884083553986467\n",
      "epoch: 2500, acc: 0.437, loss: 1.066, lr: 0.0057159188339525584\n",
      "epoch: 2600, acc: 0.437, loss: 1.066, lr: 0.005557099194220616\n",
      "epoch: 2700, acc: 0.437, loss: 1.066, lr: 0.005406866720735334\n",
      "epoch: 2800, acc: 0.437, loss: 1.066, lr: 0.0052645433008686494\n",
      "epoch: 2900, acc: 0.437, loss: 1.066, lr: 0.005129520389843549\n",
      "epoch: 3000, acc: 0.437, loss: 1.066, lr: 0.005001250312578145\n",
      "epoch: 3100, acc: 0.437, loss: 1.066, lr: 0.0048792388387411565\n",
      "epoch: 3200, acc: 0.437, loss: 1.066, lr: 0.004763038818766373\n",
      "epoch: 3300, acc: 0.437, loss: 1.066, lr: 0.004652244708071645\n",
      "epoch: 3400, acc: 0.437, loss: 1.066, lr: 0.0045464878381450335\n",
      "epoch: 3500, acc: 0.437, loss: 1.066, lr: 0.004445432318292954\n",
      "epoch: 3600, acc: 0.437, loss: 1.066, lr: 0.004348771472059143\n",
      "epoch: 3700, acc: 0.437, loss: 1.066, lr: 0.004256224728665673\n",
      "epoch: 3800, acc: 0.437, loss: 1.066, lr: 0.004167534903104814\n",
      "epoch: 3900, acc: 0.437, loss: 1.066, lr: 0.004082465809348847\n",
      "epoch: 4000, acc: 0.437, loss: 1.066, lr: 0.0040008001600320055\n",
      "epoch: 4100, acc: 0.437, loss: 1.066, lr: 0.003922337713277113\n",
      "epoch: 4200, acc: 0.437, loss: 1.066, lr: 0.003846893633391037\n",
      "epoch: 4300, acc: 0.437, loss: 1.066, lr: 0.0037742970371768252\n",
      "epoch: 4400, acc: 0.437, loss: 1.066, lr: 0.003704389701796629\n",
      "epoch: 4500, acc: 0.437, loss: 1.066, lr: 0.0036370249136206583\n",
      "epoch: 4600, acc: 0.437, loss: 1.066, lr: 0.003572066440435792\n",
      "epoch: 4700, acc: 0.437, loss: 1.066, lr: 0.00350938761186173\n",
      "epoch: 4800, acc: 0.437, loss: 1.066, lr: 0.003448870494912916\n",
      "epoch: 4900, acc: 0.437, loss: 1.066, lr: 0.003390405153415833\n",
      "epoch: 5000, acc: 0.437, loss: 1.066, lr: 0.0033338889814969164\n",
      "epoch: 5100, acc: 0.437, loss: 1.066, lr: 0.003279226102639777\n",
      "epoch: 5200, acc: 0.437, loss: 1.066, lr: 0.0032263268269075657\n",
      "epoch: 5300, acc: 0.437, loss: 1.066, lr: 0.0031751071598666455\n",
      "epoch: 5400, acc: 0.437, loss: 1.066, lr: 0.0031254883575558678\n",
      "epoch: 5500, acc: 0.437, loss: 1.066, lr: 0.0030773965225419295\n",
      "epoch: 5600, acc: 0.437, loss: 1.066, lr: 0.0030307622367025306\n",
      "epoch: 5700, acc: 0.437, loss: 1.066, lr: 0.0029855202268995375\n",
      "epoch: 5800, acc: 0.437, loss: 1.066, lr: 0.0029416090601559054\n",
      "epoch: 5900, acc: 0.437, loss: 1.066, lr: 0.0028989708653428033\n",
      "epoch: 6000, acc: 0.437, loss: 1.066, lr: 0.002857551078725532\n",
      "epoch: 6100, acc: 0.437, loss: 1.066, lr: 0.0028172982110156357\n",
      "epoch: 6200, acc: 0.437, loss: 1.066, lr: 0.0027781636338380334\n",
      "epoch: 6300, acc: 0.437, loss: 1.066, lr: 0.0027401013837511983\n",
      "epoch: 6400, acc: 0.437, loss: 1.066, lr: 0.0027030679821597515\n",
      "epoch: 6500, acc: 0.437, loss: 1.066, lr: 0.0026670222696359514\n",
      "epoch: 6600, acc: 0.437, loss: 1.066, lr: 0.0026319252533228057\n",
      "epoch: 6700, acc: 0.437, loss: 1.066, lr: 0.0025977399662293803\n",
      "epoch: 6800, acc: 0.437, loss: 1.066, lr: 0.0025644313373509426\n",
      "epoch: 6900, acc: 0.437, loss: 1.066, lr: 0.00253196607165464\n",
      "epoch: 7000, acc: 0.437, loss: 1.066, lr: 0.002500312539067383\n",
      "epoch: 7100, acc: 0.437, loss: 1.066, lr: 0.0024694406716878627\n",
      "epoch: 7200, acc: 0.437, loss: 1.066, lr: 0.0024393218685205514\n",
      "epoch: 7300, acc: 0.437, loss: 1.066, lr: 0.0024099289070972406\n",
      "epoch: 7400, acc: 0.437, loss: 1.066, lr: 0.0023812358614120725\n",
      "epoch: 7500, acc: 0.437, loss: 1.066, lr: 0.002353218025650076\n",
      "epoch: 7600, acc: 0.437, loss: 1.066, lr: 0.002325851843237586\n",
      "epoch: 7700, acc: 0.437, loss: 1.066, lr: 0.0022991148407862975\n",
      "epoch: 7800, acc: 0.437, loss: 1.066, lr: 0.0022729855665416525\n",
      "epoch: 7900, acc: 0.437, loss: 1.066, lr: 0.002247443532981234\n",
      "epoch: 8000, acc: 0.437, loss: 1.066, lr: 0.00222246916324036\n",
      "epoch: 8100, acc: 0.437, loss: 1.066, lr: 0.0021980437410704474\n",
      "epoch: 8200, acc: 0.437, loss: 1.066, lr: 0.0021741493640613113\n",
      "epoch: 8300, acc: 0.437, loss: 1.066, lr: 0.0021507688998817077\n",
      "epoch: 8400, acc: 0.437, loss: 1.066, lr: 0.002127885945313331\n",
      "epoch: 8500, acc: 0.437, loss: 1.066, lr: 0.0021054847878724074\n",
      "epoch: 8600, acc: 0.437, loss: 1.066, lr: 0.0020835503698301903\n",
      "epoch: 8700, acc: 0.437, loss: 1.066, lr: 0.0020620682544592226\n",
      "epoch: 8800, acc: 0.437, loss: 1.066, lr: 0.002041024594346362\n",
      "epoch: 8900, acc: 0.437, loss: 1.066, lr: 0.002020406101626427\n",
      "epoch: 9000, acc: 0.437, loss: 1.066, lr: 0.002000200020002\n",
      "epoch: 9100, acc: 0.437, loss: 1.066, lr: 0.0019803940984255866\n",
      "epoch: 9200, acc: 0.437, loss: 1.066, lr: 0.0019609765663300325\n",
      "epoch: 9300, acc: 0.437, loss: 1.066, lr: 0.0019419361103019711\n",
      "epoch: 9400, acc: 0.437, loss: 1.066, lr: 0.0019232618521011636\n",
      "epoch: 9500, acc: 0.437, loss: 1.066, lr: 0.0019049433279359938\n",
      "epoch: 9600, acc: 0.437, loss: 1.066, lr: 0.0018869704689121615\n",
      "epoch: 9700, acc: 0.437, loss: 1.066, lr: 0.001869333582577811\n",
      "epoch: 9800, acc: 0.437, loss: 1.066, lr: 0.0018520233354940275\n",
      "epoch: 9900, acc: 0.437, loss: 1.066, lr: 0.0018350307367648406\n",
      "epoch: 10000, acc: 0.437, loss: 1.066, lr: 0.0018183471224656785\n"
     ]
    }
   ],
   "source": [
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    " \n",
    "# Create second Dense layer with 64 input features (as we take output \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "#optimiser = Optimiser_SGD(decay=8e-8, momentum=0.9)\n",
    "\n",
    "optimiser = Optimiser_RMSProp(learning_rate=0.02,decay=1e-3) \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    " \n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(dense2.output, y) \n",
    " \n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimiser.current_learning_rate}') \n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " \n",
    "    # Update weights and biases \n",
    "    optimiser.pre_update_params() \n",
    "    optimiser.update_params(dense1) \n",
    "    optimiser.update_params(dense2) \n",
    "    optimiser.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a significant improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adam**, short for **Adaptive Momentum** , is currently the most widely-used optimizer and is built atop RMSProp, with the momentum concept from SGD added back in. \n",
    "This means that, instead of applying current gradients, we’re going to apply momentums like in the SGD optimizer with momentum, then apply a per-weight adaptive learning rate with the cache as done in RMSProp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The Adam optimizer additionally adds a bias correction mechanism. Do not confuse this with the \n",
    "layer’s bias. The bias correction mechanism is applied to the cache and momentum, compensating \n",
    "for the initial zeroed values before they warm up with initial steps.\n",
    "-  To achieve this correction, both momentum and caches are divided by (1 - $beta^{step}$).As step raises, $beta^{step}$​ ​ approaches 0​ (a fraction to the power of a rising value decreases), solving this whole expression to a fraction \n",
    "during the first steps and approaching 1​ as training progresses.\n",
    "-  For example, $beta 1$​, a fraction of momentum to apply, defaults to 0.9. This means that, during the first step, the correction value equals: \n",
    "$$1 - 0.9^1 = 0.1$$\n",
    "With training progression, as step count rises: \n",
    "$$ 1 - \\lim_{step \\to \\infty} 0.9^{step} = 1 $$\n",
    "\n",
    "- The same applies to the cache and the beta 2​ — in this case, the starting value is 0.001 and also \n",
    "approaches 1​. These values divide the momentums and the cache, respectively.\n",
    "- Division by a fraction causes them to be multiple times bigger, significantly speeding up training in the initial stages before both tables warm up during multiple initial steps. We also previously mentioned that both of these bias-correcting coefficients go towards a value of 1​ as training progresses and return parameter updates to their typical values for the later training steps.\n",
    "- To get parameter updates, we divide the scaled momentum by the scaled square-rooted cache. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following changes were made from copying the RMSProp class code: \n",
    "1. renamed class from Optimizer_RMSprop to Optimizer_Adam \n",
    "2. renamed the rho​ hyperparameter and property to beta_2​ in __init__ \n",
    "3. added beta_1​ hyperparameter and property in __init__ \n",
    "4. added momentum​ array creation in update_params() \n",
    "5. added momentum​ calculation \n",
    "6. renamed self.rho to self.beta_2 with cache calculation code in update_params \n",
    "7. added *_corrected variables as corrected momentums and weights \n",
    "8. replaced layer.dweights, ​layer.dbiases, ​layer.weight_cache, and \n",
    "layer.bias_cache with corrected arrays of values in parameter updates with \n",
    "momentum arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimiser_Adam: \n",
    " \n",
    "    # Initialize optimizer - set settings \n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, \n",
    "                 beta_1=0.9, beta_2=0.999): \n",
    "        self.learning_rate = learning_rate \n",
    "        self.current_learning_rate = learning_rate \n",
    "        self.decay = decay \n",
    "        self.iterations = 0 \n",
    "        self.epsilon = epsilon \n",
    "        self.beta_1 = beta_1 \n",
    "        self.beta_2 = beta_2 \n",
    " \n",
    "    # Call once before any parameter updates \n",
    "    def pre_update_params(self): \n",
    "        if self.decay: \n",
    "            self.current_learning_rate = self.learning_rate*(1. / (1. + self.decay * self.iterations)) \n",
    " \n",
    "    # Update parameters \n",
    "    def update_params(self, layer): \n",
    " \n",
    "        # If layer does not contain cache arrays, \n",
    "        # create them filled with zeros \n",
    "        if not hasattr(layer, 'weight_cache'): \n",
    "            layer.weight_momentums = np.zeros_like(layer.weights) \n",
    "            layer.weight_cache = np.zeros_like(layer.weights) \n",
    "            layer.bias_momentums = np.zeros_like(layer.biases) \n",
    "            layer.bias_cache = np.zeros_like(layer.biases) \n",
    " \n",
    "        # Update momentum  with current gradients \n",
    "        layer.weight_momentums = self.beta_1*layer.weight_momentums + (1 - self.beta_1)*layer.dweights \n",
    "        layer.bias_momentums = self.beta_1*layer.bias_momentums + (1 - self.beta_1)*layer.dbiases \n",
    "\n",
    "        # Get corrected momentum \n",
    "        # self.iteration is 0 at first pass \n",
    "        # and we need to start with 1 here \n",
    "        weight_momentums_corrected = layer.weight_momentums/(1 - self.beta_1 ** (self.iterations + 1)) \n",
    "        bias_momentums_corrected = layer.bias_momentums/(1 - self.beta_1 ** (self.iterations + 1)) \n",
    "\n",
    "        # Update cache with squared current gradients \n",
    "        layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2 \n",
    "        layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2 \n",
    "\n",
    "        # Get corrected cache \n",
    "        weight_cache_corrected = layer.weight_cache/(1 - self.beta_2 ** (self.iterations + 1)) \n",
    "        bias_cache_corrected = layer.bias_cache/(1 - self.beta_2 ** (self.iterations + 1)) \n",
    " \n",
    "        # Vanilla SGD parameter update + normalization \n",
    "        # with square rooted cache \n",
    "        layer.weights += -self.current_learning_rate*weight_momentums_corrected/(np.sqrt(weight_cache_corrected) + self.epsilon) \n",
    "        layer.biases += -self.current_learning_rate*bias_momentums_corrected/(np.sqrt(bias_cache_corrected) + self.epsilon) \n",
    " \n",
    "    # Call once after any parameter updates \n",
    "    def post_update_params(self): \n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again doing a test with learning_rate = 0.02,decay = 1e-5 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.377, loss: 1.099, lr: 0.02\n",
      "epoch: 100, acc: 0.500, loss: 0.930, lr: 0.01998021958261321\n",
      "epoch: 200, acc: 0.567, loss: 0.830, lr: 0.019960279044701046\n",
      "epoch: 300, acc: 0.693, loss: 0.725, lr: 0.019940378268975763\n",
      "epoch: 400, acc: 0.690, loss: 0.658, lr: 0.01992051713662487\n",
      "epoch: 500, acc: 0.710, loss: 0.621, lr: 0.01990069552930875\n",
      "epoch: 600, acc: 0.757, loss: 0.565, lr: 0.019880913329158343\n",
      "epoch: 700, acc: 0.790, loss: 0.504, lr: 0.019861170418772778\n",
      "epoch: 800, acc: 0.830, loss: 0.464, lr: 0.019841466681217078\n",
      "epoch: 900, acc: 0.827, loss: 0.426, lr: 0.01982180200001982\n",
      "epoch: 1000, acc: 0.833, loss: 0.406, lr: 0.019802176259170884\n",
      "epoch: 1100, acc: 0.857, loss: 0.393, lr: 0.01978258934311912\n",
      "epoch: 1200, acc: 0.843, loss: 0.380, lr: 0.01976304113677013\n",
      "epoch: 1300, acc: 0.853, loss: 0.370, lr: 0.019743531525483964\n",
      "epoch: 1400, acc: 0.863, loss: 0.362, lr: 0.01972406039507293\n",
      "epoch: 1500, acc: 0.863, loss: 0.354, lr: 0.019704627631799327\n",
      "epoch: 1600, acc: 0.863, loss: 0.353, lr: 0.019685233122373254\n",
      "epoch: 1700, acc: 0.863, loss: 0.343, lr: 0.019665876753950384\n",
      "epoch: 1800, acc: 0.847, loss: 0.339, lr: 0.01964655841412981\n",
      "epoch: 1900, acc: 0.853, loss: 0.334, lr: 0.019627277990951823\n",
      "epoch: 2000, acc: 0.850, loss: 0.332, lr: 0.019608035372895814\n",
      "epoch: 2100, acc: 0.870, loss: 0.326, lr: 0.01958883044887805\n",
      "epoch: 2200, acc: 0.870, loss: 0.325, lr: 0.019569663108249594\n",
      "epoch: 2300, acc: 0.850, loss: 0.320, lr: 0.01955053324079414\n",
      "epoch: 2400, acc: 0.860, loss: 0.313, lr: 0.019531440736725945\n",
      "epoch: 2500, acc: 0.873, loss: 0.309, lr: 0.019512385486687673\n",
      "epoch: 2600, acc: 0.873, loss: 0.306, lr: 0.019493367381748363\n",
      "epoch: 2700, acc: 0.873, loss: 0.304, lr: 0.019474386313401298\n",
      "epoch: 2800, acc: 0.873, loss: 0.300, lr: 0.019455442173562\n",
      "epoch: 2900, acc: 0.877, loss: 0.299, lr: 0.019436534854566128\n",
      "epoch: 3000, acc: 0.877, loss: 0.296, lr: 0.01941766424916747\n",
      "epoch: 3100, acc: 0.870, loss: 0.294, lr: 0.019398830250535893\n",
      "epoch: 3200, acc: 0.880, loss: 0.295, lr: 0.019380032752255354\n",
      "epoch: 3300, acc: 0.883, loss: 0.287, lr: 0.01936127164832186\n",
      "epoch: 3400, acc: 0.883, loss: 0.285, lr: 0.01934254683314152\n",
      "epoch: 3500, acc: 0.883, loss: 0.281, lr: 0.019323858201528515\n",
      "epoch: 3600, acc: 0.890, loss: 0.279, lr: 0.019305205648703173\n",
      "epoch: 3700, acc: 0.883, loss: 0.278, lr: 0.01928658907028997\n",
      "epoch: 3800, acc: 0.883, loss: 0.275, lr: 0.01926800836231563\n",
      "epoch: 3900, acc: 0.893, loss: 0.274, lr: 0.019249463421207133\n",
      "epoch: 4000, acc: 0.883, loss: 0.271, lr: 0.019230954143789846\n",
      "epoch: 4100, acc: 0.890, loss: 0.268, lr: 0.019212480427285565\n",
      "epoch: 4200, acc: 0.890, loss: 0.266, lr: 0.019194042169310647\n",
      "epoch: 4300, acc: 0.890, loss: 0.265, lr: 0.019175639267874092\n",
      "epoch: 4400, acc: 0.887, loss: 0.263, lr: 0.019157271621375684\n",
      "epoch: 4500, acc: 0.887, loss: 0.261, lr: 0.0191389391286041\n",
      "epoch: 4600, acc: 0.887, loss: 0.260, lr: 0.019120641688735073\n",
      "epoch: 4700, acc: 0.887, loss: 0.258, lr: 0.019102379201329525\n",
      "epoch: 4800, acc: 0.887, loss: 0.257, lr: 0.01908415156633174\n",
      "epoch: 4900, acc: 0.893, loss: 0.256, lr: 0.01906595868406753\n",
      "epoch: 5000, acc: 0.890, loss: 0.254, lr: 0.01904780045524243\n",
      "epoch: 5100, acc: 0.890, loss: 0.256, lr: 0.019029676780939874\n",
      "epoch: 5200, acc: 0.890, loss: 0.252, lr: 0.019011587562619416\n",
      "epoch: 5300, acc: 0.890, loss: 0.250, lr: 0.01899353270211493\n",
      "epoch: 5400, acc: 0.887, loss: 0.249, lr: 0.018975512101632844\n",
      "epoch: 5500, acc: 0.890, loss: 0.248, lr: 0.018957525663750367\n",
      "epoch: 5600, acc: 0.893, loss: 0.252, lr: 0.018939573291413745\n",
      "epoch: 5700, acc: 0.890, loss: 0.246, lr: 0.018921654887936498\n",
      "epoch: 5800, acc: 0.890, loss: 0.243, lr: 0.018903770356997706\n",
      "epoch: 5900, acc: 0.890, loss: 0.241, lr: 0.018885919602640248\n",
      "epoch: 6000, acc: 0.897, loss: 0.240, lr: 0.018868102529269144\n",
      "epoch: 6100, acc: 0.887, loss: 0.240, lr: 0.018850319041649778\n",
      "epoch: 6200, acc: 0.893, loss: 0.238, lr: 0.018832569044906263\n",
      "epoch: 6300, acc: 0.893, loss: 0.237, lr: 0.018814852444519702\n",
      "epoch: 6400, acc: 0.893, loss: 0.235, lr: 0.018797169146326564\n",
      "epoch: 6500, acc: 0.897, loss: 0.235, lr: 0.01877951905651696\n",
      "epoch: 6600, acc: 0.890, loss: 0.234, lr: 0.018761902081633034\n",
      "epoch: 6700, acc: 0.893, loss: 0.234, lr: 0.018744318128567278\n",
      "epoch: 6800, acc: 0.893, loss: 0.233, lr: 0.018726767104560903\n",
      "epoch: 6900, acc: 0.893, loss: 0.233, lr: 0.018709248917202218\n",
      "epoch: 7000, acc: 0.900, loss: 0.231, lr: 0.018691763474424996\n",
      "epoch: 7100, acc: 0.897, loss: 0.230, lr: 0.018674310684506857\n",
      "epoch: 7200, acc: 0.897, loss: 0.230, lr: 0.01865689045606769\n",
      "epoch: 7300, acc: 0.903, loss: 0.228, lr: 0.01863950269806802\n",
      "epoch: 7400, acc: 0.900, loss: 0.221, lr: 0.018622147319807447\n",
      "epoch: 7500, acc: 0.900, loss: 0.219, lr: 0.018604824230923075\n",
      "epoch: 7600, acc: 0.907, loss: 0.216, lr: 0.01858753334138793\n",
      "epoch: 7700, acc: 0.907, loss: 0.215, lr: 0.018570274561509396\n",
      "epoch: 7800, acc: 0.907, loss: 0.214, lr: 0.018553047801927663\n",
      "epoch: 7900, acc: 0.910, loss: 0.212, lr: 0.018535852973614212\n",
      "epoch: 8000, acc: 0.913, loss: 0.211, lr: 0.01851868998787026\n",
      "epoch: 8100, acc: 0.910, loss: 0.210, lr: 0.018501558756325222\n",
      "epoch: 8200, acc: 0.907, loss: 0.214, lr: 0.01848445919093522\n",
      "epoch: 8300, acc: 0.907, loss: 0.209, lr: 0.018467391203981567\n",
      "epoch: 8400, acc: 0.913, loss: 0.203, lr: 0.018450354708069265\n",
      "epoch: 8500, acc: 0.920, loss: 0.208, lr: 0.018433349616125496\n",
      "epoch: 8600, acc: 0.900, loss: 0.210, lr: 0.018416375841398172\n",
      "epoch: 8700, acc: 0.917, loss: 0.201, lr: 0.01839943329745444\n",
      "epoch: 8800, acc: 0.920, loss: 0.200, lr: 0.01838252189817921\n",
      "epoch: 8900, acc: 0.920, loss: 0.199, lr: 0.018365641557773718\n",
      "epoch: 9000, acc: 0.920, loss: 0.199, lr: 0.018348792190754044\n",
      "epoch: 9100, acc: 0.920, loss: 0.198, lr: 0.0183319737119497\n",
      "epoch: 9200, acc: 0.920, loss: 0.198, lr: 0.018315186036502167\n",
      "epoch: 9300, acc: 0.920, loss: 0.197, lr: 0.018298429079863496\n",
      "epoch: 9400, acc: 0.920, loss: 0.196, lr: 0.018281702757794862\n",
      "epoch: 9500, acc: 0.913, loss: 0.197, lr: 0.018265006986365174\n",
      "epoch: 9600, acc: 0.917, loss: 0.198, lr: 0.018248341681949654\n",
      "epoch: 9700, acc: 0.917, loss: 0.200, lr: 0.018231706761228456\n",
      "epoch: 9800, acc: 0.923, loss: 0.195, lr: 0.018215102141185255\n",
      "epoch: 9900, acc: 0.913, loss: 0.197, lr: 0.018198527739105907\n",
      "epoch: 10000, acc: 0.913, loss: 0.195, lr: 0.018181983472577025\n"
     ]
    }
   ],
   "source": [
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    " \n",
    "# Create second Dense layer with 64 input features (as we take output \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "#optimiser = Optimiser_SGD(decay=8e-8, momentum=0.9)\n",
    "\n",
    "optimiser = Optimiser_Adam(learning_rate=0.02,decay=1e-5) \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    " \n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(dense2.output, y) \n",
    " \n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimiser.current_learning_rate}') \n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " \n",
    "    # Update weights and biases \n",
    "    optimiser.pre_update_params() \n",
    "    optimiser.update_params(dense1) \n",
    "    optimiser.update_params(dense2) \n",
    "    optimiser.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the best result so far, but let’s adjust the learning rate to be a bit higher, to 0.05​ and change \n",
    "decay to 5e-7 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.317, loss: 1.099, lr: 0.05\n",
      "epoch: 100, acc: 0.643, loss: 0.823, lr: 0.04999752512250644\n",
      "epoch: 200, acc: 0.667, loss: 0.696, lr: 0.04999502549496326\n",
      "epoch: 300, acc: 0.710, loss: 0.629, lr: 0.049992526117345455\n",
      "epoch: 400, acc: 0.743, loss: 0.563, lr: 0.04999002698961558\n",
      "epoch: 500, acc: 0.787, loss: 0.513, lr: 0.049987528111736124\n",
      "epoch: 600, acc: 0.797, loss: 0.478, lr: 0.049985029483669646\n",
      "epoch: 700, acc: 0.820, loss: 0.442, lr: 0.049982531105378675\n",
      "epoch: 800, acc: 0.833, loss: 0.421, lr: 0.04998003297682575\n",
      "epoch: 900, acc: 0.840, loss: 0.401, lr: 0.049977535097973466\n",
      "epoch: 1000, acc: 0.850, loss: 0.378, lr: 0.049975037468784345\n",
      "epoch: 1100, acc: 0.860, loss: 0.362, lr: 0.049972540089220974\n",
      "epoch: 1200, acc: 0.847, loss: 0.364, lr: 0.04997004295924593\n",
      "epoch: 1300, acc: 0.867, loss: 0.337, lr: 0.04996754607882181\n",
      "epoch: 1400, acc: 0.867, loss: 0.326, lr: 0.049965049447911185\n",
      "epoch: 1500, acc: 0.877, loss: 0.327, lr: 0.04996255306647668\n",
      "epoch: 1600, acc: 0.883, loss: 0.311, lr: 0.049960056934480884\n",
      "epoch: 1700, acc: 0.890, loss: 0.302, lr: 0.04995756105188642\n",
      "epoch: 1800, acc: 0.887, loss: 0.316, lr: 0.049955065418655915\n",
      "epoch: 1900, acc: 0.890, loss: 0.282, lr: 0.04995257003475201\n",
      "epoch: 2000, acc: 0.897, loss: 0.279, lr: 0.04995007490013731\n",
      "epoch: 2100, acc: 0.900, loss: 0.276, lr: 0.0499475800147745\n",
      "epoch: 2200, acc: 0.890, loss: 0.260, lr: 0.0499450853786262\n",
      "epoch: 2300, acc: 0.900, loss: 0.260, lr: 0.0499425909916551\n",
      "epoch: 2400, acc: 0.853, loss: 0.358, lr: 0.04994009685382384\n",
      "epoch: 2500, acc: 0.907, loss: 0.245, lr: 0.04993760296509512\n",
      "epoch: 2600, acc: 0.923, loss: 0.236, lr: 0.049935109325431604\n",
      "epoch: 2700, acc: 0.923, loss: 0.231, lr: 0.049932615934796004\n",
      "epoch: 2800, acc: 0.923, loss: 0.227, lr: 0.04993012279315098\n",
      "epoch: 2900, acc: 0.927, loss: 0.223, lr: 0.049927629900459285\n",
      "epoch: 3000, acc: 0.933, loss: 0.220, lr: 0.049925137256683606\n",
      "epoch: 3100, acc: 0.933, loss: 0.216, lr: 0.04992264486178666\n",
      "epoch: 3200, acc: 0.937, loss: 0.213, lr: 0.04992015271573119\n",
      "epoch: 3300, acc: 0.937, loss: 0.209, lr: 0.04991766081847992\n",
      "epoch: 3400, acc: 0.937, loss: 0.206, lr: 0.049915169169995596\n",
      "epoch: 3500, acc: 0.937, loss: 0.201, lr: 0.049912677770240964\n",
      "epoch: 3600, acc: 0.920, loss: 0.203, lr: 0.049910186619178794\n",
      "epoch: 3700, acc: 0.923, loss: 0.203, lr: 0.04990769571677183\n",
      "epoch: 3800, acc: 0.933, loss: 0.192, lr: 0.04990520506298287\n",
      "epoch: 3900, acc: 0.907, loss: 0.222, lr: 0.04990271465777467\n",
      "epoch: 4000, acc: 0.920, loss: 0.196, lr: 0.049900224501110035\n",
      "epoch: 4100, acc: 0.930, loss: 0.189, lr: 0.04989773459295174\n",
      "epoch: 4200, acc: 0.940, loss: 0.184, lr: 0.04989524493326262\n",
      "epoch: 4300, acc: 0.937, loss: 0.185, lr: 0.04989275552200545\n",
      "epoch: 4400, acc: 0.917, loss: 0.194, lr: 0.04989026635914307\n",
      "epoch: 4500, acc: 0.933, loss: 0.182, lr: 0.04988777744463829\n",
      "epoch: 4600, acc: 0.947, loss: 0.174, lr: 0.049885288778453954\n",
      "epoch: 4700, acc: 0.927, loss: 0.182, lr: 0.049882800360552884\n",
      "epoch: 4800, acc: 0.940, loss: 0.170, lr: 0.04988031219089794\n",
      "epoch: 4900, acc: 0.950, loss: 0.169, lr: 0.049877824269451976\n",
      "epoch: 5000, acc: 0.927, loss: 0.186, lr: 0.04987533659617785\n",
      "epoch: 5100, acc: 0.943, loss: 0.167, lr: 0.04987284917103844\n",
      "epoch: 5200, acc: 0.947, loss: 0.166, lr: 0.04987036199399661\n",
      "epoch: 5300, acc: 0.873, loss: 0.336, lr: 0.04986787506501525\n",
      "epoch: 5400, acc: 0.950, loss: 0.162, lr: 0.04986538838405724\n",
      "epoch: 5500, acc: 0.950, loss: 0.160, lr: 0.049862901951085496\n",
      "epoch: 5600, acc: 0.950, loss: 0.159, lr: 0.049860415766062906\n",
      "epoch: 5700, acc: 0.950, loss: 0.158, lr: 0.0498579298289524\n",
      "epoch: 5800, acc: 0.950, loss: 0.157, lr: 0.04985544413971689\n",
      "epoch: 5900, acc: 0.947, loss: 0.155, lr: 0.049852958698319315\n",
      "epoch: 6000, acc: 0.947, loss: 0.156, lr: 0.04985047350472258\n",
      "epoch: 6100, acc: 0.947, loss: 0.151, lr: 0.04984798855888967\n",
      "epoch: 6200, acc: 0.943, loss: 0.162, lr: 0.049845503860783506\n",
      "epoch: 6300, acc: 0.937, loss: 0.162, lr: 0.049843019410367055\n",
      "epoch: 6400, acc: 0.960, loss: 0.149, lr: 0.04984053520760327\n",
      "epoch: 6500, acc: 0.947, loss: 0.151, lr: 0.049838051252455155\n",
      "epoch: 6600, acc: 0.950, loss: 0.155, lr: 0.049835567544885655\n",
      "epoch: 6700, acc: 0.950, loss: 0.146, lr: 0.04983308408485778\n",
      "epoch: 6800, acc: 0.950, loss: 0.152, lr: 0.0498306008723345\n",
      "epoch: 6900, acc: 0.927, loss: 0.178, lr: 0.04982811790727884\n",
      "epoch: 7000, acc: 0.937, loss: 0.154, lr: 0.04982563518965381\n",
      "epoch: 7100, acc: 0.937, loss: 0.153, lr: 0.049823152719422406\n",
      "epoch: 7200, acc: 0.957, loss: 0.145, lr: 0.049820670496547675\n",
      "epoch: 7300, acc: 0.957, loss: 0.145, lr: 0.04981818852099264\n",
      "epoch: 7400, acc: 0.950, loss: 0.142, lr: 0.049815706792720335\n",
      "epoch: 7500, acc: 0.950, loss: 0.150, lr: 0.0498132253116938\n",
      "epoch: 7600, acc: 0.960, loss: 0.136, lr: 0.04981074407787611\n",
      "epoch: 7700, acc: 0.960, loss: 0.135, lr: 0.049808263091230306\n",
      "epoch: 7800, acc: 0.940, loss: 0.140, lr: 0.04980578235171948\n",
      "epoch: 7900, acc: 0.967, loss: 0.133, lr: 0.04980330185930667\n",
      "epoch: 8000, acc: 0.950, loss: 0.132, lr: 0.04980082161395499\n",
      "epoch: 8100, acc: 0.940, loss: 0.136, lr: 0.04979834161562752\n",
      "epoch: 8200, acc: 0.947, loss: 0.134, lr: 0.04979586186428736\n",
      "epoch: 8300, acc: 0.940, loss: 0.135, lr: 0.04979338235989761\n",
      "epoch: 8400, acc: 0.943, loss: 0.134, lr: 0.04979090310242139\n",
      "epoch: 8500, acc: 0.960, loss: 0.133, lr: 0.049788424091821805\n",
      "epoch: 8600, acc: 0.967, loss: 0.127, lr: 0.049785945328062006\n",
      "epoch: 8700, acc: 0.950, loss: 0.127, lr: 0.0497834668111051\n",
      "epoch: 8800, acc: 0.940, loss: 0.134, lr: 0.049780988540914256\n",
      "epoch: 8900, acc: 0.963, loss: 0.124, lr: 0.0497785105174526\n",
      "epoch: 9000, acc: 0.967, loss: 0.124, lr: 0.04977603274068329\n",
      "epoch: 9100, acc: 0.943, loss: 0.135, lr: 0.04977355521056952\n",
      "epoch: 9200, acc: 0.927, loss: 0.175, lr: 0.049771077927074414\n",
      "epoch: 9300, acc: 0.807, loss: 0.725, lr: 0.0497686008901612\n",
      "epoch: 9400, acc: 0.957, loss: 0.123, lr: 0.04976612409979302\n",
      "epoch: 9500, acc: 0.953, loss: 0.120, lr: 0.0497636475559331\n",
      "epoch: 9600, acc: 0.957, loss: 0.119, lr: 0.049761171258544616\n",
      "epoch: 9700, acc: 0.960, loss: 0.119, lr: 0.0497586952075908\n",
      "epoch: 9800, acc: 0.967, loss: 0.118, lr: 0.04975621940303483\n",
      "epoch: 9900, acc: 0.967, loss: 0.117, lr: 0.049753743844839965\n",
      "epoch: 10000, acc: 0.967, loss: 0.117, lr: 0.04975126853296942\n"
     ]
    }
   ],
   "source": [
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    " \n",
    "# Create second Dense layer with 64 input features (as we take output \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "#optimiser = Optimiser_SGD(decay=8e-8, momentum=0.9)\n",
    "\n",
    "optimiser = Optimiser_Adam(learning_rate=0.05,decay=5e-7) \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    " \n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(dense2.output, y) \n",
    " \n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimiser.current_learning_rate}') \n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " \n",
    "    # Update weights and biases \n",
    "    optimiser.pre_update_params() \n",
    "    optimiser.update_params(dense1) \n",
    "    optimiser.update_params(dense2) \n",
    "    optimiser.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An improvement over the last hyperparameters. <br> \n",
    "**NOTE:** <br>\n",
    "*While Adam has performed the best here and is usually the best optimizer of those shown, that’s not always the case. It’s usually a good idea to try the Adam optimizer first but to also try the others, especially if you’re not getting the results you hoped for. Sometimes simple SGD or SGD + momentum performs better than Adam. Reasons why will vary, but keep this in mind.* <br><br>\n",
    "- *A general starting learning rate for SGD is 1.0, with a decay down to 0.1.*\n",
    "- *For Adam, a good starting LR is 0.001 (1e-3), decaying down to 0.0001 (1e-4). Different problems may require different \n",
    "values here, but these are decent to start.*\n",
    "- **We achieved a 96.7% accuracy and a loss apporoaching 0.1 but this might ni be as good as you expect because this can mean overfitting. There are cases where you can truly achieve valid results as good as \n",
    "these, but, in this case, we’ve been ignoring a major concept in machine learning: out-of-sample \n",
    "testing data (which can shed light on over-fitting)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
