{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4 : Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  The activation function is applied to the output of a neuron (or layer of neurons), which modifies outputs. We use activation functions because if \n",
    "the activation function itself is nonlinear, it allows for neural networks with usually two or more hidden layers to map nonlinear functions.\n",
    "- In general, your neural network will have two types of activation functions. The first will be the activation function used in hidden layers, and the second will be used in the output layer. Usually, the activation function used for hidden neurons will be the same for all of them, but it doesn’t \n",
    "have to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few examples include:\n",
    "- **Step Activation Function :** This activation function has been used historically in hidden layers, but nowadays, it is rarely a \n",
    "choice.\n",
    "- **Linear Activation Function :** A linear function is simply the equation of a line. It will appear as a straight line when graphed, \n",
    "where y=x and the output value equals the input.This activation function is usually applied to the last layer’s output in the case of a regression \n",
    "model — a model that outputs a scalar value instead of a classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid Activation Function:** <br>\n",
    "Problem with step function is that it's less informative, it's either 1 or 0. So you can't tell how close this step function was to activating or deactivating.  Thus, when it comes time to optimize weights and biases, it’s easier for the optimizer if we have activation functions that are more granular and informative. <br>\n",
    "Originally,sigmoid activaton function was used. <br>\n",
    "![](img1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Advantages?** <br>\n",
    "In this case, we’re getting a value that can be reversed to its original value; the returned value contains all the information from the input,contrary to a function like the step function, where an input of 3 will output the same value as an input of 300,000. The output from the Sigmoid function, being in the range of 0 to 1, also works better with neural networks — especially compared to the range of the negative to the positive infinity — and adds nonlinearity. <br>\n",
    "The Sigmoid function, historically used in hidden layers, was eventually replaced by the **Rectified Linear Units** activation function (or ReLU)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img2.png) <br>\n",
    "This simple yet powerful activation function is the most widely used activation function at the \n",
    "time of writing for various reasons — mainly speed and efficiency. While the sigmoid activation \n",
    "function isn’t the most complicated, it’s still much more challenging to compute than the ReLU \n",
    "activation function. The ReLU activation function is extremely close to being a linear activation \n",
    "function while remaining nonlinear, due to that bend after 0. This simple property is, however, \n",
    "very effective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE** <br>\n",
    "In most cases,for a neural network to fit a non-linear function,we need to contain teo or more hidden layers,and those hidden layers to use a nonlinear activation function. <br>\n",
    "**Also check out the example and functioning of ReLU from the book.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "input = [0,2,-1,3.3,-2.7,1.1,2.2,-100]\n",
    "output = []\n",
    "for i in input:\n",
    "    if i>0:\n",
    "        output.append(i)\n",
    "    else:\n",
    "        output.append(0)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for i in input:\n",
    "    output.append(max(0,i))\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  2.  0.  3.3 0.  1.1 2.2 0. ]\n"
     ]
    }
   ],
   "source": [
    "output = np.maximum(0,input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self,inputs):\n",
    "        # calculate ouput values from inputs\n",
    "        self.outputs = np.maximum(0,inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nnfs\n",
    "nnfs.init()\n",
    "import matplotlib.pyplot as plt\n",
    "import nnfs.datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        # Initialise weights and biases\n",
    "        self.weights = 0.01*np.random.randn(n_inputs,n_neurons) # the transpose step has been omitted hence\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self,inputs):\n",
    "        # Calculate output values from inputs, weights and biases \n",
    "        self.outputs = np.dot(inputs,self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.        ]\n",
      " [0.         0.00011395 0.        ]\n",
      " [0.         0.00031729 0.        ]\n",
      " [0.         0.00052666 0.        ]\n",
      " [0.         0.00071401 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# create dataset\n",
    "X,y = nnfs.datasets.spiral_data(samples=100,classes=3)\n",
    "\n",
    "# create dense layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2,3)\n",
    "\n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    "\n",
    "# perform forward pass of our training data through this layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# forward pass through activation func.\n",
    "activation1.forward(dense1.outputs)\n",
    "\n",
    "# output of first few samples\n",
    "print(activation1.outputs[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Softmax Activation function**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now if we want this model for classification,there's problems with ReLU. \n",
    "- unbounded\n",
    "- not normalised\n",
    "- exclusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“Not normalized” implies the values can be anything, an output of [12, 99, 318]​ is without context, and “exclusive” means each output is independent of the others. <br>\n",
    "To address this lack of context, the softmax activation on the output data can take in non-normalized, or uncalibrated, inputs and \n",
    "produce a normalized distribution of probabilities for our classes. This distribution returned by the softmax activation function represents confidence scores for each class and will add up to 1. The predicted class is associated with the output neuron that returned the largest confidence score.<br> <br>\n",
    "Here's the function for softmax:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$S_{i,j} = \\frac{e^{z_{i,j}}}{\\sum_{l=1}^{L} e^{z_{i,l}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the numerator and the denominator of the Softmax function contain e​ raised to the power of \n",
    "z​, where z​, given indices, means a singular output value — the index i​ means the current sample \n",
    "and the index j​ means the current output in this sample. The numerator exponentiates the current \n",
    "output value and the denominator takes a sum of all of the exponentiated outputs for a given \n",
    "sample. <br>\n",
    "1. Exponentiation serves multiple purposes. To calculate the probabilities, we need non-negative values. <br>\n",
    "2. The exponential function is a monotonic function. This means that, with higher input values, \n",
    "outputs are also higher, so we won’t change the predicted class after applying it while making \n",
    "sure that we get non-negative values. <br>\n",
    "3.  It also adds stability to the result as the normalized \n",
    "exponentiation is more about the difference between numbers than their magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for the softmax function is the normalisation part of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponential values:\n",
      "[121.51041752   3.35348465  10.85906266]\n",
      "normalised exponential values:\n",
      "[0.89528266 0.02470831 0.08000903]\n",
      "sum of normalised values: 0.9999999999999999\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "# Values from the earlier previous when we described \n",
    "# what a neural network is \n",
    "layer_outputs = [4.8, 1.21, 2.385]\n",
    "\n",
    "# for each avlue in vector ,calculate the exponent\n",
    "exp_values = np.exp(layer_outputs)\n",
    "print('exponential values:')\n",
    "print(exp_values)\n",
    "\n",
    "# normalising the values\n",
    "norm_values = exp_values/np.sum(exp_values)\n",
    "print('normalised exponential values:')\n",
    "print(norm_values)\n",
    "print('sum of normalised values:',np.sum(norm_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’re trying to sum all the outputs from a layer for each sample in a batch; <br>\n",
    "converting the layer’s output array with row length equal to the number of neurons in the layer, to just one value. We need a column vector with these values since it will let us normalize the whole batch of samples, sample-wise, with a single calculation.<br>\n",
    "Adding all this in softmax class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax:\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self,inputs):\n",
    "\n",
    "        # get unnormalised probablities\n",
    "        exp_values = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
    "        \n",
    "        #normalise them for each sample\n",
    "        probabilities = exp_values/np.sum(exp_values,axis = 1,keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice we did a subtraction too. There are two main pervasive challenges with neural networks: “dead neurons” \n",
    "and very large numbers (referred to as “exploding” values). “Dead” neurons and enormous \n",
    "numbers can wreak havoc down the line and render a network useless over time. The exponential \n",
    "function used in softmax activation is one of the sources of exploding values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.718281828459045\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22026.465794806718\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6881171418161356e+43\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_22168\\2528843860.py:1: RuntimeWarning: overflow encountered in exp\n",
      "  print(np.exp(1000))\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we subtract the maximum value from a list of input values. We would then change the output values to always be in a range from some negative value up to 0, as the largest number subtracted by itself returns 0, and any smaller number subtracted by it will result in a negative number. And we know the exponential of such numbers lies between 0 and 1.With Softmax, thanks to the normalization, we can subtract any value from all of the inputs, and it will not change the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09003057 0.24472847 0.66524096]]\n"
     ]
    }
   ],
   "source": [
    "softmax = Activation_Softmax()\n",
    "softmax.forward([[1,2,3]])\n",
    "print(softmax.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09003057 0.24472847 0.66524096]]\n"
     ]
    }
   ],
   "source": [
    "# subtracted 3 from each\n",
    "softmax.forward([[-2,-1,0]])\n",
    "print(softmax.output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can add another dense layer as the output layer, setting it to contain as many inputs as \n",
    "the previous layer has outputs and as many outputs as our data includes classes. Then we can \n",
    "apply the softmax activation to the output of this new layer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Full code till now:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnfs.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    def __init__(self,n_inputs,n_neurons):\n",
    "        # Initialise weights and biases\n",
    "        self.weights = 0.01*np.random.randn(n_inputs,n_neurons) # the transpose step has been omitted hence\n",
    "        self.biases = np.zeros((1,n_neurons))\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self,inputs):\n",
    "        # Calculate output values from inputs, weights and biases \n",
    "        self.outputs = np.dot(inputs,self.weights) + self.biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU activation\n",
    "class Activation_ReLU:\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self,inputs):\n",
    "        # calculate ouput values from inputs\n",
    "        self.outputs = np.maximum(0,inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Softmax activation\n",
    "class Activation_Softmax:\n",
    "\n",
    "    #forward pass\n",
    "    def forward(self,inputs):\n",
    "\n",
    "        # get unnormalised probablities\n",
    "        exp_values = np.exp(inputs - np.max(inputs,axis=1,keepdims=True))\n",
    "        \n",
    "        #normalise them for each sample\n",
    "        probabilities = exp_values/np.sum(exp_values,axis = 1,keepdims=True)\n",
    "\n",
    "        self.output = probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n"
     ]
    }
   ],
   "source": [
    "#Create dataset \n",
    "X,Y = spiral_data(samples=100,classes=3)\n",
    "\n",
    "# create dense layer with 2 input features and 3 neurons\n",
    "dense1 = Layer_Dense(2,3)\n",
    "\n",
    "#create ReLU activation(to be used with dense layer)\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# create second dense layer with 3 inputs features(as we take outout from last layer) \n",
    "# and 3 output values \n",
    "dense2 = Layer_Dense(3,3)\n",
    "\n",
    "#create Softmax Activation (to be used with dense layer)\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# make a forward pass of our training data through thsi layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# make a forward pass through activation function\n",
    "activation1.forward(dense1.outputs)\n",
    "\n",
    "#make a forward pass through second dense layer\n",
    "dense2.forward(activation1.outputs)\n",
    "\n",
    "# make a forward pass through axtivation function\n",
    "activation2.forward(dense2.outputs)\n",
    "\n",
    "# output of first few samples\n",
    "print(activation2.output[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the distribution of predictions is almost equal, as each of the samples has ~33% \n",
    "(0.33) predictions for each class. This results from the random initialization of weights (a draw \n",
    "from the normal distribution, as not every random initialization will result in this) and zeroed \n",
    "biases. These outputs are also our “confidence scores.” To determine which classification the \n",
    "model has chosen to be the prediction, we perform an argmax​ on these outputs, which checks \n",
    "which of the classes in the output distribution has the highest confidence and returns its index - \n",
    "the predicted class index. That said, the confidence score can be as important as the class \n",
    "prediction itself. For example, the argmax of [0.22, 0.6, 0.18] ​ is the same as the argmax for  \n",
    "[0.32, 0.36, 0.32]. In both of these, the argmax function would return an index value of 1 \n",
    "(the 2nd element in Python’s zero-indexed paradigm), but obviously, a 60% confidence is much \n",
    "better than a 36% confidence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
