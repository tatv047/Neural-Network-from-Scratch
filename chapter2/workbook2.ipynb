{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "646a726f",
   "metadata": {},
   "source": [
    "### A single neuron\n",
    "Each neuron has a set of inputs. Each input also needs a weight associated with it. Inputs are the data that we pass into the model to get desired outputs. The values for weights and biases are what get “trained” during the training process. Lets say we hve 3 inputs for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2137a80f-0690-4aff-a08e-f820f7fd5c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1,2,3]\n",
    "weights = [0.2,0.8,-0.5]\n",
    "bias = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee069778-554f-489b-940a-919cd65c5ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3\n"
     ]
    }
   ],
   "source": [
    "output = inputs[0]*weights[0] + inputs[1]*weights[1] + inputs[2]*weights[2] + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5388e699",
   "metadata": {},
   "source": [
    "### A layer of neuron\n",
    "The layer’s output is a set of each of these outputs — one per each neuron. Let’s say we have a scenario with \n",
    "3 neurons in a layer and 4 inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "235225bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "inputs = [1,2,3,2.5]\n",
    "weights1 = [0.2, 0.8, -0.5, 1]\n",
    "weights2 = [0.5, -0.91, 0.26, -0.5] \n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87] \n",
    "bias1 = 2\n",
    "bias2 = 3\n",
    "bias3 = 0.5\n",
    "outputs = [\n",
    "    # neuron 1\n",
    "    inputs[0]*weights1[0] +\n",
    "    inputs[1]*weights1[1] +\n",
    "    inputs[2]*weights1[2] +\n",
    "    inputs[3]*weights1[3] + bias1,\n",
    "\n",
    "    # neuron 2\n",
    "    inputs[0]*weights2[0] +\n",
    "    inputs[1]*weights2[1] +\n",
    "    inputs[2]*weights2[2] +\n",
    "    inputs[3]*weights2[3] + bias2,\n",
    "\n",
    "    # neuron 3\n",
    "    inputs[0]*weights3[0] +\n",
    "    inputs[1]*weights3[1] +\n",
    "    inputs[2]*weights3[2] +\n",
    "    inputs[3]*weights3[3] + bias3,\n",
    "]\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a958b1f",
   "metadata": {},
   "source": [
    "Above is an example of \"fully connected \" neural network: <br>\n",
    "Every neuron of the current layer is connected to every neuron of the last layer. Remember there's no need to connect neurons like this.<br>\n",
    "A more efficient way of above sol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34bdc1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "# my code\n",
    "inputs = [1, 2, 3, 2.5] \n",
    "weights = [[0.2, 0.8, -0.5, 1], \n",
    "           [0.5, -0.91, 0.26, -0.5], \n",
    "           [-0.26, -0.27, 0.17, 0.87]] \n",
    "biases = [2, 3, 0.5] \n",
    "\n",
    "layer_outputs = []\n",
    "\n",
    "for i in range(len(weights)):\n",
    "    sum = 0\n",
    "    for j in range(len(weights[i])):\n",
    "        sum+= inputs[j]*weights[i][j]\n",
    "    \n",
    "    sum += biases[i]\n",
    "    layer_outputs.append(sum)\n",
    "\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5879cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "# books code\n",
    "inputs = [1, 2, 3, 2.5] \n",
    "weights = [[0.2, 0.8, -0.5, 1], \n",
    "           [0.5, -0.91, 0.26, -0.5], \n",
    "           [-0.26, -0.27, 0.17, 0.87]] \n",
    "biases = [2, 3, 0.5] \n",
    "\n",
    "layer_outputs = []\n",
    "\n",
    "for neuron_weights,neuron_bias in zip(weights,biases):\n",
    "    neuron_output = 0\n",
    "    for n_inputs,weight in zip(inputs,neuron_weights):\n",
    "        neuron_output+=n_inputs*weight\n",
    "    neuron_output+=neuron_bias\n",
    "    layer_outputs.append(neuron_output)\n",
    "\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1561a2ed",
   "metadata": {},
   "source": [
    "### Tensors,Array and Vectors\n",
    "For a list of list to be an array it has to be homologous. A list of lists is \n",
    "homologous if each list along a dimension is identically long, and this must be true for each \n",
    "dimension. \n",
    "```\n",
    "another_list_of_lists = [[4,2,3],\n",
    "                        [5,1]] \n",
    "```\n",
    "Note that every dimension need not have the same length,perfectly normal to have an array of size 4X3.\n",
    "<br><br>\n",
    "Now a matrix is a rectangular array. It has columns and rows. It is two dimensional.So a matrix can be an array (a 2D array). <br>\n",
    "**Can all arrays be matrices?** NO. <br>\n",
    "An array can be fatr more than columns and rows,as it could have four dimensions, twenty dimensions, and so on.\n",
    "```\n",
    "list_matrix_array = [[4,2],\n",
    "                    [5,1],\n",
    "                    [8,2]] \n",
    "```\n",
    "**“What is a tensor, to a computer scientist, in the context of deep learning?”** <br>\n",
    "A tensor object is an object that can be represented as an array. <br>\n",
    "What this means is, as programmers, we can (and will) treat tensors as arrays in the context of deep learning, and that’s really all the thought we have to put into it. <br>\n",
    "Are all tensors just​ arrays? No, but they are represented as arrays in our code, so, to us, they’re only arrays. <br>\n",
    "<br>\n",
    "**What is an array?**\n",
    "we define an array as an ordered homologous container for numbers, and mostly use this term when working with the NumPy package since that’s what the \n",
    "main data structure is called within it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec910ff",
   "metadata": {},
   "source": [
    "### A single neuron with Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d0709a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "inputs = [1.0, 2.0, 3.0, 2.5] \n",
    "weights = [0.2, 0.8, -0.5, 1.0] \n",
    "bias = 2.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "737c3e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "output = np.dot(inputs,weights) + bias\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bfa05a4",
   "metadata": {},
   "source": [
    "### Layer of Neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7227e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1.0, 2.0, 3.0, 2.5] \n",
    "weights = [[0.2, 0.8, -0.5, 1], \n",
    "            [0.5, -0.91, 0.26, -0.5], \n",
    "            [-0.26, -0.27, 0.17, 0.87]] \n",
    "biases = [2.0, 3.0, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8e5c2a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8   1.21  2.385]\n"
     ]
    }
   ],
   "source": [
    "layer_outputs = np.dot(weights,inputs) + biases\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2f5e53",
   "metadata": {},
   "source": [
    "To explain the order of parameters we are passing into np.dot()​, we should think of it as whatever comes first will decide the output shape.<br>\n",
    "In our case, we are passing a list of neuron weights first and then the inputs, as our goal is to get a list of neuron outputs.As we mentioned, <br>\n",
    "a dot product of a matrix and a vector results in a list of dot products. <br>\n",
    "The np.dot()​ method treats the matrix as a list of vectors and performs a dot product of each of those vectors with the other vector.In this example,<br> we used that property to pass a matrix, which was a list of neuron weight vectors and a vector of inputs and get a list of dot products — neuron outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16589be",
   "metadata": {},
   "source": [
    "### A Batch of Data \n",
    "\n",
    "To train, neural networks tend to receive data in batches. So far, the example input data have been only one sample (or observation) of various features called a feature set: <br>\n",
    "inputs = [1, 2, 3, 2.5] <br>\n",
    "Each of these values is a feature observation datum, and together they form a feature set instance, also called an observation, or most commonly, a sample.<br><br>\n",
    "Often, neural networks expect to take in many samples at a time for two reasons.**One reason is that it’s faster to train in batches in parallel processing**, and **the other reason is that batches help with generalization during training.** <br>\n",
    "If you fit (perform a step of a training process) on one sample at a time, you’re highly likely to keep fitting to that individual sample, rather than \n",
    "slowly producing general tweaks to weights and biases that fit the entire dataset. Fitting or training in batches gives you a higher chance of making more meaningful changes to weights and biases. <br>\n",
    "We have a matrix of inputs and a matrix of weights now, and we need to perform the dot product \n",
    "on them somehow, but how and what will the result be? **Matrix Product** <br> <br>\n",
    "**NOTE:** NumPy does not have a dedicated method for performing matrix product — the dot product and matrix product are both implemented in a single method: np.dot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a3e8f3",
   "metadata": {},
   "source": [
    "### A Layer of Neurons & Batch of Data w/ NumPy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5238006d",
   "metadata": {},
   "source": [
    "The first argument is inputs and the second one is the transposed weight.<br>\n",
    "You'll learn that it’s more useful to have a result consisting of a list of layer outputs per each sample than a list of neurons and their outputs sample-wise , that's why the current ordering.We want the resulting array to be sample-related and not neuron-related as we’ll pass those samples further \n",
    "through the network, and the next layer will expect a batch of inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93082949",
   "metadata": {},
   "source": [
    "We can perform np.dot()​ on a plain Python list of lists as NumPy will convert them to matrices internally. We are converting weights ourselves <br>\n",
    "though to perform transposition operation first, T​ in the code, as plain Python list of lists does not support it. Speaking of biases, we do not <br>\n",
    "need to make it a NumPy array for the same reason — NumPy is going to do that internally. <br>\n",
    "Biases are a list, though, so they are a 1D array as a NumPy array. The addition of this bias vector to a matrix (of the dot products in this case) <br>\n",
    "works similarly to the dot product of a matrix and vector that we described earlier; The bias vector will be added to each row vector of the matrix. \n",
    "Since each column of the matrix product result is an output of one neuron, and the vector is going to be added to each row vector, the first bias is <br>\n",
    "going to be added to each first element of those vectors, second to second, etc. That’s what we need — the bias of each neuron needs to be added to all <br>\n",
    "of the results of this neuron performed on all input vectors (samples). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2349a7a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab6b87c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.8  ,  1.21 ,  2.385],\n",
       "       [ 8.9  , -1.81 ,  0.2  ],\n",
       "       [ 1.41 ,  1.051,  0.026]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = [[1.0,2.0,3.0,2.5],\n",
    "          [2.0,5.0,-1.0,2.0],\n",
    "          [-1.5,2.7,3.3,-0.8]]\n",
    "weights = [[0.2,0.8,-0.5,1.0],\n",
    "            [0.5,-0.91,0.26,-0.5],\n",
    "            [-0.26,-0.27,0.17,0.87]]\n",
    "biases = [2.0,3.0,0.5]\n",
    "\n",
    "outputs = np.dot(inputs,np.array(weights).T) + biases\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bd220f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
