{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 : Calculating Network Error with Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **loss function**, also referred to as the **cost function** algorithm that quantifies how wrong a model is.**Loss**, is the is the measure of this metric. Since loss is the model’s error, we ideally want it to be 0. <br>\n",
    "You may wonder why we do not calculate the error of a model based on the argmax accuracy. <br>\n",
    "Recall our earlier example of confidence: [0.22, 0.6, 0.18] vs [0.32, 0.36, 0.32]. If the correct class were indeed the middle one (index 1), the model accuracy would be identical between the two above. But are these two examples really​ as accurate as each other?<br>\n",
    "They are not, because accuracy is simply applying an argmax to the output to find the index of the biggest value. The output of a neural network is actually confidence, and more confidence in the correct answer is better. Because of this, we strive to **increase correct confidence and decrease misplaced confidence.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Categorical Cross-Entropy Loss**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are dealing with a classification problem here. The model has a softmax activation function for the output layer, which means it’s \n",
    "outputting a probability distribution. **Categorical cross-entropy** is explicitly used to compare a “**ground-truth**” probability (y ​ or ​ “targets​”) and some predicted distribution (y-hat ​ or “predictions​”), so it makes sense to use cross-entropy here. **It is also one of the most \n",
    "commonly used loss functions with a softmax activation on the output layer.** <br>\n",
    "<br>\n",
    "The formula for calculating the categorical cross-entropy of y​ (actual/desired distribution) and y-hat​ (predicted distribution) is:\n",
    "$$L_{i} = -\\sum_{j} y_{i,j}\\log{\\hat{y_{i,j}}}$$ \n",
    "Where $L_{i}$ denotes sample loss value, $i$​ is the i-th sample in the set, $j$ ​ is the label/output index, y denotes the target values, and y-hat​ denotes the predicted values. <br>\n",
    "In our case, we have a classification model that returns a probability distribution over all of the outputs. Cross-entropy compares two probability distributions. In our case, we have a softmax output, let’s say it’s: <br>\n",
    "```\n",
    "softmax_output = [0.7,0.1,0.2]\n",
    "```\n",
    "Which probability distribution do we intend to compare this to? <br>\n",
    "We have 3 class confidences in the above output, and let’s assume that the desired prediction is the first class (index 0, which is currently 0.7). If that’s the intended prediction, then the desired probability distribution is  [1, 0, 0]. <br> <br>\n",
    "When comparing the model’s results to a one-hot vector using cross-entropy, the other parts of the equation zero out, and the target probability’s log loss is multiplied by 1, making the cross-entropy calculation relatively simple. This is also a special case of the cross-entropy calculation, called categorical cross-entropy. To exemplify this — if we take a softmax output of [0.7, 0.1, 0.2] and targets of [1, 0, 0], we can apply the \n",
    "calculations as follows: \n",
    "$$L_{i} = - {1 \\cdot \\log{0.7} + 0 \\cdot \\log{0.1} + 0 \\cdot \\log{0.2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For coding purposes , we can simplify it to : \n",
    "$$L_{i} = -\\log{\\hat{y_{i,k}}}$$ \n",
    "Where $L_{i}$ denotes sample loss value, $i$​ is the i-th sample in a set, $k$​ is the index of the target label \n",
    "(ground-true label), y​ denotes the target values and y-hat​ denotes the predicted values. <br>\n",
    "Above is possible due to **one-hot** vectors. Arrays or vectors like this are called one-hot,​ meaning one of the values is “hot” (on), with a value of 1, and the rest are “cold” (off), with values of 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both these examples\n",
    "```\n",
    "[0.22, 0.6, 0.18] \n",
    "[0.32, 0.36, 0.32] \n",
    "```\n",
    "the argmax​ of these vectors will return the second class as the prediction, but the model’s confidence about these predictions is high only for one of \n",
    "them. The Categorical Cross-Entropy Loss accounts for that and **outputs a larger loss the lower the confidence** is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we proceed about this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A batch of three samples with three classes gives the following softmax distribution\n",
    "softmax_outputs = [[0.7,0.1,0.2],\n",
    "                   [0.1,0.5,0.4],\n",
    "                   [0.02,0.9,0.08]]\n",
    "\n",
    "# the classes are dogs[0],cats[1] & humans[2]\n",
    "# target values corresponding these three samples are \n",
    "class_targets = [0,1,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first value, 0, in class_targets means the first softmax output distribution’s intended \n",
    "prediction was the one at the 0th index of [0.7, 0.1, 0.2];<br>\n",
    "the model has a 0.7 confidence score that this observation is a dog. <br>\n",
    "This continues throughout the batch, where the intended target \n",
    "of the 2nd softmax distribution, [0.1, 0.5, 0.4], was at an index of 1; <br>\n",
    "the model only has a 0.5 confidence score that this is a cat — the model is less certain about this observation. <br>\n",
    "In the last sample, it’s also the 2nd index from the softmax distribution, a value of 0.9 in this case — a pretty high confidence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n",
      "0.5\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "for targ_idx,distribution in zip(class_targets,softmax_outputs):\n",
    "    print(distribution[targ_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7 0.5 0.9]\n"
     ]
    }
   ],
   "source": [
    "# using numpy \n",
    "import numpy as np\n",
    "softmax_outputs = np.array(softmax_outputs)\n",
    "print(softmax_outputs[range(len(softmax_outputs)),class_targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35667494 0.69314718 0.10536052]\n"
     ]
    }
   ],
   "source": [
    "print(-np.log(softmax_outputs[range(len(softmax_outputs)),class_targets]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we want an average loss per batch to have an idea about how our model is doing during training. We'll use the arithmetic mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "neg_loss = -np.log(softmax_outputs[range(len(softmax_outputs)),class_targets])\n",
    "avg_loss = np.mean(neg_loss)\n",
    "print(avg_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target vakues can be **sparse** or **one-hot** encoded, so we need to add a check in loss calculation. If target are single dimensional like lists then it's sparse, but if there are two dimensions like a list of lists then it's one-hot encoded. <br>\n",
    "In this second case, we’ll implement a solution using the first equation from this chapter, instead of filtering out the confidences at the target labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax_outputs = np.array([[0.7,0.1,0.2],\n",
    "                   [0.1,0.5,0.4],\n",
    "                   [0.02,0.9,0.08]])\n",
    "class_targets = np.array([[1,0,0],\n",
    "                          [0,1,0],\n",
    "                          [0,1,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "# probablities of target values\n",
    "# only if categorical values\n",
    "if len(class_targets.shape) == 1:\n",
    "    correct_confidences = softmax_outputs[\n",
    "        range(len(softmax_outputs)),\n",
    "        class_targets\n",
    "    ]\n",
    "\n",
    "# mask values - only for one-hot encoded labels\n",
    "elif len(class_targets.shape) == 2:\n",
    "    correct_confidences = np.sum(\n",
    "        softmax_outputs*class_targets,\n",
    "        axis = 1\n",
    "    )\n",
    "\n",
    "# losses\n",
    "neg_loss = -np.log(correct_confidences)\n",
    "\n",
    "average_loss = np.mean(neg_loss)\n",
    "print(average_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Another Problem:**\n",
    "Before we move on, there is one additional problem to solve. The softmax output, which is also \n",
    "an input to this loss function, consists of numbers in the range from 0 to 1 - a list of confidences. \n",
    "It is possible that the model will have full confidence for one label making all the remaining \n",
    "confidences zero. Similarly, it is also possible that the model will assign full confidence to a value \n",
    "that wasn’t the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_13340\\2933082444.py:1: RuntimeWarning: divide by zero encountered in log\n",
      "  np.log(0)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-inf"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**: <br>\n",
    "Clip both the edges of the confidence interval using very small number,say one tenth of a million {1e-7}. <br>\n",
    "This will stop loss from being exactly zero or $-\\infin$ ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.11809565095832"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(1e-7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0000000494736474e-07"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-np.log(1-1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Categorical Cross-Entropy Loss Class**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No matter which loss function we’ll use, the overall loss is always a mean value of all sample losses. <br>\n",
    "The Loss class is containing the calculate method that will call our loss object’s forward method and calculate the mean value of the returned sample losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common loss class\n",
    "class Loss:\n",
    "    # Calculates the data and regularization losses \n",
    "    # given model output and ground truth values\n",
    "    def calculate(self,output,y):\n",
    "        # calculate sample losses\n",
    "        sample_losses = self.forward(output,y)\n",
    "\n",
    "        #calculate mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "\n",
    "        #Return loss\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Entropy Loss\n",
    "class Loss_CategoricalCrossEntropy(Loss): #using inheritance here\n",
    "\n",
    "    def forward(self,y_pred,y_true):\n",
    "\n",
    "        # number of sample in a batch\n",
    "        samples = len(y_pred)\n",
    "\n",
    "        # Clip data to prevent division by 0 \n",
    "        # Clip both sides to not drag mean towards any value \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) \n",
    "\n",
    "        # Probabilities for target values - \n",
    "        # only if categorical labels \n",
    "        if len(y_true.shape) == 1: \n",
    "            correct_confidences = y_pred_clipped[ \n",
    "                range(samples), \n",
    "                y_true \n",
    "            ] \n",
    " \n",
    "        # Mask values - only for one-hot encoded labels \n",
    "        elif len(y_true.shape) == 2: \n",
    "            correct_confidences = np.sum( \n",
    "                y_pred_clipped * y_true, axis=1 \n",
    "            ) \n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_funtion = Loss_CategoricalCrossEntropy()\n",
    "loss = loss_funtion.calculate(softmax_outputs,class_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy calculation** <br>\n",
    "While loss is a useful metric for optimizing a model, the metric commonly used in practice along with loss is the **accuracy**, which describes how often the largest confidence is the correct class in terms of a fraction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Probabilities of 3 samples \n",
    "softmax_outputs = np.array([[0.7, 0.2, 0.1], \n",
    "                            [0.5, 0.1, 0.4], \n",
    "                            [0.02, 0.9, 0.08]]) \n",
    "# Target (ground-truth) labels for 3 samples \n",
    "class_targets = np.array([0, 1, 1]) \n",
    " \n",
    "# Calculate values along second axis (axis of index 1) \n",
    "predictions = np.argmax(softmax_outputs, axis=1) \n",
    "\n",
    "# If targets are one-hot encoded - convert them \n",
    "if len(class_targets.shape) == 2: \n",
    "    class_targets = np.argmax(class_targets, axis=1) \n",
    "\n",
    "# True evaluates to 1; False to 0 \n",
    "accuracy = np.mean(predictions==class_targets) \n",
    " \n",
    " \n",
    "print('acc:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining everything**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "loss: 1.0986104\n",
      "acc: 0.34\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import nnfs \n",
    "from nnfs.datasets import spiral_data \n",
    " \n",
    "nnfs.init() \n",
    " \n",
    " \n",
    "# Dense layer \n",
    "class Layer_Dense: \n",
    " \n",
    "    # Layer initialization \n",
    "    def __init__(self, n_inputs, n_neurons): \n",
    "        # Initialize weights and biases \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons) \n",
    "        self.biases = np.zeros((1, n_neurons)) \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs): \n",
    "        # Calculate output values from inputs, weights and biases \n",
    "        self.output = np.dot(inputs, self.weights) + self.biases \n",
    " \n",
    " \n",
    "# ReLU activation \n",
    "class Activation_ReLU: \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs): \n",
    "        # Calculate output values from inputs \n",
    "        self.output = np.maximum(0, inputs)\n",
    "    \n",
    "# Softmax activation \n",
    "class Activation_Softmax: \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs): \n",
    " \n",
    "        # Get unnormalized probabilities \n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, \n",
    "                                            keepdims=True)) \n",
    "        # Normalize them for each sample \n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, \n",
    "                                            keepdims=True) \n",
    " \n",
    "        self.output = probabilities \n",
    " \n",
    " \n",
    "# Common loss class \n",
    "class Loss: \n",
    " \n",
    "    # Calculates the data and regularization losses \n",
    "    # given model output and ground truth values \n",
    "    def calculate(self, output, y): \n",
    " \n",
    "        # Calculate sample losses \n",
    "        sample_losses = self.forward(output, y) \n",
    " \n",
    "        # Calculate mean loss \n",
    "        data_loss = np.mean(sample_losses) \n",
    " \n",
    "        # Return loss \n",
    "        return data_loss \n",
    " \n",
    " \n",
    "# Cross-entropy loss \n",
    "class Loss_CategoricalCrossentropy(Loss): \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, y_pred, y_true): \n",
    " \n",
    "        # Number of samples in a batch \n",
    "        samples = len(y_pred) \n",
    " \n",
    "        # Clip data to prevent division by 0 \n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "         # Probabilities for target values - \n",
    "        # only if categorical labels \n",
    "        if len(y_true.shape) == 1: \n",
    "            correct_confidences = y_pred_clipped[ \n",
    "                range(samples), \n",
    "                y_true \n",
    "            ] \n",
    " \n",
    "        # Mask values - only for one-hot encoded labels \n",
    "        elif len(y_true.shape) == 2: \n",
    "            correct_confidences = np.sum( \n",
    "                y_pred_clipped * y_true, \n",
    "                axis=1 \n",
    "            ) \n",
    " \n",
    "        # Losses \n",
    "        negative_log_likelihoods = -np.log(correct_confidences) \n",
    "        return negative_log_likelihoods \n",
    " \n",
    " \n",
    " \n",
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    " \n",
    "# Create Dense layer with 2 input features and 3 output values \n",
    "dense1 = Layer_Dense(2, 3) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    " \n",
    "# Create second Dense layer with 3 input features (as we take output \n",
    "# of previous layer here) and 3 output values \n",
    "dense2 = Layer_Dense(3, 3) \n",
    " \n",
    "# Create Softmax activation (to be used with Dense layer): \n",
    "activation2 = Activation_Softmax() \n",
    " \n",
    "# Create loss function \n",
    "loss_function = Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Perform a forward pass of our training data through this layer \n",
    "dense1.forward(X) \n",
    " \n",
    "# Perform a forward pass through activation function \n",
    "# it takes the output of first dense layer here \n",
    "activation1.forward(dense1.output) \n",
    "\n",
    "# Perform a forward pass through second Dense layer \n",
    "# it takes outputs of activation function of first layer as inputs \n",
    "dense2.forward(activation1.output) \n",
    "\n",
    "# Perform a forward pass through activation function \n",
    "# it takes the output of second dense layer here \n",
    "activation2.forward(dense2.output) \n",
    "\n",
    "# Let's see output of the first few samples: \n",
    "print(activation2.output[:5]) \n",
    "\n",
    "# Perform a forward pass through loss function \n",
    "# it takes the output of second dense layer here and returns loss \n",
    "loss = loss_function.calculate(activation2.output, y) \n",
    "\n",
    "# Print loss value \n",
    "print('loss:', loss)\n",
    "\n",
    "# Calculate accuracy from output of activation2 and targets \n",
    "# calculate values along first axis \n",
    "predictions = np.argmax(activation2.output, axis=1) \n",
    "if len(y.shape) == 2: \n",
    "   y = np.argmax(y, axis=1) \n",
    "accuracy = np.mean(predictions==y) \n",
    "# Print accuracy \n",
    "print('acc:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we get ~0.33​ values since the model is random, and its average loss is also not great for \n",
    "these data, as we’ve not yet trained our model on how to correct its errors."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
